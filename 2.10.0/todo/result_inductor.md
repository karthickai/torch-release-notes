
# Release Notes worksheet inductor

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## inductor
### bc breaking
### deprecation
### new features
### improvements
- [Triton] [Inductor] Pruned failed compilations from Autotuning candidates ([#162673](https://github.com/pytorch/pytorch/pull/162673))
- Extend triton_mm auto-tune options for HIM shapes ([#163273](https://github.com/pytorch/pytorch/pull/163273))
### bug fixes
- Fix some edge cases ([#162295](https://github.com/pytorch/pytorch/pull/162295))
- Fix TMA transpose logic to handle 1D shapes + string differences ([#163966](https://github.com/pytorch/pytorch/pull/163966))
- fix flex attention eager: dont round down scores to low-precision (closes #163588) ([#163986](https://github.com/pytorch/pytorch/pull/163986))
- Fix a condition error in torch/_inductor/codegen/debug_utils.py ([#165033](https://github.com/pytorch/pytorch/pull/165033))
- Thread deterministic config vars to subproc compilation ([#165729](https://github.com/pytorch/pytorch/pull/165729))
### performance
- [Inductor] Naive foreach autotune support ([#162053](https://github.com/pytorch/pytorch/pull/162053))
- [Inductor] Naive foreach autotune support ([#162053](https://github.com/pytorch/pytorch/pull/162053))
- [Inductor] Naive foreach autotune support ([#162053](https://github.com/pytorch/pytorch/pull/162053))
- [Inductor] Naive foreach autotune support ([#162053](https://github.com/pytorch/pytorch/pull/162053))
### docs
### devs
### Untopiced
- [standalone_compile] binary format write should be atomic ([#162432](https://github.com/pytorch/pytorch/pull/162432))
- [Optimus] Add batch dropout pattern ([#162443](https://github.com/pytorch/pytorch/pull/162443))
- [ROCm][Inductor][CK backend] Install rocm-composable-kernel python package on ROCm Linux CI docker images ([#162288](https://github.com/pytorch/pytorch/pull/162288))
- [Cutlass] Add tanh activation and test case for activations ([#162535](https://github.com/pytorch/pytorch/pull/162535))
- [Cutlass] Add exp and sigmoid activations ([#162536](https://github.com/pytorch/pytorch/pull/162536))
- [inductor] Add FLOAT_IS_NAN and COMPLEX_IS_NAN guards ([#162537](https://github.com/pytorch/pytorch/pull/162537))
- [CUDAGraph][UX] warn many times for rerecording from dynamic shapes ([#162696](https://github.com/pytorch/pytorch/pull/162696))
- [Inductor] Fix ComboKernels failing due to missing helper functions ([#162759](https://github.com/pytorch/pytorch/pull/162759))
- [Triton] [Inductor] Enable TMA store for TMA mm templates ([#160480](https://github.com/pytorch/pytorch/pull/160480))
- [Triton] [Inductor] Add a Blackwell specific Template for persistent matmul ([#162916](https://github.com/pytorch/pytorch/pull/162916))
- Fix provenance tracking kernel name for fallback kernels ([#162628](https://github.com/pytorch/pytorch/pull/162628))
- Add to inductor provenance tracking doc ([#162975](https://github.com/pytorch/pytorch/pull/162975))
- [Graph Partition] allow sharing default device context ([#162873](https://github.com/pytorch/pytorch/pull/162873))
- [AOTInductor] Use CudaCachingAllocator for memory allocation ([#162893](https://github.com/pytorch/pytorch/pull/162893))
- Fix windows path escape characters ([#162761](https://github.com/pytorch/pytorch/pull/162761))
- Fix: ShapeEnv not propagated properly to inductor SizeVars ([#162927](https://github.com/pytorch/pytorch/pull/162927))
- [Flex] Changing how bwd configs are setup and updating default b200 config ([#163318](https://github.com/pytorch/pytorch/pull/163318))
- [Inductor][Intel GPU] Save `threads_per_warp` from tirton compiled kernel for launching kernel correctly in cpp wrapper. ([#163315](https://github.com/pytorch/pytorch/pull/163315))
- [Opitmus] fix fp8 activation quatization for duplicates forward output ([#163364](https://github.com/pytorch/pytorch/pull/163364))
- [AOTInductor] Add grid information for Triton Kernels ([#160131](https://github.com/pytorch/pytorch/pull/160131))
- [inductor] Support out_dtype arg to matmul ([#163393](https://github.com/pytorch/pytorch/pull/163393))
- [multi-kernel] shape-similarity kernel selection ([#163090](https://github.com/pytorch/pytorch/pull/163090))
- [AOTI] Pass comments from metadata to the autotune block ([#163600](https://github.com/pytorch/pytorch/pull/163600))
- [inductor] Fix bugs in emulate_precision_casts ([#163520](https://github.com/pytorch/pytorch/pull/163520))
- [Triton] [Inductor] Enable Epilogue Subtiling in the blackwell ws template ([#163145](https://github.com/pytorch/pytorch/pull/163145))
- [Triton] [Inductor] Set default configs for Blackwell Matmul Template ([#163740](https://github.com/pytorch/pytorch/pull/163740))
- [Inductor] Update DeviceAssert op to behave like store ([#163696](https://github.com/pytorch/pytorch/pull/163696))
- Fix redundant H2D/D2H memcpy in cpp_wrapper by creating scalar tensors on CPU ([#160584](https://github.com/pytorch/pytorch/pull/160584))
- [AOTInductor] Add input information for Triton Kernels in AOTI ([#160380](https://github.com/pytorch/pytorch/pull/160380))
- Refactor Provenance Tracking ([#163378](https://github.com/pytorch/pytorch/pull/163378))
- [Triton] [Inductor] Prune template selection based on decompose_k ([#163781](https://github.com/pytorch/pytorch/pull/163781))
- [aoti] Save compute information ([#163792](https://github.com/pytorch/pytorch/pull/163792))
- Allow unbacked to unbacked replacements if rhs unbacked symbols are all inputs ([#163652](https://github.com/pytorch/pytorch/pull/163652))
- [Inductor] Check if profiling before using record_function in CompiledFxGraph ([#163747](https://github.com/pytorch/pytorch/pull/163747))
- [inductor] Fix unbounded number of substitutions when equality checks contain Max expr ([#163685](https://github.com/pytorch/pytorch/pull/163685))
- [scan] create fw and bw graphs via partitioning ([#162754](https://github.com/pytorch/pytorch/pull/162754))
- [inductor] pdl: enable launch and deduplicate waits ([#162014](https://github.com/pytorch/pytorch/pull/162014))
- Back out "Revert D81959389" ([#163905](https://github.com/pytorch/pytorch/pull/163905))
- [inductor] use hint_override in kernel benchmark args ([#164207](https://github.com/pytorch/pytorch/pull/164207))
- [aoti] AOTI mingw cross compilation  ([#163188](https://github.com/pytorch/pytorch/pull/163188))
- [inductor] add a runtime assert for triton shapes ([#164242](https://github.com/pytorch/pytorch/pull/164242))
- [AOTI win] Add ABI stable method for updating constant buffer ([#163819](https://github.com/pytorch/pytorch/pull/163819))
- [cutlass-4][take 2] upgrade to cutlass 4.2.1 ([#164159](https://github.com/pytorch/pytorch/pull/164159))
- [inductor] thread hint_override in more kernel args ([#164494](https://github.com/pytorch/pytorch/pull/164494))
- [Inductor] deterministic mode ([#163589](https://github.com/pytorch/pytorch/pull/163589))
- Add suppressions to torch/_inductor ([#165062](https://github.com/pytorch/pytorch/pull/165062))
- Add an option to put store large mmap weights on disk ([#164526](https://github.com/pytorch/pytorch/pull/164526))
- [inductor] fix issue for example value with unbacked strides ([#163660](https://github.com/pytorch/pytorch/pull/163660))
- [Inductor] Fix out-of-bounds indices in repeat_interleave decomposition ([#165368](https://github.com/pytorch/pytorch/pull/165368))
- Support libtorch and posix mingw flavor ([#165574](https://github.com/pytorch/pytorch/pull/165574))
- Add suppressions for _inductor/codegen ([#165659](https://github.com/pytorch/pytorch/pull/165659))
- Fix incorrect function signature in template ([#165567](https://github.com/pytorch/pytorch/pull/165567))
- Remove unnecessary noqa suppressions  ([#164106](https://github.com/pytorch/pytorch/pull/164106))
- [ROCm][inductor] autotune support for persistent reduction kernels ([#163908](https://github.com/pytorch/pytorch/pull/163908))
- Fix self assignment ([#165816](https://github.com/pytorch/pytorch/pull/165816))
- [FlexAttention] Fix dynamic shaped heads flex_flash check ([#165866](https://github.com/pytorch/pytorch/pull/165866))
- Add type suppressions to _inductor/runtime ([#165918](https://github.com/pytorch/pytorch/pull/165918))
- [ATen] Add reduction tag to reduction operators ([#165155](https://github.com/pytorch/pytorch/pull/165155))
- [ROCm][inductor] heuristic improvements for reduction kernels ([#161280](https://github.com/pytorch/pytorch/pull/161280))
- Allow GraphPickler to pickle graph modules containing AOTCompiled subgraphs ([#165844](https://github.com/pytorch/pytorch/pull/165844))
- Fix pyrefly ignore syntax in _inductor ([#166247](https://github.com/pytorch/pytorch/pull/166247))
- [AOTI] Remove c10 as linked library ([#165489](https://github.com/pytorch/pytorch/pull/165489))
- [Optimus] Rename the post_grad_graph tlparse log ([#166109](https://github.com/pytorch/pytorch/pull/166109))
- [inductor] Fix argmin/argmax returning incorrect indices for non-contiguous tensor ([#165983](https://github.com/pytorch/pytorch/pull/165983))
- [Inductor] Lower fallback nodes annotated with "should_fallback" ([#166339](https://github.com/pytorch/pytorch/pull/166339))
- [cpu/gpu split] ([#165696](https://github.com/pytorch/pytorch/pull/165696))
- Add python stack trace to AOTI generated code ([#160539](https://github.com/pytorch/pytorch/pull/160539))
- [ROCm][inductor] More configs for pointwise kernels. ([#166470](https://github.com/pytorch/pytorch/pull/166470))
- [Inductor] Enable Custom op Autotune Decompositions and Parameter Tuning ([#164212](https://github.com/pytorch/pytorch/pull/164212))
- [FlexFlash] Wire up mask_mod + blockmask to flash impl ([#166359](https://github.com/pytorch/pytorch/pull/166359))
- [FlexFlash] CuteDSL flat indexer needs to be colexigraphic in coordinate space ([#166657](https://github.com/pytorch/pytorch/pull/166657))
- Use Python 3.10 typing ([#148418](https://github.com/pytorch/pytorch/pull/148418))
- [11/N] Apply ruff UP035 rule  ([#166225](https://github.com/pytorch/pytorch/pull/166225))
- [RFC] Add experimental Pallas TorchInductor backend ([#166822](https://github.com/pytorch/pytorch/pull/166822))
- Invert unary read and write for fusion ([#161404](https://github.com/pytorch/pytorch/pull/161404))
- [Inductor] Fix unbacked float symbol handling in kernel codegen ([#166890](https://github.com/pytorch/pytorch/pull/166890))
- Expose torch.compiler.config.force_disable_caches as a public API ([#166699](https://github.com/pytorch/pytorch/pull/166699))
- CustomOp Inline Fusion ([#165952](https://github.com/pytorch/pytorch/pull/165952))
- [Inductor] Fix unbacked float symbol handling in kernel codegen ([#166890](https://github.com/pytorch/pytorch/pull/166890))
- fix static_input_indices subclass remapping under training ([#167127](https://github.com/pytorch/pytorch/pull/167127))
- [ROCm] Enable multi-arch compilation and unit tests for AOT Inductor ([#166357](https://github.com/pytorch/pytorch/pull/166357))
- [user-streams] Add fallbacks for record and wait event ([#167260](https://github.com/pytorch/pytorch/pull/167260))
- [2/N] Use context managers ([#167404](https://github.com/pytorch/pytorch/pull/167404))
- [AOTI] Fix a mixed-device bug for scatter_add ([#167341](https://github.com/pytorch/pytorch/pull/167341))
- Fix torch.cond HOP device in inductor ([#167354](https://github.com/pytorch/pytorch/pull/167354))
- [Inductor] optimize the heuristics of sum reduction ([#163144](https://github.com/pytorch/pytorch/pull/163144))
- [Inductor] fix CppTile2DKernel for fp8 datatype ([#167451](https://github.com/pytorch/pytorch/pull/167451))
- [inductor] Only generate compile-time auto-tuning block in the main graph ([#167131](https://github.com/pytorch/pytorch/pull/167131))
- [inductor] Optimize cold compile time when cudagraphs-partition is enabled ([#167132](https://github.com/pytorch/pytorch/pull/167132))
- Inductor Lite Mode ([#167115](https://github.com/pytorch/pytorch/pull/167115))
- [inductor] Remove output copy_ for pallas backend in some cases ([#167516](https://github.com/pytorch/pytorch/pull/167516))
- [inductor][NFC][1/X] extract create_no_valid_choices from AlgorithmSelectorCache.__call__ ([#167487](https://github.com/pytorch/pytorch/pull/167487))
- [effects] Add register_effectful_op ([#163284](https://github.com/pytorch/pytorch/pull/163284))
- [Inductor] Remove bf16 fallback for atomic_add ([#167380](https://github.com/pytorch/pytorch/pull/167380))
- [Inductor] Add support bound methods in pattern matcher ([#167795](https://github.com/pytorch/pytorch/pull/167795))
- [Inductor] optimize scalar welford_reduce ([#162709](https://github.com/pytorch/pytorch/pull/162709))
- [AOTI] Fix unknown constant type for device-moved constants ([#168138](https://github.com/pytorch/pytorch/pull/168138))
- Add dynamic config generation for custom op autotuning ([#167193](https://github.com/pytorch/pytorch/pull/167193))
- Fix aot_compile typing. ([#168320](https://github.com/pytorch/pytorch/pull/168320))
- [inductor] Fix a user-defined Triton kernel output + .cpu() correctness issue ([#168281](https://github.com/pytorch/pytorch/pull/168281))
- [inductor] Reduce cold compilation time caused by duplicated user-defined Triton kernels ([#168292](https://github.com/pytorch/pytorch/pull/168292))
- [Inductor][Quant]Support qconv_pointwise.tensor and qconv2d_pointwise.binary_tensor ([#166608](https://github.com/pytorch/pytorch/pull/166608))
- [Inductor] support masked vectorization for the tail_loop for integer and bool datatypes ([#165885](https://github.com/pytorch/pytorch/pull/165885))
- [AOTI] Set device info for subgraphs ([#169001](https://github.com/pytorch/pytorch/pull/169001))
- [Inductor] handle GroupedSchedulerNode in combo kernel fusion ([#168109](https://github.com/pytorch/pytorch/pull/168109))
- [AOTI] Fix a small buffer mutation issue ([#169347](https://github.com/pytorch/pytorch/pull/169347))
- Resolve collective autotuning test failure on arm ([#168919](https://github.com/pytorch/pytorch/pull/168919))
- [Inductor] Fix combo kernels for cpu backend ([#167781](https://github.com/pytorch/pytorch/pull/167781))
- Add check_lowerbound config for AOTI lowering ([#169430](https://github.com/pytorch/pytorch/pull/169430))
- [Inductor] Add debug output for specific pattern matching ([#169603](https://github.com/pytorch/pytorch/pull/169603))
- [Inductor] Fix combo kernels by populating constants for equal_to_1 args ([#168127](https://github.com/pytorch/pytorch/pull/168127))
- Fix slotscheck warnings ([#169348](https://github.com/pytorch/pytorch/pull/169348))
- [Inductor] Fix pattern matcher FailedMatch format string ([#169611](https://github.com/pytorch/pytorch/pull/169611))
- [AOTI] Use arg signature to derive the right kernel input type for "sympy.Integer" as well ([#169135](https://github.com/pytorch/pytorch/pull/169135))
- [ROCm][inductor] Codegen support for fast_tanhf ([#162052](https://github.com/pytorch/pytorch/pull/162052))
- Fix incorrect homogeneous tuple types ([#169463](https://github.com/pytorch/pytorch/pull/169463))
- [Inductor] Optimize identity permute in empty_permuted decomposition ([#169731](https://github.com/pytorch/pytorch/pull/169731))
- [inductor] Disable mixed-order reduction for cpp-wrapper ([#169859](https://github.com/pytorch/pytorch/pull/169859))
- [Inductor] Fix combo kernels variable collision with ND tiled reductions ([#168946](https://github.com/pytorch/pytorch/pull/168946))
- Fix Triton group_m config ([#169514](https://github.com/pytorch/pytorch/pull/169514))
- Revert PR#161280 "[ROCm][inductor] heuristic improvements for reduction kernels" ([#169792](https://github.com/pytorch/pytorch/pull/169792))
### not user facing
- [inductor] V.choices.get_mm_configs override point ([#161349](https://github.com/pytorch/pytorch/pull/161349))
- [inductor][template heuristics] don't take layout to generate choices ([#162238](https://github.com/pytorch/pytorch/pull/162238))
- [triton] enable int64 indexing in convolution and mm template ([#162506](https://github.com/pytorch/pytorch/pull/162506))
- [FlexAttn][Minor] Update FlexConfig doc ([#162533](https://github.com/pytorch/pytorch/pull/162533))
- [Inductor][UT] Fix flex attention related inductor cases ([#162450](https://github.com/pytorch/pytorch/pull/162450))
- [inductor] Enable combo kernels with unbacked inputs ([#162442](https://github.com/pytorch/pytorch/pull/162442))
- [ez] add docstring/typing for codegen_kernel_benchmark ([#162609](https://github.com/pytorch/pytorch/pull/162609))
- Use upper bound for persistent rblock ([#162441](https://github.com/pytorch/pytorch/pull/162441))
- [inductor] Add shape for store_output in matmul templates ([#162426](https://github.com/pytorch/pytorch/pull/162426))
- [inductor] Add shape to load_input in matmul templates ([#162513](https://github.com/pytorch/pytorch/pull/162513))
- [AOTI] Fix Windows fail to zip opened file. ([#162617](https://github.com/pytorch/pytorch/pull/162617))
- [dynamic shapes] unbacked-safe should_swap ([#160473](https://github.com/pytorch/pytorch/pull/160473))
- [FlexAttention][Easy] turn off TMA when cannot use it ([#162569](https://github.com/pytorch/pytorch/pull/162569))
- [Inductor][FP8] Add new scaled_mm and scaled_persistent_mm configs to Inductor FP8 Triton templates ([#162699](https://github.com/pytorch/pytorch/pull/162699))
- Reland "Fix conv exhaustive autotuning and expand Exhaustive test coverage" ([#161957](https://github.com/pytorch/pytorch/pull/161957))
- [Inductor][FP8] Validate exhaustive autotuning for FP8 Inductor templates ([#162678](https://github.com/pytorch/pytorch/pull/162678))
- [aoti] add config for libtorch free so ([#162655](https://github.com/pytorch/pytorch/pull/162655))
- [inductor] Fix removal of constexpr args from the launcher signature ([#161924](https://github.com/pytorch/pytorch/pull/161924))
- [inductor][ez] add src_hash property for Templates ([#161468](https://github.com/pytorch/pytorch/pull/161468))
- An improved heuristic for operator reordering for peak memory + debugging logs ([#161810](https://github.com/pytorch/pytorch/pull/161810))
- [BE] [Inductor] Update NoValidChoicesError logic ([#162814](https://github.com/pytorch/pytorch/pull/162814))
- fusion of large accumulated reads only at ir level ([#161978](https://github.com/pytorch/pytorch/pull/161978))
- [inductor] fix expand_shape when copy_shape is not a string ([#162739](https://github.com/pytorch/pytorch/pull/162739))
- [Inductor][CPP] Reuse the pre-existing kernel for the same kernels ([#158404](https://github.com/pytorch/pytorch/pull/158404))
- [pcache] Cache and AsyncCache implementations ([#162777](https://github.com/pytorch/pytorch/pull/162777))
- Workaround for mtia double init issue in has_triton ([#162974](https://github.com/pytorch/pytorch/pull/162974))
- [inductor][ez] add ChoiceCaller annotations ([#162672](https://github.com/pytorch/pytorch/pull/162672))
- [Fix XPU CI][Inductor UT] Fix test cases broken by community. ([#162933](https://github.com/pytorch/pytorch/pull/162933))
- [inductor] Fix convolution autotune check when groups != 1 ([#163094](https://github.com/pytorch/pytorch/pull/163094))
- [WOQ] Integrate CUDA support for int8pack_mm woq optimization pattern ([#161680](https://github.com/pytorch/pytorch/pull/161680))
- [inductor][choices] pass through annotations from KTC to ChoiceCaller ([#163117](https://github.com/pytorch/pytorch/pull/163117))
- write conv1d decomposition ([#163080](https://github.com/pytorch/pytorch/pull/163080))
- [Inductor] support mixed dtype in the native_layer_norm_backward meta function ([#159830](https://github.com/pytorch/pytorch/pull/159830))
- [pcache] Generalize testing + All Caches thread-safe ([#163173](https://github.com/pytorch/pytorch/pull/163173))
- [CPU][GEMM Template] Improve A16W8 performance ([#162479](https://github.com/pytorch/pytorch/pull/162479))
- [inductor][heuristics] add kernel template params ([#162781](https://github.com/pytorch/pytorch/pull/162781))
- replace more // with FloorDiv in inductor code ([#162969](https://github.com/pytorch/pytorch/pull/162969))
- [BE] [Triton] [Inductor] Add an assert for store_output val_shape to use a tuple ([#162887](https://github.com/pytorch/pytorch/pull/162887))
- [submodule] CUTLASS upgrade to 4.2.0 and change cutlass to cutlass_cppgen ([#163092](https://github.com/pytorch/pytorch/pull/163092))
- [WOQ] Integrate CUDA support for concat linear int8pack_mm woq optimization pattern ([#161848](https://github.com/pytorch/pytorch/pull/161848))
- Add local file path to inductor_output_code trace metadata ([#160920](https://github.com/pytorch/pytorch/pull/160920))
- add more restriction to fusion with large accumulate reads ([#163163](https://github.com/pytorch/pytorch/pull/163163))
- [Graph Partition] improve custom op output alias ([#163227](https://github.com/pytorch/pytorch/pull/163227))
- [Inductor][Triton][FP8] Add a Blackwell-specific scaled persistent + TMA template for GEMMs ([#163147](https://github.com/pytorch/pytorch/pull/163147))
- [inductor] avoid creating LoopBody twice ([#162101](https://github.com/pytorch/pytorch/pull/162101))
- [Inductor] don't call sympy_str when not needed ([#162126](https://github.com/pytorch/pytorch/pull/162126))
- [Inductor] do loop reordering in a separate final round ([#162355](https://github.com/pytorch/pytorch/pull/162355))
- [graph partition] Add way to register custom rule ([#163310](https://github.com/pytorch/pytorch/pull/163310))
- [inductor][triton heuristics] move allow tf32 out of config params ([#163305](https://github.com/pytorch/pytorch/pull/163305))
- [inductor][choices] move extra kwargs out of get_template_configs ([#163209](https://github.com/pytorch/pytorch/pull/163209))
- [inductor] bugfix: keep WeakDeps (WAR deps) during fusion ([#162316](https://github.com/pytorch/pytorch/pull/162316))
- Triton template IMA reads on B200 ([#163460](https://github.com/pytorch/pytorch/pull/163460))
- Allow add_persistent_r_block to scale up rblock up to a limit ([#162296](https://github.com/pytorch/pytorch/pull/162296))
- [inductor] Fix bug where viewed outputs get padded ([#163398](https://github.com/pytorch/pytorch/pull/163398))
- [inductor] Fallback on strided complex add ([#163387](https://github.com/pytorch/pytorch/pull/163387))
- [inductor] Skip test_baddmm on XPU ([#163414](https://github.com/pytorch/pytorch/pull/163414))
- [inductor] Don't require_dense for grid_sampler_2d_backward ([#163415](https://github.com/pytorch/pytorch/pull/163415))
- [Inductor] avoid CUDA__equal when constant tensors are from different device ([#163529](https://github.com/pytorch/pytorch/pull/163529))
- Replace Literal[None] with None in typing ([#163489](https://github.com/pytorch/pytorch/pull/163489))
- [sdpa] make sure to recompile if alignment is different than before ([#163083](https://github.com/pytorch/pytorch/pull/163083))
- [pt2][cache] rework cache for true generic usage + better tests ([#163488](https://github.com/pytorch/pytorch/pull/163488))
- [inductor] fix as_strided lowering with .view(dtype) inputs ([#163319](https://github.com/pytorch/pytorch/pull/163319))
- [inductor] libdevice.sqrt => tl.sqrt_rn ([#163419](https://github.com/pytorch/pytorch/pull/163419))
- [inductor] Freeze layouts in FlexAttention ([#163434](https://github.com/pytorch/pytorch/pull/163434))
- [inductor] Fix error from custom CUDA allocators ([#163422](https://github.com/pytorch/pytorch/pull/163422))
- [Inductor] Remove `no_type_check` annotation on properties ([#163570](https://github.com/pytorch/pytorch/pull/163570))
- Less aggressive persistent reduction when it could induce large masking with dynamic shapes ([#163365](https://github.com/pytorch/pytorch/pull/163365))
- use reduction hint for aggressive rblock ([#163371](https://github.com/pytorch/pytorch/pull/163371))
- [inductor] in emulate_precision_casts, disable fma fusion in triton ([#163073](https://github.com/pytorch/pytorch/pull/163073))
- [inductor] Fix issue with scalar arg handling ([#163481](https://github.com/pytorch/pytorch/pull/163481))
- [inductor] Fix divmod error in decomp ([#163482](https://github.com/pytorch/pytorch/pull/163482))
- [inductor] turn on loaf (for oss) by default ([#162030](https://github.com/pytorch/pytorch/pull/162030))
- Revert "[inductor] Fix issue with scalar arg handling" ([#163737](https://github.com/pytorch/pytorch/pull/163737))
- Revert to old behaviour of not padding strides if shape or stride is dynamic ([#163639](https://github.com/pytorch/pytorch/pull/163639))
- [WOQ][Inductor] Enable CUDA coverage for _weight_int8pack_mm ([#163461](https://github.com/pytorch/pytorch/pull/163461))
- Always produce kernel_info.json ([#163715](https://github.com/pytorch/pytorch/pull/163715))
- [Inductor] add a new config fallback_embedding_bag_byte_unpack ([#163803](https://github.com/pytorch/pytorch/pull/163803))
- Add SDPA patterns for T5 variants when batch size is 1 ([#163252](https://github.com/pytorch/pytorch/pull/163252))
- [inductor] skip bmm when converting channel last ([#159459](https://github.com/pytorch/pytorch/pull/159459))
- [Inductor] address comments from https://github.com/pytorch/pytorch/pull/163803 ([#163901](https://github.com/pytorch/pytorch/pull/163901))
- [AOTI] Pass in shape_env for get_stride_order ([#163925](https://github.com/pytorch/pytorch/pull/163925))
- [AOTI] log error triton kernel name during autotune ([#163889](https://github.com/pytorch/pytorch/pull/163889))
- [scan] materialize combine_fn in forward add more autograd tests  ([#161732](https://github.com/pytorch/pytorch/pull/161732))
- [inductor] add subsystem to pattern matcher ([#163922](https://github.com/pytorch/pytorch/pull/163922))
- [Inductor][CPP] Fix the test case of test_linear_reuse_kernels ([#163723](https://github.com/pytorch/pytorch/pull/163723))
- [Quant] extend the op list for quant lift up ([#163621](https://github.com/pytorch/pytorch/pull/163621))
- [inductor] fix: 'get_raw_stream' undefined ([#163707](https://github.com/pytorch/pytorch/pull/163707))
- [Inductor][FP8] Add op_name for ScaledMM TMA template heuristic ([#164019](https://github.com/pytorch/pytorch/pull/164019))
- [Inductor][ATen][FP8] Relax stride check for block-wise scaling when scaling dimension is 1 ([#163829](https://github.com/pytorch/pytorch/pull/163829))
- [Max Autotune][B200] Add addmm config to avoid test OOM ([#164020](https://github.com/pytorch/pytorch/pull/164020))
- [Easy] Add pointwise tag to fma ([#164149](https://github.com/pytorch/pytorch/pull/164149))
- [inductor] Small refactor of CachingAutotuner ([#162406](https://github.com/pytorch/pytorch/pull/162406))
- [submodule] upgrade cutlass version to 4.2.1 and completely resolved python/cutlass name collision ([#164156](https://github.com/pytorch/pytorch/pull/164156))
- [inductor] fx comm overlap: align runtime estimations across dist ranks ([#164226](https://github.com/pytorch/pytorch/pull/164226))
- [inductor][templates] Template hooks should be finalised inside a kernel context ([#164229](https://github.com/pytorch/pytorch/pull/164229))
- skip non memory deps in memory estimator ([#164294](https://github.com/pytorch/pytorch/pull/164294))
- Use dataclass features in two classes ([#164221](https://github.com/pytorch/pytorch/pull/164221))
- Fix non-TMA loads in grouped MM Triton kernel ([#163895](https://github.com/pytorch/pytorch/pull/163895))
- [inductor] Fix constant shape for float constants ([#164241](https://github.com/pytorch/pytorch/pull/164241))
- [inductor] log kernel autotuning result to a csv ([#164191](https://github.com/pytorch/pytorch/pull/164191))
- [cutass backend] remove cutlass presets ([#164380](https://github.com/pytorch/pytorch/pull/164380))
- Fix unbacked replacement where LHS is purely backed expr and RHS is unbacked expr ([#164013](https://github.com/pytorch/pytorch/pull/164013))
- [inductor] teach bisector to look at pre_grad passes ([#164250](https://github.com/pytorch/pytorch/pull/164250))
- [inductor] separate preamble from main work in compile_fx ([#164169](https://github.com/pytorch/pytorch/pull/164169))
- [inductor] Handle patterns where input/output nodes are the same ([#163994](https://github.com/pytorch/pytorch/pull/163994))
- Fix SAC + Flex issue ([#164421](https://github.com/pytorch/pytorch/pull/164421))
- [inductor] fix TestTemplateRender in select_algorithm ([#164158](https://github.com/pytorch/pytorch/pull/164158))
- Improved support for autotuning in wrapper_fxir ([#164132](https://github.com/pytorch/pytorch/pull/164132))
- Update mask dtype ([#164472](https://github.com/pytorch/pytorch/pull/164472))
- remove unnecessary registration ([#164481](https://github.com/pytorch/pytorch/pull/164481))
- Move call to output generated code in inductor ([#161615](https://github.com/pytorch/pytorch/pull/161615))
- config for dcache + unit tests ([#164512](https://github.com/pytorch/pytorch/pull/164512))
- Add missing TypeIs to torch/_inductor/ir.py ([#164489](https://github.com/pytorch/pytorch/pull/164489))
- [inductor] Enable triton kernels with unbacked inputs ([#164509](https://github.com/pytorch/pytorch/pull/164509))
- Add hop for additional control dependencies ([#164568](https://github.com/pytorch/pytorch/pull/164568))
- [BE][cutlass backend] BE changes post cutlass_cppgen name change ([#164589](https://github.com/pytorch/pytorch/pull/164589))
- torch.compile: Increase subprocess parent death check interval to lower cpu ([#164594](https://github.com/pytorch/pytorch/pull/164594))
- context + unit tests ([#164549](https://github.com/pytorch/pytorch/pull/164549))
- exceptions + unit tests ([#164550](https://github.com/pytorch/pytorch/pull/164550))
- fix copy_ for scalar in inductor ([#164167](https://github.com/pytorch/pytorch/pull/164167))
- [inductor] Improve bound on the number of dims to match for the block ([#163755](https://github.com/pytorch/pytorch/pull/163755))
- [multi-kernel] base tensor sizes for shape cache key ([#164499](https://github.com/pytorch/pytorch/pull/164499))
- [dynamic shapes] make backed_size_oblivious behavior consistent b/w symbolic_shapes/inductor ([#164796](https://github.com/pytorch/pytorch/pull/164796))
- utils + unit tests ([#164551](https://github.com/pytorch/pytorch/pull/164551))
- [inductor][codecache] Print bytes in codecache debug output ([#164898](https://github.com/pytorch/pytorch/pull/164898))
- locks + unit tests ([#164636](https://github.com/pytorch/pytorch/pull/164636))
- [inductor][templates] Distinguish between kernel input nodes and codegen input nodes ([#163752](https://github.com/pytorch/pytorch/pull/163752))
- [inductor] don't tune xblock for reduction ([#164801](https://github.com/pytorch/pytorch/pull/164801))
- [inductor] ban benchmarking by default in deterministic mode ([#164532](https://github.com/pytorch/pytorch/pull/164532))
- Refactor memory estimator to use node storages, add test ([#164783](https://github.com/pytorch/pytorch/pull/164783))
- Limit coll bucketing within node idxs ([#164944](https://github.com/pytorch/pytorch/pull/164944))
- Consider collective inputs to be deallocated only when wait is completed ([#164945](https://github.com/pytorch/pytorch/pull/164945))
- [RecSys][Combo Kernel] skip combo kernel generation if parition group is empty ([#164918](https://github.com/pytorch/pytorch/pull/164918))
- non-fb impls + unit tests ([#164722](https://github.com/pytorch/pytorch/pull/164722))
- [inductor] Remove Repeated Code in Subgraph ([#164892](https://github.com/pytorch/pytorch/pull/164892))
- [inductor] custom_graph_pass.get_hash_for_files: don't hash paths ([#165020](https://github.com/pytorch/pytorch/pull/165020))
- [easy][while_loop] use copy_input instead of clone in _clone_aliased_inputs ([#164955](https://github.com/pytorch/pytorch/pull/164955))
- Add Flash Attention support to FlexAttention ([#161118](https://github.com/pytorch/pytorch/pull/161118))
- Add Loads from fixed inputs ([#162031](https://github.com/pytorch/pytorch/pull/162031))
- Enable B904 check of flake8 ([#165047](https://github.com/pytorch/pytorch/pull/165047))
- Fix identity expansion ([#165066](https://github.com/pytorch/pytorch/pull/165066))
- [Inductor][ATen] Fix stride rounding on Blockwise128x128 to accommodate for small shapes ([#164953](https://github.com/pytorch/pytorch/pull/164953))
- Make LOCK_TIMEOUT in codecache configurable ([#165030](https://github.com/pytorch/pytorch/pull/165030))
- Add tlparse artifact for autotune_at_compile_time ([#164984](https://github.com/pytorch/pytorch/pull/164984))
- Fix loop pipelining for 2d/2d case of Triton grouped MM ([#165265](https://github.com/pytorch/pytorch/pull/165265))
- [pt2][caching] fix runtime error in context on cpu-only machine when compile for gpu ([#165220](https://github.com/pytorch/pytorch/pull/165220))
- [inductor][ez] properly print Pointwise ([#165369](https://github.com/pytorch/pytorch/pull/165369))
- [atomically_apply_size_hint] Make unbacked replacements reconciles to a single expr ([#164324](https://github.com/pytorch/pytorch/pull/164324))
- Make truediv numerics change external only for now ([#165328](https://github.com/pytorch/pytorch/pull/165328))
- [autobucketing] aten autobucketing fix to enable aot_eager pass ([#165063](https://github.com/pytorch/pytorch/pull/165063))
- [Inductor][ATen][FP8] Add note for supported blockwise scaling strategy pairs ([#165450](https://github.com/pytorch/pytorch/pull/165450))
- Add Memory Estimation Tracker ([#165059](https://github.com/pytorch/pytorch/pull/165059))
- [Graph Partition] fix partition x memory plan issue ([#165514](https://github.com/pytorch/pytorch/pull/165514))
- Gate division bitwise numerics under a flag ([#165566](https://github.com/pytorch/pytorch/pull/165566))
- [Fix XPU CI] [Inductor UT] Fix test cases broken by community. ([#165406](https://github.com/pytorch/pytorch/pull/165406))
- Codemod codecache.py from Optional to union none ([#165604](https://github.com/pytorch/pytorch/pull/165604))
- Codemod inductor/runtime from Optional to union none ([#165605](https://github.com/pytorch/pytorch/pull/165605))
- Codemod inductor/fx_passes from Optional to union none ([#165606](https://github.com/pytorch/pytorch/pull/165606))
- Add cse for make_block_ptr in Triton codegen ([#163399](https://github.com/pytorch/pytorch/pull/163399))
- [inductor][bucketing] Fx collectives bucketing of multiple dtypes ([#162470](https://github.com/pytorch/pytorch/pull/162470))
- [Inductor][Triton][FP8] Refactor scaled_mm template to accept scaling mode ([#164318](https://github.com/pytorch/pytorch/pull/164318))
- Use union syntax in torch/_inductor runtime and fx_passes ([#165652](https://github.com/pytorch/pytorch/pull/165652))
- use statically_known_leq & *=2 instead of bound_sympy in persistent rblock ([#165657](https://github.com/pytorch/pytorch/pull/165657))
- Fix bug with serialization after AOTAutogradCache hit ([#165474](https://github.com/pytorch/pytorch/pull/165474))
- [Inductor] Support fallback for all gemm like ops ([#165755](https://github.com/pytorch/pytorch/pull/165755))
- set unbacked bindings in reinplace pass for newly created nodes during generalize_scatter decomp ([#164948](https://github.com/pytorch/pytorch/pull/164948))
- [ROCm][inductor] heuristic improvements for pointwise kernels ([#163197](https://github.com/pytorch/pytorch/pull/163197))
- [Inductor][CuTeDSL] Move load_template up two directories ([#165868](https://github.com/pytorch/pytorch/pull/165868))
- [Graph Partition] fix graph partition input signature for fallback kernels ([#165815](https://github.com/pytorch/pytorch/pull/165815))
- fix bad merge duplicate pre pass ([#165917](https://github.com/pytorch/pytorch/pull/165917))
- A temporary fix to autotune out of range and related IMA ([#165943](https://github.com/pytorch/pytorch/pull/165943))
- [aoti][win] add support for a list of shim libraries ([#165914](https://github.com/pytorch/pytorch/pull/165914))
- Forward fix to D80948073 ([#166023](https://github.com/pytorch/pytorch/pull/166023))
- [inductor] turn Inductor deterministic mode on with torch.use_deterministic_algorithms ([#165950](https://github.com/pytorch/pytorch/pull/165950))
- [FlexAttention][CUDA] Add flex configs for Blackwell ([#165760](https://github.com/pytorch/pytorch/pull/165760))
- [Inductor] Restore original dtype for rank-0 CPU tensors ([#166118](https://github.com/pytorch/pytorch/pull/166118))
- [inductor][ez] fix score fusion memory typo ([#166029](https://github.com/pytorch/pytorch/pull/166029))
- [FlexFlash] update names ([#166193](https://github.com/pytorch/pytorch/pull/166193))
- [Inductor][Autotune] Gracefully restart the autotune process after ULF failure ([#166073](https://github.com/pytorch/pytorch/pull/166073))
- Mark FlexAttentionBackward as cacheable ([#165996](https://github.com/pytorch/pytorch/pull/165996))
- Remove JITFunction constexpr and some arg_names ([#166280](https://github.com/pytorch/pytorch/pull/166280))
- [Inductor] Make combo kernel MAX_NUM_ARGS configurable ([#166274](https://github.com/pytorch/pytorch/pull/166274))
- [inductor] a few workspace api change ([#166204](https://github.com/pytorch/pytorch/pull/166204))
- [inductor][ez] add overridable env var for disabling fx graph cache ([#166138](https://github.com/pytorch/pytorch/pull/166138))
- [reland] Add provenance to inductor IR nodes created after graph.run (#164255) ([#164746](https://github.com/pytorch/pytorch/pull/164746))
- [Inductor][Triton][FP8] Support deepseek-style scaling in Inductor ([#164404](https://github.com/pytorch/pytorch/pull/164404))
- [inductor] generate fused rms/layer norm bwd ([#165370](https://github.com/pytorch/pytorch/pull/165370))
- [FlexAttention] Add mechanism to get optimal autotune decision ([#165817](https://github.com/pytorch/pytorch/pull/165817))
- refactor: pull _replace_node common functionality out of Scheduler.finalize_multi_template_buffers ([#163368](https://github.com/pytorch/pytorch/pull/163368))
- [inductor] add in-kernel nan-check ([#166008](https://github.com/pytorch/pytorch/pull/166008))
- intfs + unit tests ([#164723](https://github.com/pytorch/pytorch/pull/164723))
- [Inductor] No longer throw error in bmm out_dtype lowering due to template heuristics ([#166457](https://github.com/pytorch/pytorch/pull/166457))
- [Inductor] Prevent kernel fusion with too many unique inputs and outputs ([#166275](https://github.com/pytorch/pytorch/pull/166275))
- [Graph Partition] move custom rules to inductor config ([#166458](https://github.com/pytorch/pytorch/pull/166458))
- helper function for replacing nodes in aug graph ([#166309](https://github.com/pytorch/pytorch/pull/166309))
- Add waitcounters for torch.compile subprocess pool ([#164527](https://github.com/pytorch/pytorch/pull/164527))
- [xpu][test] Reuse native_mm and mix_order_reduction for Intel GPU. ([#166384](https://github.com/pytorch/pytorch/pull/166384))
- [inductor][mi350] add tech specs for MI350 ([#166576](https://github.com/pytorch/pytorch/pull/166576))
- [xpu][fix] [Inductor] Avoid using tl.sqrt_rn on XPU before triton is ready ([#165740](https://github.com/pytorch/pytorch/pull/165740))
- [Inductor] Fix an inductor_provenance bug ([#166432](https://github.com/pytorch/pytorch/pull/166432))
- [wip] fix searchsorted non dense ([#165064](https://github.com/pytorch/pytorch/pull/165064))
- [inductor] Fix constant folder ([#166655](https://github.com/pytorch/pytorch/pull/166655))
- [inductor] Mark / restrict tests that only work if ATen is used for matmul ([#166518](https://github.com/pytorch/pytorch/pull/166518))
- [PT2] set choice handler in config ([#166607](https://github.com/pytorch/pytorch/pull/166607))
- Add regional aot eager support to AOTAutogradCacheEntry ([#166650](https://github.com/pytorch/pytorch/pull/166650))
- compile_worker: Make a timer class ([#166465](https://github.com/pytorch/pytorch/pull/166465))
- [inductor] pre grad graph bisecting ([#166344](https://github.com/pytorch/pytorch/pull/166344))
- [inductor] track reduction before splitting ([#166053](https://github.com/pytorch/pytorch/pull/166053))
- [inductor] more aggressive mix order reduction ([#166382](https://github.com/pytorch/pytorch/pull/166382))
- [inductor] Make mix-order-reduction split size not depends on split-reduction heuristics ([#166461](https://github.com/pytorch/pytorch/pull/166461))
- [Inductor] mix order reduction heuristics and tuning ([#166585](https://github.com/pytorch/pytorch/pull/166585))
- [inductor] Collectives estimations: option to use nccl estimator for fx node ([#166521](https://github.com/pytorch/pytorch/pull/166521))
- [Inductor][Triton][FP8] Support tile-wise (1x128) scaling in Inductor ([#165132](https://github.com/pytorch/pytorch/pull/165132))
- [XPU][Fix] Register convolution_overrideable for flops count ([#166839](https://github.com/pytorch/pytorch/pull/166839))
- [xpu][feature][inductor] Enable decompose_mm_pass and UT on Intel GPU ([#166613](https://github.com/pytorch/pytorch/pull/166613))
- [inductor] do not hard fail on FakePG with nccl estimator ([#166869](https://github.com/pytorch/pytorch/pull/166869))
- subproc_pool: Add support for enabling quiesce via a timer ([#166467](https://github.com/pytorch/pytorch/pull/166467))
- [inductor] coordesc not tune XBLOCK for mix-order-reduction ([#166669](https://github.com/pytorch/pytorch/pull/166669))
- [inductor] runtime estimations disable use_nccl_estimator by default ([#166973](https://github.com/pytorch/pytorch/pull/166973))
- [Minor][Inductor] move some combo kernel log from warning to debug ([#166993](https://github.com/pytorch/pytorch/pull/166993))
- [inductor][AMD] Filter out invalid Triton Configs for MI350X _scaled_mm ([#166442](https://github.com/pytorch/pytorch/pull/166442))
- use_cpp_bmm_template supports more use cases ([#165469](https://github.com/pytorch/pytorch/pull/165469))
- Check cluster_dims attribute exists before access ([#167187](https://github.com/pytorch/pytorch/pull/167187))
- [inductor] Use runtime estimations in iterative reorder collectives pass ([#167080](https://github.com/pytorch/pytorch/pull/167080))
- [inductor] Use runtime estimations in iterative sink waits pass ([#167081](https://github.com/pytorch/pytorch/pull/167081))
- [inductor] fix dashbaord regression due to mix order reduction ([#166938](https://github.com/pytorch/pytorch/pull/166938))
- [inductor] let mix-order-red tune XBLOCK and num-stages ([#167161](https://github.com/pytorch/pytorch/pull/167161))
- [pallas backend] use dlpack directly ([#167243](https://github.com/pytorch/pytorch/pull/167243))
- [OC][Torch] Extend autotune options for OC OBA 200x shapes ([#166931](https://github.com/pytorch/pytorch/pull/166931))
- [Inductor][Tritonparse] Ensure inductor meta has config_args ([#167261](https://github.com/pytorch/pytorch/pull/167261))
- fix alpha beta in decomp ([#167317](https://github.com/pytorch/pytorch/pull/167317))
- inductor: compile_worker - Fix potential race condition with quiesce waitcounters ([#167025](https://github.com/pytorch/pytorch/pull/167025))
- subproc_pool: Fix quiesce waitcounter ([#167350](https://github.com/pytorch/pytorch/pull/167350))
- Additional fix on top of D85172267 (#167267) ([#167279](https://github.com/pytorch/pytorch/pull/167279))
- [Inductor] Decouple flags for optimization and debug symbols ([#167385](https://github.com/pytorch/pytorch/pull/167385))
- [pallas backend] add cpu backend and parametrize the tests ([#167388](https://github.com/pytorch/pytorch/pull/167388))
- [Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel ([#167340](https://github.com/pytorch/pytorch/pull/167340))
- [inductor] Wrap pallas_call in jax.jit ([#167441](https://github.com/pytorch/pytorch/pull/167441))
- [inductor][determinism] type errors + use odc to dump imc on exit ([#167136](https://github.com/pytorch/pytorch/pull/167136))
- allow sym_stride, and sym_size lowering in inductor to return ints ([#167345](https://github.com/pytorch/pytorch/pull/167345))
- [simplefsdp] fix autobucketing pass that takes comm op as input ([#167484](https://github.com/pytorch/pytorch/pull/167484))
- [pallas backend] Implementing Strided/Scatter Access ([#167426](https://github.com/pytorch/pytorch/pull/167426))
- [pallas backend] implement complex indexing ([#167493](https://github.com/pytorch/pytorch/pull/167493))
- [inductor] Fix constant creation ([#167398](https://github.com/pytorch/pytorch/pull/167398))
- fix sym_size_, sym_stride lowering ([#167565](https://github.com/pytorch/pytorch/pull/167565))
- [Inductor][2/2] Decouple flags for optimization and debug symbols ([#167575](https://github.com/pytorch/pytorch/pull/167575))
- [inductor][NFC][2/X] extract do_autotuning/autotune/benchmark from AlgorithmSelectorCache.__call__ ([#167489](https://github.com/pytorch/pytorch/pull/167489))
- Only remove_noop in pre_grad passes if remove_noop is not in the remove_passes_list ([#167479](https://github.com/pytorch/pytorch/pull/167479))
- [inductor][ez] skip cache for unit test via envvar ([#167237](https://github.com/pytorch/pytorch/pull/167237))
- add assume_32bit_indexing inductor config ([#167784](https://github.com/pytorch/pytorch/pull/167784))
- [inductor] layout constraint for weight-norm-bwd ([#167667](https://github.com/pytorch/pytorch/pull/167667))
- [pallas backend] implement gpu tiles/mask for power of 2 ([#167584](https://github.com/pytorch/pytorch/pull/167584))
- Enable PyTorch OSS numerics changes, inductor heuristics ([#167799](https://github.com/pytorch/pytorch/pull/167799))
- [inductor] Expose config for fx bucket all_reduces ([#167634](https://github.com/pytorch/pytorch/pull/167634))
- Cleanup in inductor usage of nccl estimator after its fix ([#167633](https://github.com/pytorch/pytorch/pull/167633))
- [inductor] unittest for run2run determinism ([#167482](https://github.com/pytorch/pytorch/pull/167482))
- [pallas backend] remove unnecessary mypy comment ([#167954](https://github.com/pytorch/pytorch/pull/167954))
- [inductor][fix] subproc autotuning respect cache dir changes ([#167918](https://github.com/pytorch/pytorch/pull/167918))
- Introduce HOP for inductor compiled regions to allow torch dispatch ([#167844](https://github.com/pytorch/pytorch/pull/167844))
- [pallas backend] implement complex numbers ([#167947](https://github.com/pytorch/pytorch/pull/167947))
- [pallas backend] implement more ops ([#167951](https://github.com/pytorch/pytorch/pull/167951))
- [pallas backend] support reductions ([#167953](https://github.com/pytorch/pytorch/pull/167953))
- [xpu][feature][inductor] Enable pad_mm Pass on Intel GPU ([#166618](https://github.com/pytorch/pytorch/pull/166618))
- Fix inductor collective runtime units ([#168055](https://github.com/pytorch/pytorch/pull/168055))
- [FlexFlash] Specify lowering w/ new `BACKEND` kernel option ([#168017](https://github.com/pytorch/pytorch/pull/168017))
- [FlexFlash] Blackwell fwd support ([#167040](https://github.com/pytorch/pytorch/pull/167040))
- Reland"Fix different seq length (#167481)" ([#168144](https://github.com/pytorch/pytorch/pull/168144))
- Move pointwise_scatter optimization to joint_graph stage from post_grad ([#165463](https://github.com/pytorch/pytorch/pull/165463))
- [Inductor] Freeze layout for potentially padded strides in template autotuning ([#168032](https://github.com/pytorch/pytorch/pull/168032))
- [submodule][inductor]Fix an AMD CPU max-autotune breakage ([#168079](https://github.com/pytorch/pytorch/pull/168079))
- [inductor] make mix order reduction work with dynamic shapes ([#168117](https://github.com/pytorch/pytorch/pull/168117))
- [BE][Inductor] Move mm templates into separate files ([#168179](https://github.com/pytorch/pytorch/pull/168179))
- better use of mem tracking ([#168121](https://github.com/pytorch/pytorch/pull/168121))
- [simplefsdp] fix simplefsdp llama3 run ([#168311](https://github.com/pytorch/pytorch/pull/168311))
- [FlexFlash] Add wiring for backwards ([#168319](https://github.com/pytorch/pytorch/pull/168319))
- [inductor] find benchmark scripts for r2r determinism unit test ([#168041](https://github.com/pytorch/pytorch/pull/168041))
- [Inductor] Properly enlarge XBLOCK/set num_warps=1 for B200 inner persistent reductions ([#168335](https://github.com/pytorch/pytorch/pull/168335))
- [Inductor] Mix Order Reduction Heuristics ([#168361](https://github.com/pytorch/pytorch/pull/168361))
- feat(pallas): add Pallas TPU backend ([#167774](https://github.com/pytorch/pytorch/pull/167774))
- [Flex] Fix symbolic shapes lowering ([#168383](https://github.com/pytorch/pytorch/pull/168383))
- [inductor] fix picking wrong contiguous node ([#168371](https://github.com/pytorch/pytorch/pull/168371))
- [inductor] Add option to transpose tensor descriptors on load / store ([#165541](https://github.com/pytorch/pytorch/pull/165541))
- [reland][inductor] fix the decision of inner reduction ([#168391](https://github.com/pytorch/pytorch/pull/168391))
- [Inductor] [Triton] Capture Timeout errors without crashing the job ([#169064](https://github.com/pytorch/pytorch/pull/169064))
- [pallas backend] Add multi-output support to Pallas backend ([#169323](https://github.com/pytorch/pytorch/pull/169323))
- [pallas backend] support bitcast ([#169324](https://github.com/pytorch/pytorch/pull/169324))
- [inductor] Increase tolerance for test_emulate_precision_casts_mean_ratio_chain ([#169309](https://github.com/pytorch/pytorch/pull/169309))
- [inductor] Increase tolerance for test_conv3d_binary_broadcast_shapes ([#169310](https://github.com/pytorch/pytorch/pull/169310))
- Add shape logging to autotuning to debug timeout issues ([#169062](https://github.com/pytorch/pytorch/pull/169062))
- [CUDA][FlexAttention] Use `sm8x` configs for `sm12x` for backward ([#168367](https://github.com/pytorch/pytorch/pull/168367))
- fix for using cross-pg overlap ([#169384](https://github.com/pytorch/pytorch/pull/169384))
- Triton 3.5+fb: Drop the legacy support for autoWS ([#169089](https://github.com/pytorch/pytorch/pull/169089))
- [reland][Full Inductor][Pytorch] Prevent decomposition to support fallback of aten.native_layer_norm for MTIA ([#168986](https://github.com/pytorch/pytorch/pull/168986))
- [Inductor] Fix Diode / exhaustive autotune crash on AMD ([#169225](https://github.com/pytorch/pytorch/pull/169225))
- Fix device determination logic in Conditional ([#169199](https://github.com/pytorch/pytorch/pull/169199))
- Correctly set max_numwarps in coordinate_descent_tuner ([#159146](https://github.com/pytorch/pytorch/pull/159146))
- [pallas backend] fix breakages ([#169406](https://github.com/pytorch/pytorch/pull/169406))
- [hoo] Invoke subgraph + effects Inductor support ([#167364](https://github.com/pytorch/pytorch/pull/167364))
- [pallas backend] Add special function support to Pallas backend ([#169422](https://github.com/pytorch/pytorch/pull/169422))
- fix typo: ommunication -> communication ([#169557](https://github.com/pytorch/pytorch/pull/169557))
- Switch off of deprecaated API ([#169517](https://github.com/pytorch/pytorch/pull/169517))
- [inductor] skip the r2r determ test in fbcode ([#169074](https://github.com/pytorch/pytorch/pull/169074))
- [pallas backend] Fix Pallas backend mixed indexing for multi-dimensional outputs ([#169651](https://github.com/pytorch/pytorch/pull/169651))
- [pallas backend] Fix reduction and store handling for non-contiguous axes ([#169653](https://github.com/pytorch/pytorch/pull/169653))
- Fix the fixme-s in grouped MM Triton kernel ([#168980](https://github.com/pytorch/pytorch/pull/168980))
- Add Hash func to avoid expensive cpu side check ([#169601](https://github.com/pytorch/pytorch/pull/169601))
- [pallas backend] Fix reductions, squeeze, and add argmax/argmin support ([#169654](https://github.com/pytorch/pytorch/pull/169654))
- Add an api for scheduling overlap from inductor configs ([#169693](https://github.com/pytorch/pytorch/pull/169693))
- [inductor] do mix order reduction fusion earlier and allow fusing more nodes into it ([#168209](https://github.com/pytorch/pytorch/pull/168209))
- [Windows] Fix SyntaxError: truncated \UXXXXXXXX escape in generated Compile-time auto-tuning block. ([#169286](https://github.com/pytorch/pytorch/pull/169286))
- [Inductor][CPU] GEMM template: add an AVX512-VNNI-based micro kernel ([#166846](https://github.com/pytorch/pytorch/pull/166846))
- [inductor] Quiesce triton compile workers by default ([#169485](https://github.com/pytorch/pytorch/pull/169485))
- [xpu][inductor] update Intel Triton for PyTorch release 2.10 ([#168950](https://github.com/pytorch/pytorch/pull/168950))
- [Inductor] Create benchmark request for Extern Kernels ([#169605](https://github.com/pytorch/pytorch/pull/169605))
- [pallas backend] Fix DLPack export error for tensors with requires_grad in Pallas backend ([#169804](https://github.com/pytorch/pytorch/pull/169804))
- Filter control deps while calculating lifetime ([#169496](https://github.com/pytorch/pytorch/pull/169496))
- Check if all required tensor_meta exist ([#169066](https://github.com/pytorch/pytorch/pull/169066))
- [xpu][test] reduce xpu disabled cases (1/N) ([#167786](https://github.com/pytorch/pytorch/pull/167786))
- Issue 146167 inductor type hints lowering 1 ([#159861](https://github.com/pytorch/pytorch/pull/159861))
- [inductor] overlap scheduling nodes benchmarking logs and fix ([#169947](https://github.com/pytorch/pytorch/pull/169947))
- [pallas backend] implement truncdiv and floordiv ([#169993](https://github.com/pytorch/pytorch/pull/169993))
- [cutedsl] clean up codegen hooks to handle views ([#169906](https://github.com/pytorch/pytorch/pull/169906))
### security
