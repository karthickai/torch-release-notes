
# Release Notes worksheet skip

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## skip
### bc breaking
- [BE] Update Python min version to 3.10 ([#162310](https://github.com/pytorch/pytorch/pull/162310))
- [BE] Update Python min version to 3.10 ([#162310](https://github.com/pytorch/pytorch/pull/162310))
- [BE] Update Python min version to 3.10 ([#162310](https://github.com/pytorch/pytorch/pull/162310))
### deprecation
### new features
### improvements
### bug fixes
- Make PT2 compile backprop through custom op without autograd key a hard error ([#166367](https://github.com/pytorch/pytorch/pull/166367))
- Make PT2 compile backprop through custom op without autograd key a hard error ([#166367](https://github.com/pytorch/pytorch/pull/166367))
- Make PT2 compile backprop through custom op without autograd key a hard error ([#166367](https://github.com/pytorch/pytorch/pull/166367))
### performance
### docs
### devs
### Untopiced
- fix torch.sparse.log_softmax on CPU ([#161959](https://github.com/pytorch/pytorch/pull/161959))
- Revert "fix torch.sparse.log_softmax on CPU (4dd73e659a8)
- Revert "Use same NVSHMEM version across CUDA builds (5ccf3ca3ec8)
- python fastpath for DTensor detach(), confirm that aliasing DTensorSpec is ok ([#160580](https://github.com/pytorch/pytorch/pull/160580))
- Revert "[MPS] mps sparse mul op implementation (82f1eb9b031)
- [dynamo] Graph break on on user-defined class in compiled region ([#161670](https://github.com/pytorch/pytorch/pull/161670))
- Revert "[dynamo] Graph break on on user-defined class in compiled region (a3e26d17274)
- test fixing benchmarks ([#162503](https://github.com/pytorch/pytorch/pull/162503))
- Put torchao (0.13.0) back to benchmark workflow ([#162227](https://github.com/pytorch/pytorch/pull/162227))
- [dynamo] Graph break on on user-defined class in compiled region ([#161670](https://github.com/pytorch/pytorch/pull/161670))
- Fix DCE eliminating in-place operations by improving Node.is_impure() ([#162267](https://github.com/pytorch/pytorch/pull/162267))
- Revert "test fixing benchmarks (e1f0a699439)
- Fix decorators skipping NCCL tests  ([#158846](https://github.com/pytorch/pytorch/pull/158846))
- Revert "Fix DCE eliminating in-place operations by improving Node.is_impure() (fc1b09a52ab)
- [nn] Assert parsed iterable arguments are an appropriate length ([#162340](https://github.com/pytorch/pytorch/pull/162340))
- Build and Install Arm Compute Library in manylinux docker image ([#159737](https://github.com/pytorch/pytorch/pull/159737))
- [torch][c10d] fix split_group in mixed backend case ([#162424](https://github.com/pytorch/pytorch/pull/162424))
- Revert "Put torchao (0.13.0) back to benchmark workflow (80d4da893cf)
- Revert "[torch][c10d] fix split_group in mixed backend case (d033d11d26b)
- Revert "Make functorch notebook symlinks PEP 517 valid (053251b98da)
- Revert "Fix decorators skipping NCCL tests  (40ea6e418a3)
- Revert "Move inductor jobs 3.9->3.10 (23170dfebcd)
- fix torch.sparse.log_softmax on CPU ([#161959](https://github.com/pytorch/pytorch/pull/161959))
- Revert "Build and Install Arm Compute Library in manylinux docker image (9f783e172d7)
- Revert "Move prioritized text linker optimization code from setup.py to cmake (94db2ad51d4)
- [torch][c10d] fix split_group in mixed backend case ([#162424](https://github.com/pytorch/pytorch/pull/162424))
- Revert "[inductor][choices] rename get_mm_configs to get_template_configs (cef05b1202b)
- Revert "[inductor] leverage template stacking in V.choices.get_mm_configs (934f8788838)
- Revert "[inductor] FlexibleLayout for ExternKernelChoice for mms (1c6dfbe5578)
- Revert "[nn] Assert parsed iterable arguments are an appropriate length (468c1f9e9d9)
- [Triton] [Inductor] Restrict subprocess autotuning to just Triton ([#162688](https://github.com/pytorch/pytorch/pull/162688))
- Revert "[2/N]Port several test files under test/distributed to Intel GPU (92f9ed7ac37)
- [indexing] Prevent integer overflow from large step values in C++ ([#161707](https://github.com/pytorch/pytorch/pull/161707))
- Revert "AMD CPU CI - Add freezing + fix label trigger (222ec8d28ec)
- [RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) ([#162594](https://github.com/pytorch/pytorch/pull/162594))
- Revert "[indexing] Prevent integer overflow from large step values in C++ (00e9ba75cde)
- Revert "[RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) (6b59a19242e)
- [DeviceMesh] Make CuTe layout as mesh layout to be ready for using in DeviceMesh ([#162414](https://github.com/pytorch/pytorch/pull/162414))
- [Reland] Use std::string_view in torchgen ([#158625](https://github.com/pytorch/pytorch/pull/158625))
- [RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) ([#162594](https://github.com/pytorch/pytorch/pull/162594))
- Revert "[DeviceMesh] Make CuTe layout as mesh layout to be ready for using in DeviceMesh (b5f4a7dc141)
- Revert "[MTIA Runtime] Add foreach_div ops to native_functions.yaml (cdfa298a3b3)
- [DCP] Decrease checkpoint background process Gloo pg init timeout ([#162760](https://github.com/pytorch/pytorch/pull/162760))
- Revert "python fastpath for DTensor detach(), confirm that aliasing DTensorSpec is ok (7dd5f7b1256)
- Revert "port some distributed tensor test files for Intel GPU (31040b6357e)
- Revert "Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available, or cpu-only build (9c93dc81230)
- Revert "[Reland] Use std::string_view in torchgen (deb7ebe0a39)
- [DeviceMesh] Make CuTe layout as mesh layout to be ready for using in DeviceMesh ([#162414](https://github.com/pytorch/pytorch/pull/162414))
- Revert "[lint][CI] Don't checkout submodules for lintrunner-noclang (fa919feab6a)
- Revert "[BE] Make PyObjectSlot use a global PyInterpreter (4db203f8759)
- Revert "[dynamo][hop] Introduce Local Map HOP (e7c3f802ffa)
- remove unnecessary sync point in AveragedModel update ([#158017](https://github.com/pytorch/pytorch/pull/158017))
- Revert "Set the credential to upload vLLM nightly wheels on schedule and workflow_dispatch (d4554bc2846)
- Revert "[Triton] [Inductor] Restrict subprocess autotuning to just Triton (e13cf68d034)
- Revert "remove unnecessary sync point in AveragedModel update (a5419743c64)
- Revert "[Reland] Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available, or cpu-only build (79fd4974239)
- Revert "[CI] Update NVIDIA driver to `580.82.07` (4ca3f435fbf)
- Revert "[torch][cuda][device_limits] Library for querying device hardware limits for flops and bandwidth (4b7aed89d82)
- Revert "[BE] Update Python min version to 3.10 (578047838cc)
- Revert "[CI] Move Windows build/tests to Python-3.10 (17081209e5a)
- Revert "[dynamo][guards] Do not construct entire framelocals dict for LAMBDA_GUARD (1302637a230)
- Revert "[dynamo][guards] Fail on an unknown framelocals to dict conversion (32ad29b72a7)
- Revert "Fix boxcox to return same result for same input in one batch (a3b68c7c57a)
- Revert "Improve device info with new flops and bandwidth formula based on hardware libraries (2a308c7dee7)
- [SymmMem] Promote `@requires_nvshmem` instead of `enable_triton` ([#163423](https://github.com/pytorch/pytorch/pull/163423))
- Revert "[SymmMem] Promote `@requires_nvshmem` instead of `enable_triton` (3a7db34cf91)
- Revert "[RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) (f0078941cf4)
- Revert "Delete functorch C extension entirely. (ae5be038a63)
- Revert "[BE] Make PyObjectSlot use a global PyInterpreter (edafc902d76)
- Revert "[BE] Update Python min version to 3.10 (10adeb9044a)
- [opaque_obj] Add set_payload + docs ([#163276](https://github.com/pytorch/pytorch/pull/163276))
- Revert "[opaque_obj] Add set_payload + docs (eaa613bf66e)
- [opaque_obj] Add set_payload + docs ([#163276](https://github.com/pytorch/pytorch/pull/163276))
- [RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) ([#162594](https://github.com/pytorch/pytorch/pull/162594))
- Revert "Update cutlass version for fbcode (19b754dff8e)
- Revert "Add fake_impl for _native_multi_head_attention (aff76c046d6)
- Revert "[precompile] Add option to disable guard check on aot-compiled function. (221ac810436)
- [Triton] [Inductor] Restrict subprocess autotuning to just Triton ([#162688](https://github.com/pytorch/pytorch/pull/162688))
- [CUDA] Compare major version of the runtime device arch against the built version of the pytorch binary ([#161299](https://github.com/pytorch/pytorch/pull/161299))
- Revert "[RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889) (00059db0345)
- Revert "[CUDA] Compare major version of the runtime device arch against the built version of the pytorch binary (112e2047978)
- Revert "Update ruff to 0.13.1 (7bad9c5a642)
- Revert "Add less warps config to inner reductions (bfd21cd3e63)
- Add magic TORCH_MAKE_PYBIND_ENUM_FASTER macro ([#163527](https://github.com/pytorch/pytorch/pull/163527))
- [inductor] require shape in TritonCSEVariable ([#162275](https://github.com/pytorch/pytorch/pull/162275))
- [fx] Allow customization of submod name in split graph ([#164035](https://github.com/pytorch/pytorch/pull/164035))
- Revert "[inductor] require shape in TritonCSEVariable (6b473c90cfc)
- Revert "[dynamo] Special path for cloning of torch dispatch tensors (6650f5af74c)
- Revert "[export] Skip the check instead of disable (9e792f583af)
- Revert "Helper to augment graph with additional deps (84dc54ae5e8)
- Revert "refactor bucketing (b28e4f1f874)
- Revert "[inductor] do comm compute overlap at aten fx level (0f619c1f893)
- [PP] Customize pipeline's submod name ([#164037](https://github.com/pytorch/pytorch/pull/164037))
- CUDACachingHostAllocatorImpl skip event query during capture ([#164001](https://github.com/pytorch/pytorch/pull/164001))
- Revert "Enable outer reductions in fbcode (ca19815e3cb)
- Add functions to setup PrivateUse1 as a python backend device. ([#157859](https://github.com/pytorch/pytorch/pull/157859))
- Revert "Add functions to setup PrivateUse1 as a python backend device. (410ed3006b0)
- Revert "Add less warps config to inner reductions (71b4fada576)
- Revert "[CI] Push `viable/strict/${time}` tags (79fcfd49d66)
- Revert "Consistently use c10_ovrsource in arvr mode everywhere (0fb89b84b92)
- Revert "[BE] Remove HermeticPyObjectTLS and Simplify PythonOpRegistrationTrampoline (cc5d74c366e)
- Build and Install Arm Compute Library in manylinux docker image ([#159737](https://github.com/pytorch/pytorch/pull/159737))
- Revert "CUDACachingHostAllocatorImpl skip event query during capture (07d896fa487)
- Revert "[PP] Customize pipeline's submod name (36a37b81cdf)
- Revert "[fx] Allow customization of submod name in split graph (59a86cb137d)
- Revert "Add num_store to inductor_meta and use it to scale persistent reduction x block (20edc5b26a1)
- [fx] Allow customization of submod name in split graph ([#164035](https://github.com/pytorch/pytorch/pull/164035))
- [PP] Customize pipeline's submod name ([#164037](https://github.com/pytorch/pytorch/pull/164037))
- Revert "[dynamo, 3.14] fix _detect_and_normalize_assert_statement for 3.14 (69c5c08a015)
- Add functions to setup PrivateUse1 as a python backend device. ([#157859](https://github.com/pytorch/pytorch/pull/157859))
- Revert "[DCP] Decrease checkpoint background process Gloo pg init timeout (a10207e61be)
- C++-accessible Placements via pybind11 ([#163030](https://github.com/pytorch/pytorch/pull/163030))
- Revert "Improve repeat op to a single copy (7cfecd76b21)
- [DeviceMesh] Simplifying internal bookkeeping with CuTe layout ([#163213](https://github.com/pytorch/pytorch/pull/163213))
- Revert "Add magic TORCH_MAKE_PYBIND_ENUM_FASTER macro (c6329524d86)
- Revert "Stop parsing command line arguments every time common_utils is imported. (39189592fd6)
- Revert "[vision hash update] update the pinned vision hash (0319556a35b)
- Revert "Use TMA loads always for Triton grouped MM kernel (6bb021c1255)
- Revert "C++-accessible Placements via pybind11 (f6f76767563)
- Revert "Speed up FP precision lookup (2a7c4867501)
- [inductor] require shape in TritonCSEVariable ([#162275](https://github.com/pytorch/pytorch/pull/162275))
- Revert "Add provenance to inductor IR nodes created after graph.run (a34797e0317)
- Revert "[DeviceMesh] Simplifying internal bookkeeping with CuTe layout (22e219d9969)
- Support setting grad_dtype on leaf tensors ([#162815](https://github.com/pytorch/pytorch/pull/162815))
- multimem reduce ([#164517](https://github.com/pytorch/pytorch/pull/164517))
- [DeviceMesh] Simplifying internal bookkeeping with CuTe layout ([#163213](https://github.com/pytorch/pytorch/pull/163213))
- Revert "[CUDA] Add experimental green context support for SM carveout (8ec8c14aced)
- Revert "[inductor] require shape in TritonCSEVariable (0b4f2b46d9e)
- Revert "Support setting grad_dtype on leaf tensors (3ddf2018d0b)
- Fix refine_ranges corner case ([#164075](https://github.com/pytorch/pytorch/pull/164075))
- Add device argument to torch.random.get_rng_state ([#163034](https://github.com/pytorch/pytorch/pull/163034))
- Revert "Make custom op alias check consistent (6f6a9193666)
- Revert "Add pure view support in autograd Function (f46bb04dcc3)
- Revert "Add device argument to torch.random.get_rng_state (2e1742dd63c)
- [2/N] Fix clang-tidy readability checks  ([#164652](https://github.com/pytorch/pytorch/pull/164652))
- Enable all SIM rules except disabled ones ([#164645](https://github.com/pytorch/pytorch/pull/164645))
- Revert "Enable all SIM rules except disabled ones (5d7360bb033)
- Revert "[2/N] Fix clang-tidy readability checks  (2c5ed6e7c06)
- Revert "[BE] Make PyObjectSlot use a global PyInterpreter (331191ce4b2)
- [2/N] Fix clang-tidy readability checks  ([#164652](https://github.com/pytorch/pytorch/pull/164652))
- [dynamo] Support torch.fx.traceback.annotate ([#164678](https://github.com/pytorch/pytorch/pull/164678))
- Revert "[dynamo] Support torch.fx.traceback.annotate (cfc5cc17dc4)
- Revert "Fix refine_ranges corner case (3912ba3e940)
- Reapply "C++-accessible Placements via pybind11 (#163030)" ([#164519](https://github.com/pytorch/pytorch/pull/164519))
- Revert "Numpy zerotensor handling (1fc71d1b578)
- Revert "Fix mesh.get_local_rank when it is > 1d (afee8062d51)
- [dynamo] Support torch.fx.traceback.annotate ([#164678](https://github.com/pytorch/pytorch/pull/164678))
- Revert "Reapply "C++-accessible Placements via pybind11 (#163030)" (df640df68a5)
- Revert "multimem reduce (f505caa71bd)
- Revert "[CUDA] Add experimental green context support for SM carveout (1e42fde45ef)
- Reland vision pinned commit hash update ([#164492](https://github.com/pytorch/pytorch/pull/164492))
- Fix double dispatch to Python for detach ([#163671](https://github.com/pytorch/pytorch/pull/163671))
- Revert "Enable all flake8-logging-format rules (f713abab16c)
- fix flex attention eager bwd: more rounding ([#164317](https://github.com/pytorch/pytorch/pull/164317))
- Revert "Fix double dispatch to Python for detach (97463d4cf3c)
- Revert "[dynamo] Support torch.fx.traceback.annotate (3040a5d294b)
- list_stored_sd_metadata API. ([#160610](https://github.com/pytorch/pytorch/pull/160610))
- Revert "Reland vision pinned commit hash update (1927783aa3a)
- multimem reduce ([#164517](https://github.com/pytorch/pytorch/pull/164517))
- Revert "fix flex attention eager bwd: more rounding (20082d71366)
- Revert "Add memory estimator (f8d0d65ddc6)
- Revert "Limit path search within range (b5e93ffdcf7)
- Revert "list_stored_sd_metadata API. (fd4bde430a5)
- C++ API handle optimizer defaults  ([#161825](https://github.com/pytorch/pytorch/pull/161825))
- [dynamo] Support torch.fx.traceback.annotate ([#164678](https://github.com/pytorch/pytorch/pull/164678))
- [export] Turn on install_free_tensors flag ([#164691](https://github.com/pytorch/pytorch/pull/164691))
- Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed ([#164939](https://github.com/pytorch/pytorch/pull/164939))
- Revert "Use runner with more memory for ASAN builds (a753ffa9aff)
- Revert "Fix Avoid DDE in item numel check (5209c8ce070)
- Revert "[vllm hash update] update the pinned vllm hash (5b8174bc286)
- Revert "Fix truediv numerics between eager and compile (e09fb44ef17)
- AOTI MPS Shim Implementation ([#163865](https://github.com/pytorch/pytorch/pull/163865))
- Revert "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (06d86e58d03)
- Revert "AOTI MPS Shim Implementation (4412026949b)
- Revert "[CUDA][cuBLAS] addmm -- some refactoring for easier navigation between the Lt and non-Lt paths (f79e212733c)
- fix flex attention eager bwd: more rounding ([#164317](https://github.com/pytorch/pytorch/pull/164317))
- Revert "[BC-Breaking] Remove long-deprecated casting functions from native_functions.yaml (3d1fa40ae1f)
- AOTI MPS Shim Implementation ([#163865](https://github.com/pytorch/pytorch/pull/163865))
- Revert "[Code Clean] Remove support of python3.9 (91040f49348)
- Revert "Enable mimalloc on non-Windows platforms and make default for AArch64 builds (688efd9741d)
- [ATen] Fix CUDA reduction warp shuffle order ([#164790](https://github.com/pytorch/pytorch/pull/164790))
- Revert "Fix truediv numerics between eager and compile (ed2d514ad86)
- Revert "[ATen] Fix CUDA reduction warp shuffle order (4d7f9f3aed6)
- Revert "Call internal log_compilation_event if it exists (47956196d99)
- Revert "[export] Turn on install_free_tensors flag (34ac9b61cbf)
- Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed ([#164939](https://github.com/pytorch/pytorch/pull/164939))
- Revert "Add SVE128 ISA (7614338b694)
- Revert "[CD] Do not propagate download.pytorch.org IP into container (daea35df5c1)
- Revert "Hotfix test scaled matmul cuda (7ab00c7c178)
- Revert "[inductor][eazy] change how torch.use_deterministic_algorithms affect inductor (df514a6d5a4)
- [2/N] More ruff SIM fixes ([#165031](https://github.com/pytorch/pytorch/pull/165031))
- Revert "[inductor] verify determinism with inductor benchmark script (d2cb1833445)
- Revert "[2/N] More ruff SIM fixes (b8be796a57f)
- Revert "[AMP][Refactor] Simplify dtype support logic in autocast context manager (9420944033a)
- Revert "Fix truediv numerics between eager and compile (abb2f7179ee)
- Warn if AccumulateGrad stream does not match producer node stream ([#165065](https://github.com/pytorch/pytorch/pull/165065))
- Revert "Warn if AccumulateGrad stream does not match producer node stream (f975bd58af0)
- Revert "C++ API handle optimizer defaults  (b67785d9ebd)
- Revert "[dynamo][executorch] Do not trace into exeuctorch LoweredBackendModule (d16627f4d0a)
- Revert "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (5c3fe9fb302)
- [opaque_obj_v2] PyObject custom op schema type ([#165004](https://github.com/pytorch/pytorch/pull/165004))
- Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed ([#164939](https://github.com/pytorch/pytorch/pull/164939))
- [export] Turn on install_free_tensors flag ([#164691](https://github.com/pytorch/pytorch/pull/164691))
- Enable ruff rule E721 ([#165162](https://github.com/pytorch/pytorch/pull/165162))
- Revert "Enable ruff rule E721 (816fb7f48d1)
- [compile] Regional inductor compilation with fx.annotate ([#164776](https://github.com/pytorch/pytorch/pull/164776))
- [dynamo][annotate] Remove the need of external ctx mgr of preserve_node_meta ([#165188](https://github.com/pytorch/pytorch/pull/165188))
- Revert "[dynamo][annotate] Remove the need of external ctx mgr of preserve_node_meta (a19123b37e5)
- Revert "[compile] Regional inductor compilation with fx.annotate (8d49cd5b262)
- Reland vision pinned commit hash update ([#164492](https://github.com/pytorch/pytorch/pull/164492))
- Enable ruff rule E721 ([#165162](https://github.com/pytorch/pytorch/pull/165162))
- Revert "[dynamo][DebugMode] mask python keys in dispatch_key_set guard checks (85801126821)
- Fix double dispatch to Python for detach ([#163671](https://github.com/pytorch/pytorch/pull/163671))
- Revert "Update round size with 1 division behavior (955cd7060b7)
- Revert "[opaque_obj_v2] PyObject custom op schema type (a71ca4dcb91)
- [dynamo][annotate] Remove the need of external ctx mgr of preserve_node_meta ([#165188](https://github.com/pytorch/pytorch/pull/165188))
- [compile] Regional inductor compilation with fx.annotate ([#164776](https://github.com/pytorch/pytorch/pull/164776))
- Revert "Fix double dispatch to Python for detach (267348fe7fd)
- Revert "[export] Turn on install_free_tensors flag (fa3916f4668)
- Revert "use sym_numel, to allow fake tensors to work (33bfec27ff8)
- Revert "Update windows cuda build to use 12.8 (c5972ebdfb5)
- [2/N] More ruff SIM fixes ([#165031](https://github.com/pytorch/pytorch/pull/165031))
- [export] Turn on install_free_tensors flag ([#164691](https://github.com/pytorch/pytorch/pull/164691))
- Revert "[distributed] Replace assert statements with AssertionError exceptions (d2494cbb2b9)
- Patch the flex_attention._get_mod_type to not use inspect.signature when computing num_positional_args (an alternative fix for flex attention graph break on create_block_mask) ([#164923](https://github.com/pytorch/pytorch/pull/164923))
- [opaque_obj_v2] PyObject custom op schema type ([#165004](https://github.com/pytorch/pytorch/pull/165004))
- Revert "Patch the flex_attention._get_mod_type to not use inspect.signature when computing num_positional_args (an alternative fix for flex attention graph break on create_block_mask) (a2f34bdd7ce)
- add and fix OpInfo tests for the default partitioner ([#165372](https://github.com/pytorch/pytorch/pull/165372))
- Revert "[export] Handle kwargs better in aot_export_joint_with_descriptors (7778a58e7c3)
- Revert "varlen api (3044e1a460a)
- Fix double dispatch to Python for detach ([#163671](https://github.com/pytorch/pytorch/pull/163671))
- Revert "add and fix OpInfo tests for the default partitioner (b509fb9b5d8)
- Revert "[inductor] Expand use of generic benchmark function (84d141e910c)
- Revert "[Inductor][CuTeDSL] Move load_template up two directories (8c4b528403d)
- varlen api ([#164502](https://github.com/pytorch/pytorch/pull/164502))
- [ATen] Fix CUDA reduction warp shuffle order ([#164790](https://github.com/pytorch/pytorch/pull/164790))
- Move toString(ScalarType) and ScalarType ostream operator to headeronly ([#164405](https://github.com/pytorch/pytorch/pull/164405))
- Revert "Add mingw to docker (69b05913fb0)
- add and fix OpInfo tests for the default partitioner ([#165372](https://github.com/pytorch/pytorch/pull/165372))
- [inductor] print 0.0 as 0 for triton ([#164291](https://github.com/pytorch/pytorch/pull/164291))
- Revert "12/n : Remove fbandroid_compiler_flags (e1d71a6b353)
- Revert "[DeviceMesh] Simplify unflatten method (431c13cf617)
- Revert "[DeviceMesh] Introduce private constructor instead of _create_mesh_from_ranks (b10f463b1af)
- Revert "[DeviceMesh] Prefer using _layout over _mesh for all sorts of things (27a98e6ae97)
- Revert "[inductor] print 0.0 as 0 for triton (fb06e49ce86)
- Revert "158232  Fix autocast cache incorrectly retaining no_grad state (d2c82bafb70)
- Revert "[Fix] Use sys.executable instead of hardcoded python (470e2f61c3b)
- Revert "[Mem Snapshot] Add Metadata Field (11e20843086)
- Pyrefly suppressions 2 ([#165692](https://github.com/pytorch/pytorch/pull/165692))
- Revert "Turn some const variables into constexpr in C++ code (9e94ec76b8b)
- [annotate] add annotate_fn function decorator ([#165703](https://github.com/pytorch/pytorch/pull/165703))
- Enable all SIM rules except disabled ones ([#164645](https://github.com/pytorch/pytorch/pull/164645))
- [export] preserve_node_meta by default ([#165524](https://github.com/pytorch/pytorch/pull/165524))
- Revert "[annotate] add annotate_fn function decorator (80d2ca7566c)
- Revert "Remove torch.serialization entries from the doc ignore list (574c9fc9503)
- Revert "[export] preserve_node_meta by default (5d4da26ed06)
- [inductor] require shape in TritonCSEVariable ([#162275](https://github.com/pytorch/pytorch/pull/162275))
- Revert "Fix `_StridedShard` incorrect split (85c5433d381)
- Revert "Pyrefly suppressions 2 (2928c5c5724)
- Patch the flex_attention._get_mod_type to not use inspect.signature when computing num_positional_args (an alternative fix for flex attention graph break on create_block_mask) ([#164923](https://github.com/pytorch/pytorch/pull/164923))
- Revert "[DebugMode][1/N] refactor logs into _DebugCalls (9a71d96256d)
- Revert "[DebugMode][2/N] add nn.Module tracking (b08d8c2e506)
- Refactor out headeronly ArrayRef ([#164991](https://github.com/pytorch/pytorch/pull/164991))
- Widen ops support to take in IntHOArrayRef vs only std::vec ([#165152](https://github.com/pytorch/pytorch/pull/165152))
- Revert "shrink_group implementation to expose ncclCommShrink API (fae74cd52f3)
- [annotate] add annotate_fn function decorator ([#165703](https://github.com/pytorch/pytorch/pull/165703))
- Revert "Escaped html tags name and target to appear as strings (06d324365c2)
- Revert "Update gm.print_readable to include Annotation (e50dc40d28b)
- Revert "[Inductor][CuTeDSL] Move load_template up two directories (#165347) (69c33898fa9)
- Enable all PIE rules on ruff ([#165814](https://github.com/pytorch/pytorch/pull/165814))
- Revert "Enable all PIE rules on ruff (24520b8386a)
- Enable all PIE rules on ruff ([#165814](https://github.com/pytorch/pytorch/pull/165814))
- Revert "Enable more DTensor tests in local tensor mode and fix more integration issues (beb6b62e8c9)
- Revert "shrink_group implementation to expose ncclCommShrink API (633a3b7f67f)
- [Inductor] support masked vectorization for the tail_loop for float64 datatype ([#163316](https://github.com/pytorch/pytorch/pull/163316))
- [Inductor] support masked vectorization for the tail_loop for fp8 datatype ([#163324](https://github.com/pytorch/pytorch/pull/163324))
- Revert "12/n : Remove fbandroid_compiler_flags (47804ce4674)
- Revert "[ATen] Fix CUDA reduction warp shuffle order (602ace5eb4f)
- [1/N] Change C-style casts to static_cast or reinterpret_cast ([#165750](https://github.com/pytorch/pytorch/pull/165750))
- Revert "[1/N] Change C-style casts to static_cast or reinterpret_cast (ab82456c16e)
- Revert "[Submodule] Bump FBGEMM to latest (0da1f911dcf)
- Revert "Widen ops support to take in IntHOArrayRef vs only std::vec (62a263b8d42)
- Revert "Refactor out headeronly ArrayRef (69a4bfe8bb1)
- Revert "[dynamo][misc] Replace UserFunctionVariable with VariableTracker build (9875e70da8c)
- Revert "[dynamo][user_defined] Replace UserFunctionVariable with VariableTracker build (0bf604320f9)
- Revert "Move toString(ScalarType) and ScalarType ostream operator to headeronly (ca7360e9961)
- Revert "Remove workaround to old CUDA bug (150682ba7f5)
- Revert "[inductor] require shape in TritonCSEVariable (240c13394ee)
- Revert "[Inductor] Naive foreach autotune support (cf280ca1e82)
- [1/N] Change C-style casts to static_cast or reinterpret_cast ([#165750](https://github.com/pytorch/pytorch/pull/165750))
- [ATen] Fix CUDA reduction warp shuffle order ([#164790](https://github.com/pytorch/pytorch/pull/164790))
- Revert "[torch/utils][Code Clean] Clean asserts in `torch/utils/*.py` (bdf7cb9d9cb)
- Revert "[Inductor] support masked vectorization for the tail_loop for fp8 datatype (78bf6186f27)
- Revert "[Inductor] support masked vectorization for the tail_loop for float64 datatype (6c4412f72b6)
- Revert "[dynamo][misc] Replace UserFunctionVariable with VariableTracker build (ce8a7764e2f)
- Revert "shrink_group implementation to expose ncclCommShrink API (ad4dc52bf6b)
- Revert "[Code Clean] Clean asserts in torch/ao/quantization (root, quantizer, backend_config) (8daef35cf18)
- Revert "[AMP][Refactor] Autocast dtype handling to simplify device-specific c… (7773a22cdbe)
- [inductor][choices] lookup table choices 1/3 ([#164978](https://github.com/pytorch/pytorch/pull/164978))
- Warn if AccumulateGrad stream does not match producer node stream ([#165065](https://github.com/pytorch/pytorch/pull/165065))
- Revert "[lint] workflow consistency linter to look at all files instead of just changed files (05b2e02cb4e)
- Revert "[inductor][choices] lookup table choices 1/3 (baf91bbbfc7)
- Revert "Warn if AccumulateGrad stream does not match producer node stream (75b82958685)
- [DeviceMesh] Use _flatten_rank_map to replace _flatten_mesh_list so that we don't need to compare root mesh ([#166003](https://github.com/pytorch/pytorch/pull/166003))
- Export should use aot_export_joint_with_descriptors ([#165931](https://github.com/pytorch/pytorch/pull/165931))
- [DeviceMesh] Implement a device mesh concatenate api for submesh and SPMD use case ([#163358](https://github.com/pytorch/pytorch/pull/163358))
- Revert "[DeviceMesh] Use _flatten_rank_map to replace _flatten_mesh_list so that we don't need to compare root mesh (81577bdb3f0)
- Revert "[DeviceMesh] Implement a device mesh concatenate api for submesh and SPMD use case (28ee6b62ed7)
- Revert "Export should use aot_export_joint_with_descriptors (690c8c13b96)
- Revert "inductor: avoid unrolling argmin/argmax reductions to preserve index … (380d440d1c2)
- Revert "Export flex attention with kwargs and DTensor (516e58965ab)
- [DeviceMesh] Implement a device mesh concatenate api for submesh and SPMD use case ([#163358](https://github.com/pytorch/pytorch/pull/163358))
- Revert "Simplify the CUPTI  CMake check for kineto (a988510c339)
- Export should use aot_export_joint_with_descriptors ([#165931](https://github.com/pytorch/pytorch/pull/165931))
- [inductor][choices] lookup table choices 1/3 ([#164978](https://github.com/pytorch/pytorch/pull/164978))
- Revert "[CD] Upgrade to CUDA 13.0.2 for nightly binaries (74336f8c77f)
- Revert "[inductor][choices] lookup table choices 1/3 (110efe4df47)
- Revert "Update cuDNN 9.10.2 in Manylinux 2.28 Docker files (0eacd934bc1)
- Revert "[cuDNN] Smoke-test runtime cuDNN version matches compile time version in CI (a4a0378e6bb)
- Revert "[Inductor] Naive foreach autotune support (7379972cc0e)
- Revert "[Pytorch] Update Kineto Submodule (1fdef664a5d)
- [1/N] Remove unused loop variables ([#166258](https://github.com/pytorch/pytorch/pull/166258))
- [Inductor] support masked vectorization for the tail_loop for float64 datatype ([#163316](https://github.com/pytorch/pytorch/pull/163316))
- [user-streams] Make device-agnostic streams weakref compatible ([#164304](https://github.com/pytorch/pytorch/pull/164304))
- [User-streams] Make torch.Event weakref compatible ([#164522](https://github.com/pytorch/pytorch/pull/164522))
- [pytree] add `treespec_{leaf,tuple,dict}` functions for args_spec modification ([#160843](https://github.com/pytorch/pytorch/pull/160843))
- Revert "[1/N] Remove unused loop variables (1dd6b769143)
- Revert "[BE] Move GreenContext implementation details to cpp (5e7272b60a5)
- `nn.Linear`: nD contiguous input + bias -- dispatch to addmm also when weight is sparse ([#166071](https://github.com/pytorch/pytorch/pull/166071))
- Revert "bwd pass (d6d6fa26f54)
- Revert "[User-streams] Make torch.Event weakref compatible (5cdbcb52334)
- Revert "`nn.Linear`: nD contiguous input + bias -- dispatch to addmm also when weight is sparse (c594950e867)
- Revert "[user-streams] Make device-agnostic streams weakref compatible (5fd1d41e62c)
- Revert "[dynamo][guards] 1/N Guard selectively for DTensor (d7040e6d751)
- Revert "[PyTorch] Improve aarch64 performance of bfloat16 ops (2c9f877fa71)
- Revert "[pytree] add `treespec_{leaf,tuple,dict}` functions for args_spec modification (972030fe2ef)
- [user-streams] Make device-agnostic streams weakref compatible ([#164304](https://github.com/pytorch/pytorch/pull/164304))
- [User-streams] Make torch.Event weakref compatible ([#164522](https://github.com/pytorch/pytorch/pull/164522))
- [inductor][choices] lookup table choices 1/3 ([#164978](https://github.com/pytorch/pytorch/pull/164978))
- [user-streams] Fix stream graph output semantics ([#164819](https://github.com/pytorch/pytorch/pull/164819))
- [user-streams] Add current stream source ([#165211](https://github.com/pytorch/pytorch/pull/165211))
- [user-streams] Track symbolic current stream ([#165212](https://github.com/pytorch/pytorch/pull/165212))
- [triton][sigmoid] Fix kernel cache and serialization issue for triton sigmoid + CUDA kernel bug ([#166568](https://github.com/pytorch/pytorch/pull/166568))
- [user-streams] Handle returning the current stream with/without device index ([#165356](https://github.com/pytorch/pytorch/pull/165356))
- [2/N] Add strict parameter to Python zip calls  ([#166257](https://github.com/pytorch/pytorch/pull/166257))
- [1/N] Remove unused loop variables ([#166258](https://github.com/pytorch/pytorch/pull/166258))
- Revert "[2/N] Add strict parameter to Python zip calls  (f60751024ec)
- Revert "[user-streams] Handle returning the current stream with/without device index (9ee1afbf66a)
- Revert "[user-streams] Track symbolic current stream (95b55347730)
- Revert "[triton][sigmoid] Fix kernel cache and serialization issue for triton sigmoid + CUDA kernel bug (fa8e073a4e1)
- Revert "[user-streams] Add current stream source (ad02bd13dfa)
- [triton][sigmoid] Fix kernel cache and serialization issue for triton sigmoid + CUDA kernel bug ([#166568](https://github.com/pytorch/pytorch/pull/166568))
- [inductor] print 0.0 as 0 for triton ([#164291](https://github.com/pytorch/pytorch/pull/164291))
- Revert "Fix comparing inductor actual strides vs bw graph for activations should not throw DDE.  (3f1824742ca)
- Revert "[user-streams] Fix stream graph output semantics (0a3ac47c0a3)
- Revert "shrink_group implementation to expose ncclCommShrink API (694d205143b)
- Revert "address DDE in matmul decomp (8f40a0c634a)
- [CodeClean] Remove the Unused MACRO for AOT Inductor Runtime ([#165139](https://github.com/pytorch/pytorch/pull/165139))
- [CodeClean] Replace std::runtime_error with TORCH_CHECK ([#165119](https://github.com/pytorch/pytorch/pull/165119))
- [Inductor] support masked vectorization for the tail_loop for fp8 datatype ([#163324](https://github.com/pytorch/pytorch/pull/163324))
- Add CUDA MXFP4 scaled mm support via. FBGEMM ([#166526](https://github.com/pytorch/pytorch/pull/166526))
- Revert "[BE][Typing][Dynamo] Type misc files in `torch/_dynamo/variables/` (7d39401fa07)
- [pytree] add `treespec_{leaf,tuple,dict}` functions for args_spec modification ([#160843](https://github.com/pytorch/pytorch/pull/160843))
- [xpu][feature] Integrate OneDNN SDPA training forward/backward into XPU OVERRIDEABLE Backend ([#162454](https://github.com/pytorch/pytorch/pull/162454))
- Fixes the sparse tensor issue ([#163535](https://github.com/pytorch/pytorch/pull/163535))
- Revert "Fix torch.full with dynamic tensor fill_value in torch.compile (657f8c3e21b)
- Revert "[GraphPartition] cache get_free_symbol_uses (26534e9809e)
- Revert "[BE] Move GreenContext implementation details to cpp (4e8ba37ce33)
- Revert "Make PT2 compile backprop through custom op without autograd key a hard error (5bcfdae71da)
- Revert "[BE][Typing][Dynamo] Type misc files in `torch/_dynamo/variables/` (fcc10635660)
- Revert "[pytree] add `treespec_{leaf,tuple,dict}` functions for args_spec modification (85b85f6c2c7)
- Revert "Add CUDA MXFP4 scaled mm support via. FBGEMM (93a70c717a5)
- [MPS] Fix `smooth_l1_loss` backward for fp16 ([#166687](https://github.com/pytorch/pytorch/pull/166687))
- Revert "[xpu][feature] Integrate OneDNN SDPA training forward/backward into XPU OVERRIDEABLE Backend (2699f5410b6)
- [2/N] Add strict parameter to Python zip calls  ([#166257](https://github.com/pytorch/pytorch/pull/166257))
- Revert "Remove setup-env instructions; it's confusing (60333de85de)
- [pytree] add `treespec_{leaf,tuple,dict}` functions for args_spec modification ([#160843](https://github.com/pytorch/pytorch/pull/160843))
- Revert "Avoid DDE in narrow with unbacked start (13549e0e105)
- Revert "Make PT2 compile backprop through custom op without autograd key a hard error (82fafb3304f)
- [user-streams] Fix stream graph output semantics ([#164819](https://github.com/pytorch/pytorch/pull/164819))
- [user-streams] Add current stream source ([#165211](https://github.com/pytorch/pytorch/pull/165211))
- [user-streams] Track symbolic current stream ([#165212](https://github.com/pytorch/pytorch/pull/165212))
- [user-streams] Handle returning the current stream with/without device index ([#165356](https://github.com/pytorch/pytorch/pull/165356))
- Revert "Fix: list index out of range with softmax when using 0 dim (5e05a0ae99c)
- Revert "Add min/max support for barebones uint types (3eddf049221)
- Revert "Back out "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (#164939)" (#165910)" ([#166812](https://github.com/pytorch/pytorch/pull/166812))
- Revert "[MPS] Error out when BatchNorm is called for Complex (a4077b568f8)
- Revert "Give full Dynamo stack traces in CI (5d6230779d9)
- Revert "[MPS] Fix `smooth_l1_loss` backward for fp16 (1656b253c5f)
- Revert "Fixes torch.compile(nn.ModuleList()) changes bool() behavior  (61bcc8d75ad)
- Revert "Back out "Do not decompose in functionalization/proxy tensor if autograd wouldn't have decomposed (#164939)" (#165910)" ([#166812](https://github.com/pytorch/pytorch/pull/166812))
- [FSDP][Replicate] final version integrating 1D device mesh replicate into fsdp ([#166433](https://github.com/pytorch/pytorch/pull/166433))
- [FSDP][Replicate] added two replicate overload declarations and changed device_mesh to mesh ([#166459](https://github.com/pytorch/pytorch/pull/166459))
- Revert "[FSDP][Replicate] added two replicate overload declarations and changed device_mesh to mesh (2f3f88f445c)
- Revert "[FSDP][Replicate] final version integrating 1D device mesh replicate into fsdp (fa0fd6be13c)
- Revert "Avoid DDE in narrow with unbacked start (c10975d2e66)
- Revert "[Inductor] addmm with bias -> unfuse bias if there is a pointwise/reduction consumer (86b2d82e848)
- [FSDP][Replicate] final version integrating 1D device mesh replicate into fsdp ([#166433](https://github.com/pytorch/pytorch/pull/166433))
- [FSDP][Replicate] added two replicate overload declarations and changed device_mesh to mesh ([#166459](https://github.com/pytorch/pytorch/pull/166459))
- [MPS] Fix `smooth_l1_loss` backward for fp16 ([#166687](https://github.com/pytorch/pytorch/pull/166687))
- Revert "[CUDA] Skip pynvml test on platforms that don't have complete support (665a4113517)
- Revert "Fix  unused assignments  (79ff2c66c88)
- [inductor] require shape in TritonCSEVariable ([#162275](https://github.com/pytorch/pytorch/pull/162275))
- Revert "[inductor] require shape in TritonCSEVariable (d3cf90ada54)
- Add CUDA MXFP4 scaled mm support via. FBGEMM ([#166526](https://github.com/pytorch/pytorch/pull/166526))
- Revert "[Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel (d77c24caac4)
- [DebugMode] output, tensor id annotations for DebugMode ([#165076](https://github.com/pytorch/pytorch/pull/165076))
- Add model code stack trace to torch.profile ([#166677](https://github.com/pytorch/pytorch/pull/166677))
- Revert "Add model code stack trace to torch.profile (81038fd3268)
- Revert "make narrow_tensor_symint DDE-free (53b03f1a2b4)
- Revert "Avoid DDE in narrow with unbacked start (a743f9eeb57)
- [12/N] Apply ruff UP035 rule ([#166929](https://github.com/pytorch/pytorch/pull/166929))
- Add model code stack trace to torch.profile ([#166677](https://github.com/pytorch/pytorch/pull/166677))
- Revert "[Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel (5c639466f7b)
- Refactor out headeronly ArrayRef ([#164991](https://github.com/pytorch/pytorch/pull/164991))
- Widen ops support to take in IntHOArrayRef vs only std::vec ([#165152](https://github.com/pytorch/pytorch/pull/165152))
- Revert "Update triton to 3.5.1 release (9c2c3dbc156)
- Revert "Add model code stack trace to torch.profile (c86540f1203)
- Revert "[cuDNN] Smoke-test runtime cuDNN version matches compile time version in CI (ad5c7c20e0d)
- Revert "[Inductor] Fix unbacked float symbol handling in kernel codegen (8e8cbb85ee9)
- Revert "[12/N] Apply ruff UP035 rule (6d30666bc1c)
- Revert "[DebugMode] output, tensor id annotations for DebugMode (ef3f953966d)
- [DebugMode] output, tensor id annotations for DebugMode ([#165076](https://github.com/pytorch/pytorch/pull/165076))
- [12/N] Apply ruff UP035 rule ([#166929](https://github.com/pytorch/pytorch/pull/166929))
- Revert "Don't hardcode double argument for reduction base (b2d72a4008f)
- `nn.Linear`: nD contiguous input + bias -- dispatch to addmm also when weight is sparse ([#166071](https://github.com/pytorch/pytorch/pull/166071))
- Revert "[Inductor] addmm with bias -> unfuse bias if there is a pointwise/reduction consumer (9b4ac45d2fd)
- Revert "[BE][Typing][Dynamo] Type torch/_dynamo/variables/functions.py (cd6d06a22ba)
- Revert "Move enrich_profiler_metadata config import out of gm.recompile() (31ac7642391)
- [CP] Correctly compile create_cp_block_mask ([#167153](https://github.com/pytorch/pytorch/pull/167153))
- Revert "[Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel (12860892f82)
- Revert "Update pythoncapi_compat.h (fb9e10fe255)
- Revert "[13/N] Apply ruff UP035 rule (6392b986e7e)
- Revert "Remove python workaround for ContextDecorator (bbf852d87ff)
- Revert "[CP] Correctly compile create_cp_block_mask (c131e4b390a)
- Introduce a new API torch.accelerator.get_memory_info ([#156812](https://github.com/pytorch/pytorch/pull/156812))
- [xpu][feature] Add XPU support on torch.accelerator.get_memory_info ([#162564](https://github.com/pytorch/pytorch/pull/162564))
- [dynamo, 3.14] enable dynamo in 3.14 ([#167384](https://github.com/pytorch/pytorch/pull/167384))
- Revert "Add min/max support for barebones uint types (8ef4099313f)
- [CP] Correctly compile create_cp_block_mask ([#167153](https://github.com/pytorch/pytorch/pull/167153))
- Revert "Rework PyObject preservation (406f2943d2c)
- Revert "[export, 3.14] handle patching methods with functools.partial correctly in non-strict export (7a48db0809f)
- Revert "[3.14, dataloader] handle forkserver default mp start method in 3.14 (619f329a4b2)
- Revert "[dynamo, 3.14] enable dynamo in 3.14 (a14452bfce3)
- Revert "[inductor, 3.14] catch pickle.PicklingError exceptions (b5142f74f9f)
- Revert "[inductor, 3.14] fix itertools.product pickle error in test_cpu_repro (f0fa39a7e4c)
- Revert "Expose `THPVariable_Wrap()` with a type argument (db250fa8957)
- [ARM] Improve LLM performance & mem usage using int4-bf16 KleidiAI kernels ([#158250](https://github.com/pytorch/pytorch/pull/158250))
- fix failure of exporting compiled model with nested dynamic shapes ([#166358](https://github.com/pytorch/pytorch/pull/166358))
- Revert "[Inductor] Naive foreach autotune support (56034074ca1)
- Revert "[DebugMode] record triton kernels, run-to-run determinism checks (b21856f5fc8)
- Revert "[ARM] Improve LLM performance & mem usage using int4-bf16 KleidiAI kernels (5b6ff8148df)
- Revert "Support AC in default partitioner when functionalization is enabled (2e5233d7bd7)
- Revert "Add FA4 to sdpa (1debfd44fd4)
- Revert "Use c7i.2xlarge for B200 build (25e9d8124c4)
- Revert "fix wrong accuracy_status when exception. (5ce4a8b49f9)
- Revert "[MPS] SparseMps mv op (4f6aae35fd5)
- Revert "[xpu][feature] Add XPU support on torch.accelerator.get_memory_info (a32832682ca)
- Revert "Introduce a new API torch.accelerator.get_memory_info (c5d91d9e3e5)
- Revert "Update Kineto Submodule (10a1578408d)
- Revert "Use stable topological sort in fuse_by_partitions (5f0a5b8f87b)
- Revert "fix failure of exporting compiled model with nested dynamic shapes (1311385f9d0)
- Revert "Fix thread safety in getCurrentCUDABlasHandle and getCUDABlasLtWorkspace (8f5f89c9a0d)
- Introduce a new API torch.accelerator.get_memory_info ([#156812](https://github.com/pytorch/pytorch/pull/156812))
- [xpu][feature] Add XPU support on torch.accelerator.get_memory_info ([#162564](https://github.com/pytorch/pytorch/pull/162564))
- Revert "[precompile] Integrate AOTI as a backend. (7aac506cdcb)
- [dynamo, 3.14] enable dynamo in 3.14 ([#167384](https://github.com/pytorch/pytorch/pull/167384))
- [CD] [aarch64] unify the build.sh to build for aarch64 wheel ([#166044](https://github.com/pytorch/pytorch/pull/166044))
- Revert "address DDE in matmul decomp (fe33d7cadf9)
- add device generalization support for distributed tests ([#165067](https://github.com/pytorch/pytorch/pull/165067))
- deprecate check_is_size and guard_size_oblivious ([#167198](https://github.com/pytorch/pytorch/pull/167198))
- Revert "Fix different seq length (3522e0ce74c)
- Revert "deprecate check_is_size and guard_size_oblivious (e2c68345847)
- Ops convolution_backward optional flag bug ([#165008](https://github.com/pytorch/pytorch/pull/165008))
- Revert "Re-land "Fix thread safety in getCurrentCUDABlasHandle and getCUDABlasLtWorkspace" (caca3f2eec2)
- Revert "Hide all symbols (except stable/headeronly/shim) if TORCH_STABLE_ONLY is defined (602102be502)
- Revert "[CodeClean] Replace std::runtime_error with TORCH_CHECK (5a368b80107)
- Revert "[CodeClean] Remove the Unused MACRO for AOT Inductor Runtime (7aa210d2150)
- distributed/debug: add an HTTP server for debugging running jobs ([#167395](https://github.com/pytorch/pytorch/pull/167395))
- Revert "distributed/debug: add an HTTP server for debugging running jobs (1c1638297e0)
- Revert "Ops convolution_backward optional flag bug (cfe799b4aac)
- Use c10::filesystem ([#167821](https://github.com/pytorch/pytorch/pull/167821))
- Improve char printing ([#167899](https://github.com/pytorch/pytorch/pull/167899))
- deprecate check_is_size and guard_size_oblivious ([#167198](https://github.com/pytorch/pytorch/pull/167198))
- Revert "[ATen][CUDA] Add sm_121a flag for RowwiseScaledMM (b9bccec3bc9)
- [ARM] Improve LLM performance & mem usage using int4-bf16 KleidiAI kernels ([#158250](https://github.com/pytorch/pytorch/pull/158250))
- Revert "add device generalization support for distributed tests (4c152a71add)
- Revert "Remove python workaround for ContextDecorator (39ebab1dd9e)
- Revert "Improve char printing (22ccd44d732)
- Revert "Use c10::filesystem (a4c7bf7e8dd)
- Revert "[ARM] Improve LLM performance & mem usage using int4-bf16 KleidiAI kernels (9d8ceaa36f0)
- Revert "Remove old NVTX interface (661fb534494)
- Revert "Tiling bug fix (1c04a439595)
- Revert "deprecate check_is_size and guard_size_oblivious (b7208877c8c)
- Revert "[CD] Add libopenblas to dep list for AArch64+CPU whl (86f9a9ae762)
- Revert "[CD] [aarch64] unify the build.sh to build for aarch64 wheel (9b39276255a)
- dist: add list_keys to Store API ([#167883](https://github.com/pytorch/pytorch/pull/167883))
- [SymmMem] Skip multicast init if any CUDA call fails ([#168049](https://github.com/pytorch/pytorch/pull/168049))
- Improve char printing ([#167899](https://github.com/pytorch/pytorch/pull/167899))
- distributed/debug: add an HTTP server for debugging running jobs ([#167395](https://github.com/pytorch/pytorch/pull/167395))
- [AOTI] Fix a GPU memory leak caused by reference circle ([#168063](https://github.com/pytorch/pytorch/pull/168063))
- [hoo] Invoke subgraph + effect ([#167231](https://github.com/pytorch/pytorch/pull/167231))
- Hide all symbols (except stable/headeronly/shim) if TORCH_STABLE_ONLY is defined ([#167496](https://github.com/pytorch/pytorch/pull/167496))
- Revert "[SymmMem] Skip multicast init if any CUDA call fails (5abb7bf8fee)
- Revert "[pytree][compile] Slightly faster TreeSpec init (c7cf3fb1250)
- Revert "Error when non stable/headeronly/shim headers are included by stable extension (a097e166db7)
- Revert "Hide all symbols (except stable/headeronly/shim) if TORCH_STABLE_ONLY is defined (acf5b204b03)
- Revert "[invoke_subgraph] Don't run the graph twice when autograd enabled (a6bfe2dedaa)
- Revert "[hoo] Invoke subgraph + effect (ca6175c8f0a)
- Revert "[inductor] fix the decision of inner reduction (771be8c062a)
- Revert "dist: add list_keys to Store API (f890837d979)
- Revert "[AOTI] Fix a GPU memory leak caused by reference circle (192b96e42b8)
- Revert "Change NamedTupleVariable implementation to subclass UserDefinedTupleVariable (7a064ed3eaf)
- [ARM] Improve LLM performance & mem usage using int4-bf16 KleidiAI kernels ([#158250](https://github.com/pytorch/pytorch/pull/158250))
- Revert "Improve build logic in activities for kineto (6edf2aa8f3a)
- Remove useless super() delegation ([#168235](https://github.com/pytorch/pytorch/pull/168235))
- Revert "[dynamo][pytree][compile time] Specialize tree_is_leaf (803d94be296)
- Revert "[dynamo][compile time] Special case for torch.utils._pytree._get_node_type (9396e69194e)
- Revert "Allow BlockDescriptorOptions classes to be overridden In TritonKernel (f7fc6346b0d)
- Revert "Remove useless super() delegation (e7a85200da0)
- Revert "conv: refactor for lookup table support (05b11198fd4)
- dist: add list_keys to Store API ([#167883](https://github.com/pytorch/pytorch/pull/167883))
- Revert "Revert #154859 (28656727473)
- Remove useless super() delegation ([#168235](https://github.com/pytorch/pytorch/pull/168235))
- Revert "[Full Inductor][Pytorch] Prevent decomposition and enable fallback of aten.native_layer_norm for MTIA (c23a90041e4)
- [Inductor XPU GEMM] Step 1/N: Refactor cutlass configuration. ([#160174](https://github.com/pytorch/pytorch/pull/160174))
- [DTensor] compute shape and offset for arbitrary _StridedShard ([#168146](https://github.com/pytorch/pytorch/pull/168146))
- [dynamo] add torch._dynamo.set_recursion_limit to fix 3.12/3.13 RecursionError problems ([#167888](https://github.com/pytorch/pytorch/pull/167888))
- Revert "[Inductor XPU GEMM] Step 2/N: Move out cutlass files from torch/_inductor/codegen/cuda (2c204e6dfc9)
- Revert "[Inductor XPU GEMM] Step 1/N: Refactor cutlass configuration. (3f0d46c8b00)
- Revert "[inductor] Use custom triton kernel subclass when available (1f34961aa98)
- Revert "[dynamo] add torch._dynamo.set_recursion_limit to fix 3.12/3.13 RecursionError problems (4fd97b460cd)
- Remove unnecessary uses of thrust::tuple ([#168936](https://github.com/pytorch/pytorch/pull/168936))
- Revert "bucketing compile time improve (654c5fba3e6)
- Revert "[DTensor] compute shape and offset for arbitrary _StridedShard (dfdf024e236)
- Revert "Checking if the input is finite before calculation in lowering of pow func (35944cb4236)
- Revert "Move CUDAEvent to c10 (33d4cf4fcb7)
- [CD] [aarch64] unify the build.sh to build for aarch64 wheel ([#166044](https://github.com/pytorch/pytorch/pull/166044))
- Revert "[DebugMode] wait before hashing collectives by default (40733d7891c)
- [hoo] Invoke subgraph + effect ([#167231](https://github.com/pytorch/pytorch/pull/167231))
- Revert "Removed deprecated `split_cat_fx_passes` (99024dec888)
- Revert "[DTensor] update redistribute_cost, add disable_graph_based_transform (9844fbeadd5)
- Revert "Support AC in default partitioner when functionalization is enabled (2f9b7dad7b5)
- Revert "Fix local_map default partitioner issue (4c246677784)
- Avoid std::tie and returning value constructions in qconv_unpack.cpp ([#169207](https://github.com/pytorch/pytorch/pull/169207))
- [Accelerator] Add Accelerator Capabilities API ([#165631](https://github.com/pytorch/pytorch/pull/165631))
- add device generalization support for distributed tests ([#165067](https://github.com/pytorch/pytorch/pull/165067))
- Revert "[dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker (55c4ab55484)
- Revert "[Accelerator] Add Accelerator Capabilities API (7d2a33e4ebf)
- Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (641cdb68ae2)
- [AOTI] Fix a GPU memory leak caused by reference circle ([#168063](https://github.com/pytorch/pytorch/pull/168063))
- Avoid std::tie and returning value constructions in qconv_unpack.cpp ([#169207](https://github.com/pytorch/pytorch/pull/169207))
- Revert "Refactor: Remove unnecessary ConstantVariable wrapping in raise_observed_exception (1e34fb2550e)
- Hide all symbols (except stable/headeronly/shim) if TORCH_STABLE_ONLY is defined ([#167496](https://github.com/pytorch/pytorch/pull/167496))
- [Accelerator] Add Accelerator Capabilities API ([#165631](https://github.com/pytorch/pytorch/pull/165631))
- Revert "Remove unnecessary uses of thrust::tuple (70076464a63)
- [dynamo] add torch._dynamo.set_recursion_limit to fix 3.12/3.13 RecursionError problems ([#167888](https://github.com/pytorch/pytorch/pull/167888))
- Revert "[MPS] Fix dlpack exports/imports for sliced tensors (0c281dd7877)
- Remove unnecessary uses of thrust::tuple ([#168936](https://github.com/pytorch/pytorch/pull/168936))
- Revert "[Accelerator] Add Accelerator Capabilities API (6c261c6cb07)
- Revert "[Dynamo][Guards]Fix TLParse CPP guard message with sorting get_leaf_guards and verbose_code_parts (c178ed43d3d)
- Revert " [10/N] Use Python 3.10 typing  (3418bd29475)
- [DTensor] compute shape and offset for arbitrary _StridedShard ([#168146](https://github.com/pytorch/pytorch/pull/168146))
- Enable custom collective op autotuning ([#167294](https://github.com/pytorch/pytorch/pull/167294))
- [Accelerator] Add Accelerator Capabilities API ([#165631](https://github.com/pytorch/pytorch/pull/165631))
- Revert "Triton 3.6 pin update (fdf863d5e1d)
- Revert "Enable custom collective op autotuning (305168768a9)
- Revert "[effect] Remove special handling for profiler op (65c4620d6bb)
- Revert "Add public documentation for stable_topological_sort (b3a7edb2311)
- [export] Make RNNs exportable on GPUs ([#163245](https://github.com/pytorch/pytorch/pull/163245))
- Revert "[generator] Close all open generators in compile_subgraph (2ac3ef882af)
- Revert "Support module.to in strict export (e115f9f4e4b)
- Enable custom collective op autotuning ([#167294](https://github.com/pytorch/pytorch/pull/167294))
- Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (6f53fefeb90)
- Revert "Replace `msg` by `args` in `raise_observed_exception` (cc0853af421)
- Revert "Fix torch.fx for the newer "|" union syntax (b1decff555c)
- Revert "[opaque obj] Improve error msg for intermediate opaques (b6b6c803793)
- Revert "[dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker (ba1412546f3)
- Revert "[Dynamo][Guard]Add the user-friendly TYPE_MATCH for type (b6b6d912df0)
- Revert "[export] Make RNNs exportable on GPUs (ae3a2395bf6)
- [Inductor XPU GEMM] Step 1/N: Refactor cutlass configuration. ([#160174](https://github.com/pytorch/pytorch/pull/160174))
- [inductor] require shape in TritonCSEVariable ([#162275](https://github.com/pytorch/pytorch/pull/162275))
- Revert "Remove unnecessary uses of thrust::tuple (f575ecb83c0)
- Revert "Re-enable torch.compile tests for Python 3.12 and Windows (77b90b703e0)
- [effects] Various effect/dce fixes ([#169141](https://github.com/pytorch/pytorch/pull/169141))
- Revert "[dynamo] Refactor isinstance(x, ConstantVariable) to x.is_python_constant() (6be8f42812e)
- Revert "[Inductor] ReLU/GELU(Addmm) fusions (2169e666d24)
- [3/N] Remove unused header inclusion ([#169200](https://github.com/pytorch/pytorch/pull/169200))
- Revert "[BE] Delete `install_vision` from Docker builds (09f18009ea0)
- Revert "[precompile] disable dispatch when deepcloning in PrecompileContext.record_artifact (965c7e6ece2)
- Revert "activation offloading reordering for comp<>comm overlaps (95a20369331)
- Revert "activation offloading implementation (fc8c66f48f3)
- [SymmMem] Skip multicast init if any CUDA call fails ([#168049](https://github.com/pytorch/pytorch/pull/168049))
- Revert "Add Pylint checks to linterrunner (16696182f60)
- Revert "[effects] Various effect/dce fixes (0022a14bf06)
- Revert "[BE] Rewrite IndexKernel using Dispatch_v2 (1e36d310b15)
- Revert "[CI] Workaround `relocation truncated to fit: R_AARCH64_CALL26` linker error (a25bbe5bae0)
- Revert "[Dynamo][Guard]Add the user-friendly TYPE_MATCH for type (4bebe21b056)
- Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED ([#169044](https://github.com/pytorch/pytorch/pull/169044))
- Fix unnecessary super method delegation detection on pylint ([#169350](https://github.com/pytorch/pytorch/pull/169350))
- Revert "Fix unnecessary super method delegation detection on pylint (e28e4ec14ea)
- Revert "Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED (6bb8cad00f6)
- Fix unnecessary super method delegation detection on pylint ([#169350](https://github.com/pytorch/pytorch/pull/169350))
- Revert "[3/N] Remove unused header inclusion (7dcfb47aa27)
- Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED ([#169044](https://github.com/pytorch/pytorch/pull/169044))
- Revert "Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED (066e4536968)
- Remove unneeded C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED ([#169044](https://github.com/pytorch/pytorch/pull/169044))
- [3/N] Remove unused header inclusion ([#169200](https://github.com/pytorch/pytorch/pull/169200))
- Revert "Fix strides for fa4 integration (67a371749de)
- Avoid std::tie and returning value constructions in qconv_unpack.cpp ([#169207](https://github.com/pytorch/pytorch/pull/169207))
- Revert "[dynamo][hops] Allow side effects in autograd.Function forward graph tracing (304c0ca15e6)
- Revert "[dynamo] Rehaul the autograd.Function support (fbb5bb83d4e)
- Revert "Support for vector of Tensors in autograd::Function (7c593b99eb9)
- Revert "Mempool use_on_oom order (a9dd532d999)
- Revert "[CI] Add eager tests for CUDA 13.0 (2570a8323ca)
- Revert "[Inductor XPU GEMM] Step 2/N: Move out cutlass files from torch/_inductor/codegen/cuda (a208ed2b54e)
- [dynamo] Fix frozen dataclass reconstruction ([#169614](https://github.com/pytorch/pytorch/pull/169614))
- Revert "[dynamo] Fix frozen dataclass reconstruction (05db9e354cf)
- Revert "dynamo torch factory functions apply default device mirroring __torch… (05c82334cdd)
- Revert "[BE][Typing][Dynamo] Type torch/_dynamo/variables/user_defined.py (ee22ae15f89)
- Revert "[OpenReg] Fixed the issue where streams and events could still be bou… (fe95af70ad9)
- Revert "Fixes the sparse tensor issue (da637782dc8)
- [effects] Various effect/dce fixes ([#169141](https://github.com/pytorch/pytorch/pull/169141))
- [c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 ([#168129](https://github.com/pytorch/pytorch/pull/168129))
- Add torch.backends.cuda.math_sdp.fp32_precision ([#169694](https://github.com/pytorch/pytorch/pull/169694))
- [dynamo] Fix frozen dataclass reconstruction ([#169614](https://github.com/pytorch/pytorch/pull/169614))
- Revert "Add torch.backends.cuda.math_sdp.fp32_precision (c6b3d0f359c)
- Revert "Fix evaluating sympy constant causes error (28f8916f793)
- Revert "Add `symm_mem_sync` Triton kernel to `torch.ops.symm_mem` (b0968f98797)
- Revert "[profiler] Change dynamo_timed to use torch._C._profiler._RecordFunctionFast (63cf0681cdb)
- Revert "Avoid std::tie and returning value constructions in qconv_unpack.cpp (9825967b8e8)
- Revert "[c10d][Sym mem] Make nccl backend full fledged with nccl 2.28.9-1 (1d97422133a)
- Revert "[inductor][fx] clarify padding logic (76b8fdafad5)
### not user facing
- Use same NVSHMEM version across CUDA builds ([#162206](https://github.com/pytorch/pytorch/pull/162206))
- [CUDA][float8][TF32] Disable tf32 for vs. emulated rowwise comparison ([#162387](https://github.com/pytorch/pytorch/pull/162387))
- [inductor] add kernel template choice (ktc) ([#161347](https://github.com/pytorch/pytorch/pull/161347))
- [inductor][ez] V.choices.get_mm_configs returns list of ChoiceCallers ([#161348](https://github.com/pytorch/pytorch/pull/161348))
- [Release 2.9] Add compatibility matrix, Version Bump ([#162526](https://github.com/pytorch/pytorch/pull/162526))
- Use same NVSHMEM version across CUDA builds ([#162206](https://github.com/pytorch/pytorch/pull/162206))
- [associative_scan] partial gradient support ([#162388](https://github.com/pytorch/pytorch/pull/162388))
- [nativert][triton] improve hardware registration ([#162499](https://github.com/pytorch/pytorch/pull/162499))
- [OpenReg] Update the docs about Accelerator Integration ([#162046](https://github.com/pytorch/pytorch/pull/162046))
- Move prioritized text linker optimization code from setup.py to cmake ([#160078](https://github.com/pytorch/pytorch/pull/160078))
- Make functorch notebook symlinks PEP 517 valid ([#157813](https://github.com/pytorch/pytorch/pull/157813))
- [nativert] AOTI delegate with flat inputs and outputs ([#162538](https://github.com/pytorch/pytorch/pull/162538))
- Don't unconditionally import torch._dynamo, it's slow ([#162595](https://github.com/pytorch/pytorch/pull/162595))
- [inductor] FlexibleLayout for ExternKernelChoice for mms ([#161351](https://github.com/pytorch/pytorch/pull/161351))
- [inductor] leverage template stacking in V.choices.get_mm_configs ([#161350](https://github.com/pytorch/pytorch/pull/161350))
- [inductor][choices] rename get_mm_configs to get_template_configs ([#162293](https://github.com/pytorch/pytorch/pull/162293))
- [dynamo][guards] Do not construct entire framelocals dict for LAMBDA_GUARD ([#162525](https://github.com/pytorch/pytorch/pull/162525))
- testing infra and some fixes ([#162183](https://github.com/pytorch/pytorch/pull/162183))
- Move inductor jobs 3.9->3.10 ([#162323](https://github.com/pytorch/pytorch/pull/162323))
- Improve device info with new flops and bandwidth formula based on hardware libraries ([#162245](https://github.com/pytorch/pytorch/pull/162245))
- [cuDNN][Convolution][TF32][64bit] Add `tf32_on_and_off` decorator to conv3d 64bit test ([#161004](https://github.com/pytorch/pytorch/pull/161004))
- Build fbgemm_gpu for TORCH_CUDA_ARCH_LIST=10.0 and CUDA 12.8 and 12.9 ([#162544](https://github.com/pytorch/pytorch/pull/162544))
- -ldl for nativert tests ([#162643](https://github.com/pytorch/pytorch/pull/162643))
- [2/N]Port several test files under test/distributed to Intel GPU ([#159473](https://github.com/pytorch/pytorch/pull/159473))
- Remove __torch_dispatch__ check in THPVariable_make_dtensor ([#162337](https://github.com/pytorch/pytorch/pull/162337))
- use torch.accelerator and device_module instead of cuda to make DataParallel more device agnostic. ([#162573](https://github.com/pytorch/pytorch/pull/162573))
- [dynamo][guards] Fail on an unknown framelocals to dict conversion ([#162695](https://github.com/pytorch/pytorch/pull/162695))
- Update SECURITY.md with reporting guidelines ([#162608](https://github.com/pytorch/pytorch/pull/162608))
- AMD CPU CI - Add freezing + fix label trigger ([#162176](https://github.com/pytorch/pytorch/pull/162176))
- added example for torch.is_storage ([#162614](https://github.com/pytorch/pytorch/pull/162614))
- [MTIA Runtime] Add foreach_div ops to native_functions.yaml ([#162732](https://github.com/pytorch/pytorch/pull/162732))
- fix: raise value error on init ParametrizationList if original.device != new.device ([#162717](https://github.com/pytorch/pytorch/pull/162717))
- [BE][flex attention] compute RMSE in float64 ([#162088](https://github.com/pytorch/pytorch/pull/162088))
- [BUG]Fixed handle cannot be hit in the cache in the IPC ExpandableSegment ([#161885](https://github.com/pytorch/pytorch/pull/161885))
- Move inductor jobs 3.9->3.10 ([#162323](https://github.com/pytorch/pytorch/pull/162323))
- Make functorch notebook symlinks PEP 517 valid ([#157813](https://github.com/pytorch/pytorch/pull/157813))
- [OpenReg] Implement device autoload mechanism ([#158555](https://github.com/pytorch/pytorch/pull/158555))
- Add api info for torch._C._nn.pyi ([#162361](https://github.com/pytorch/pytorch/pull/162361))
- [nativert] aoti ([#162353](https://github.com/pytorch/pytorch/pull/162353))
- Fix operator benchmark issue#162708 ([#162744](https://github.com/pytorch/pytorch/pull/162744))
- port some distributed tensor test files for Intel GPU ([#161703](https://github.com/pytorch/pytorch/pull/161703))
- Fix protobuf test comparison by parsing proto instead of raw strings ([#162644](https://github.com/pytorch/pytorch/pull/162644))
- Fix markdown link syntax in graph breaks index ([#162400](https://github.com/pytorch/pytorch/pull/162400))
- [inductor] FlexibleLayout for ExternKernelChoice for mms ([#161351](https://github.com/pytorch/pytorch/pull/161351))
- [inductor] leverage template stacking in V.choices.get_mm_configs ([#161350](https://github.com/pytorch/pytorch/pull/161350))
- [inductor][choices] rename get_mm_configs to get_template_configs ([#162293](https://github.com/pytorch/pytorch/pull/162293))
- xpu: test py_limited_api with SyclExtension ([#162546](https://github.com/pytorch/pytorch/pull/162546))
- [OpenReg] Migrate OpenReg Tests from tests/test_openreg.py into torch_openreg/tests ([#161917](https://github.com/pytorch/pytorch/pull/161917))
- [OpenReg] Strengthen Openreg's execution limits to minimize the waste of computing resources ([#161918](https://github.com/pytorch/pytorch/pull/161918))
- [OpenReg] Improve the Event and Stream capabilities of DeviceGuardImplInterface ([#160101](https://github.com/pytorch/pytorch/pull/160101))
- [OpenReg] Fix the docs of Accelerator Intergration ([#162826](https://github.com/pytorch/pytorch/pull/162826))
- [AOTI] raise PyTorchStreamWriter open failed error code on windows ([#162799](https://github.com/pytorch/pytorch/pull/162799))
- Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available, or cpu-only build ([#160532](https://github.com/pytorch/pytorch/pull/160532))
- Fix boxcox to return same result for same input in one batch ([#162772](https://github.com/pytorch/pytorch/pull/162772))
- kjt pytree registration ([#161114](https://github.com/pytorch/pytorch/pull/161114))
- Claude loves making these files in top level, ignore them for sanity. ([#162806](https://github.com/pytorch/pytorch/pull/162806))
- Add missing `tags` parameter to `custom_op` overload signatures ([#162047](https://github.com/pytorch/pytorch/pull/162047))
- [data foundation][vizard] Prevent checking the device type of numpy object in Tensorboard logger ([#162888](https://github.com/pytorch/pytorch/pull/162888))
- QoL: add pip to requirements-build.txt ([#162896](https://github.com/pytorch/pytorch/pull/162896))
- fix deterministic scatter_add path for multi-d tensors ([#162866](https://github.com/pytorch/pytorch/pull/162866))
- Update slow tests ([#162946](https://github.com/pytorch/pytorch/pull/162946))
- [easy] Fix unsigned long issue in static cuda launcher ([#162920](https://github.com/pytorch/pytorch/pull/162920))
- [easy] Handle Autotuners in get_triton_source_codes_for_gm ([#161914](https://github.com/pytorch/pytorch/pull/161914))
- Clean up 'torch.onnx' entries from public API allowlist ([#162850](https://github.com/pytorch/pytorch/pull/162850))
- [lint][CI] Don't checkout submodules for lintrunner-noclang ([#162844](https://github.com/pytorch/pytorch/pull/162844))
- AMD CPU CI - Add freezing + fix label trigger ([#162176](https://github.com/pytorch/pytorch/pull/162176))
- Add `CUDA_KERNEL_ASSERT_PRINTF`, a more flexible  `CUDA_KERNEL_ASSERT_MSG` ([#160129](https://github.com/pytorch/pytorch/pull/160129))
- [dynamo][hop] Introduce Local Map HOP ([#161458](https://github.com/pytorch/pytorch/pull/161458))
- Set the credential to upload vLLM nightly wheels on schedule and workflow_dispatch ([#163018](https://github.com/pytorch/pytorch/pull/163018))
- [CI] disable rerun of distributed tests ([#163025](https://github.com/pytorch/pytorch/pull/163025))
- [Flight Recorder][WP] Added mismatch tail as an arg ([#162991](https://github.com/pytorch/pytorch/pull/162991))
- [functional] Avoid duplicate custom get_device call in constructor ([#162889](https://github.com/pytorch/pytorch/pull/162889))
- Update torch-xpu-ops commit pin ([#162804](https://github.com/pytorch/pytorch/pull/162804))
- [PT2]: Overriding Tensor device by SubmodNameToDevice ([#162144](https://github.com/pytorch/pytorch/pull/162144))
- Replace `std::runtime_error` with `TORCH_CHECK` ([#159344](https://github.com/pytorch/pytorch/pull/159344))
- [TEST][CUDA] Use proper dtype in test_cuda_tensor_pow_scalar_tensor_cuda ([#163070](https://github.com/pytorch/pytorch/pull/163070))
- [CPU] Adding missing brackets in native MaxUnpool log ([#163039](https://github.com/pytorch/pytorch/pull/163039))
- Set the credential to upload vLLM nightly wheels on schedule and workflow_dispatch ([#163018](https://github.com/pytorch/pytorch/pull/163018))
- Update Gloo submodule ([#163112](https://github.com/pytorch/pytorch/pull/163112))
- [PyTorch] Compile SVE's box-cox only when building targeting SVE ([#163078](https://github.com/pytorch/pytorch/pull/163078))
- Add decomp rule to assert_tensor_metadata for BatchedTensors  ([#163008](https://github.com/pytorch/pytorch/pull/163008))
- [Reland] Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available, or cpu-only build ([#163016](https://github.com/pytorch/pytorch/pull/163016))
- [Reland][2/N]Port several test files under test/distributed to Intel GPU ([#159473](https://github.com/pytorch/pytorch/pull/159473))
- Fix TODO in make_tensor_for_subclass_helper ([#162336](https://github.com/pytorch/pytorch/pull/162336))
- Update placement utils and weights to handle meta device ([#162842](https://github.com/pytorch/pytorch/pull/162842))
- [dynamo][hop] Introduce Local Map HOP ([#161458](https://github.com/pytorch/pytorch/pull/161458))
- [CD] Do not enable GenAI on Windows ([#163116](https://github.com/pytorch/pytorch/pull/163116))
- [CI] Update NVIDIA driver to `580.82.07` ([#163111](https://github.com/pytorch/pytorch/pull/163111))
- Upgrades dlpack to v1.1 to include fp8/fp4 ([#162195](https://github.com/pytorch/pytorch/pull/162195))
- Fix set_grad_enabled HOP in strict mode with new tracer ([#162559](https://github.com/pytorch/pytorch/pull/162559))
- [CI] Update NVIDIA driver to `580.82.07` ([#163111](https://github.com/pytorch/pytorch/pull/163111))
- [MTIA Runtime] Add foreach_div ops to native_functions.yaml ([#162732](https://github.com/pytorch/pytorch/pull/162732))
- remove tolerance override for dynamo test_mixed_device_dtype in SGD ([#163088](https://github.com/pytorch/pytorch/pull/163088))
- compile_kernel: Add DLPack test ([#163166](https://github.com/pytorch/pytorch/pull/163166))
- [Reland] Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available ([#163016](https://github.com/pytorch/pytorch/pull/163016))
- Fix SEMI_STRUCTURED_SUPPORTED_BACKENDS selection on CUDA and ROCm ([#163223](https://github.com/pytorch/pytorch/pull/163223))
- [inductor] pdl inductor option (disabled by default) ([#160928](https://github.com/pytorch/pytorch/pull/160928))
- [torch][cuda][device_limits] Library for querying device hardware limits for flops and bandwidth ([#162942](https://github.com/pytorch/pytorch/pull/162942))
- Update Microsoft C++ Redistributable to the latest version ([#161430](https://github.com/pytorch/pytorch/pull/161430))
- Move prioritized text linker optimization code from setup.py to cmake ([#160078](https://github.com/pytorch/pytorch/pull/160078))
- [unit test] correct wrong input shape in test_flop_fx ([#163148](https://github.com/pytorch/pytorch/pull/163148))
- test: ensure editable cached wrapper is respected ([#160943](https://github.com/pytorch/pytorch/pull/160943))
- [functional] Use the saved device on storage instead for device_custom ([#162987](https://github.com/pytorch/pytorch/pull/162987))
- [CI] Move Windows build/tests to Python-3.10 ([#162862](https://github.com/pytorch/pytorch/pull/162862))
- Update torch-xpu-ops commit pin ([#163244](https://github.com/pytorch/pytorch/pull/163244))
- Rm pytorch deps platform args ([#163086](https://github.com/pytorch/pytorch/pull/163086))
- [functionalize] Avoid one more call to custom get_device on FunctionalTensorWrapper ([#163019](https://github.com/pytorch/pytorch/pull/163019))
- Fix performance regression when indexing by Numpy arrays ([#163280](https://github.com/pytorch/pytorch/pull/163280))
- [MTIA] Add MTIA dispatch for kernel foreach_maximum(Add D80022242 back) ([#161571](https://github.com/pytorch/pytorch/pull/161571))
- [FP8][cuBLAS][H100] only test fp32 outputs for rowwise `_scaled_mm` on H100 ([#162022](https://github.com/pytorch/pytorch/pull/162022))
- Add analytics ID to cpp docs ([#163370](https://github.com/pytorch/pytorch/pull/163370))
- [CI] Move Windows build/tests to Python-3.10 ([#162862](https://github.com/pytorch/pytorch/pull/162862))
- Lazy import to avoid circular import issue for DebugMode ([#163381](https://github.com/pytorch/pytorch/pull/163381))
- remove duplicate import for defaultdict ([#160519](https://github.com/pytorch/pytorch/pull/160519))
- [Caffe2] Improve SVE batch box cox by 2% ([#163360](https://github.com/pytorch/pytorch/pull/163360))
- [STABLE ABI] Add copy_ operation. ([#161895](https://github.com/pytorch/pytorch/pull/161895))
- [1/n] Support cpu_tensor.to("cuda:0") in FakeTensorMode on cuda-less machine ([#160431](https://github.com/pytorch/pytorch/pull/160431))
- [CUDA][cuBLAS][FP8] Forward-fix #162022 ([#163354](https://github.com/pytorch/pytorch/pull/163354))
- torchdim Python port ([#160236](https://github.com/pytorch/pytorch/pull/160236))
- [Docs] Fix indentations in cond.md ([#156147](https://github.com/pytorch/pytorch/pull/156147))
- Delete functorch C extension entirely. ([#163340](https://github.com/pytorch/pytorch/pull/163340))
- Add api info for torch._C._nn.pyi ([#162707](https://github.com/pytorch/pytorch/pull/162707))
- [Easy][AMP] Refactor the AMP logic for getting dtype ([#162796](https://github.com/pytorch/pytorch/pull/162796))
- Remove autograd code for Python < 3.9 ([#163313](https://github.com/pytorch/pytorch/pull/163313))
- Add torchfuzz initial impl. ([#163417](https://github.com/pytorch/pytorch/pull/163417))
- [export] Fix wrap_with_set_grad_enabled retracing ([#163295](https://github.com/pytorch/pytorch/pull/163295))
- Enable half precision types on test_conv_cudnn_nhwc_support ([#163444](https://github.com/pytorch/pytorch/pull/163444))
- Update cutlass version for fbcode ([#163091](https://github.com/pytorch/pytorch/pull/163091))
- Update fbgemm submodule ([#163411](https://github.com/pytorch/pytorch/pull/163411))
- switch from stack based to graph based aproach ([#163459](https://github.com/pytorch/pytorch/pull/163459))
- [AOTI] fix TestAOTInductorPackage temp file locked handler. ([#163499](https://github.com/pytorch/pytorch/pull/163499))
- Enable logging for absolute memory estimation ([#158799](https://github.com/pytorch/pytorch/pull/158799))
- Fix lint ([#163542](https://github.com/pytorch/pytorch/pull/163542))
- [Fix] Restrict stride normalization to 1D tensors on export ([#163282](https://github.com/pytorch/pytorch/pull/163282))
- [STABLE ABI] Add clone method to torch::stable::Tensor ([#161896](https://github.com/pytorch/pytorch/pull/161896))
- Add dynamic shapes doc ([#159428](https://github.com/pytorch/pytorch/pull/159428))
- [BE] Delete `skipIfMPSOnMacOS13` ([#163515](https://github.com/pytorch/pytorch/pull/163515))
- Remove outdated commented CMake code ([#163442](https://github.com/pytorch/pytorch/pull/163442))
- [precompile] Add option to disable guard check on aot-compiled function. ([#163432](https://github.com/pytorch/pytorch/pull/163432))
- [ignore][codex-test] Add typing to simple library registry ([#161367](https://github.com/pytorch/pytorch/pull/161367))
- Add fake_impl for _native_multi_head_attention ([#163167](https://github.com/pytorch/pytorch/pull/163167))
- [torchfuzz] Encapsulate fuzzing and codegen logic into ops ([#163547](https://github.com/pytorch/pytorch/pull/163547))
- [torchfuzz] remove supports_variable_inputs for now ([#163553](https://github.com/pytorch/pytorch/pull/163553))
- [torch][cuda][device_limits] Library for querying device hardware limits for flops and bandwidth ([#162942](https://github.com/pytorch/pytorch/pull/162942))
- [torchfuzz] cache operators ([#163554](https://github.com/pytorch/pytorch/pull/163554))
- [torchfuzz] decompose -> fuzz_inputs_specs ([#163555](https://github.com/pytorch/pytorch/pull/163555))
- [torchfuzz] shuffle compatible ops ([#163556](https://github.com/pytorch/pytorch/pull/163556))
- [torchfuzz] remove unneeded try catch ([#163557](https://github.com/pytorch/pytorch/pull/163557))
- Update Kineto Submodule ([#162222](https://github.com/pytorch/pytorch/pull/162222))
- [OpenReg][Docs] Correct docs about `openreg` usage example. ([#163235](https://github.com/pytorch/pytorch/pull/163235))
- [torchfuzz] introduce tensor and scalar pointwise ops ([#163558](https://github.com/pytorch/pytorch/pull/163558))
- CUDA 13.0 Warning update for supported architectures ([#163585](https://github.com/pytorch/pytorch/pull/163585))
- cudagraph trees ut fixes ([#163592](https://github.com/pytorch/pytorch/pull/163592))
- Large tests failing on bfloat16 ([#163537](https://github.com/pytorch/pytorch/pull/163537))
- Skip on sm100 later since Tests are non determinisitic ([#163552](https://github.com/pytorch/pytorch/pull/163552))
- [EZ] Perma-ignore UP038 ([#163649](https://github.com/pytorch/pytorch/pull/163649))
- Add num_store to inductor_meta and use it to scale persistent reduction x block ([#162446](https://github.com/pytorch/pytorch/pull/162446))
- Update tests to check for more robust pattern ([#163107](https://github.com/pytorch/pytorch/pull/163107))
- symintify fill_diagonol_ ([#163485](https://github.com/pytorch/pytorch/pull/163485))
- [ez] use list initializer syntax in fill_diagonal_ ([#163607](https://github.com/pytorch/pytorch/pull/163607))
- Update pytorch.org links in docs/conf.py ([#163682](https://github.com/pytorch/pytorch/pull/163682))
- [torchfuzz] introduce multi process fuzzer ([#163560](https://github.com/pytorch/pytorch/pull/163560))
- [Kineto] Add list of string parsing for profiler ([#163593](https://github.com/pytorch/pytorch/pull/163593))
- Support for amin, amax, and aminmax ([#163669](https://github.com/pytorch/pytorch/pull/163669))
- Add api info for torch._C._nn.pyi ([#162936](https://github.com/pytorch/pytorch/pull/162936))
- [hop] support local_map + SAC ([#163322](https://github.com/pytorch/pytorch/pull/163322))
- Delete functorch C extension entirely. ([#163340](https://github.com/pytorch/pytorch/pull/163340))
- Add mistral/gpt-oss to benchmarks ([#163565](https://github.com/pytorch/pytorch/pull/163565))
- [Code Clean] Remove deadcodes about Python3.9 [2/N] ([#163627](https://github.com/pytorch/pytorch/pull/163627))
- [Code Clean] Remove deadcodes about Python3.9 [3/N] ([#163629](https://github.com/pytorch/pytorch/pull/163629))
- [Code Clean] Remove deadcodes about Python3.9 [5/N] ([#163644](https://github.com/pytorch/pytorch/pull/163644))
- [CD] CUDA 13.0 fix preload logic to include nvidia/cu13/lib/ ([#163661](https://github.com/pytorch/pytorch/pull/163661))
- Add less warps config to inner reductions ([#162447](https://github.com/pytorch/pytorch/pull/162447))
- Avoid `at::alias` in the `repeat` op implementation ([#163455](https://github.com/pytorch/pytorch/pull/163455))
- Update pyrefly configuration file ([#163775](https://github.com/pytorch/pytorch/pull/163775))
- [torchfuzz] refactor multi_process_fuzzer to be more readable ([#163698](https://github.com/pytorch/pytorch/pull/163698))
- [torchfuzz] print out tensor descriptor as comments in codegen ([#163739](https://github.com/pytorch/pytorch/pull/163739))
- fix pickling for BitwiseFn ([#163571](https://github.com/pytorch/pytorch/pull/163571))
- remove allow-untyped-defs from ./torch/nn/utils/_expanded_weights/embedding_expanded_weights.py ([#163475](https://github.com/pytorch/pytorch/pull/163475))
- remove allow-untyped-defs from ./torch/utils/benchmark/op_fuzzers/sparse_unary.py ([#163476](https://github.com/pytorch/pytorch/pull/163476))
- Improve MANIFEST.in for source distribution ([#157814](https://github.com/pytorch/pytorch/pull/157814))
- Add sdist handling to version finding ([#160315](https://github.com/pytorch/pytorch/pull/160315))
- [Code Clean] Replace std::runtime_error with TORCH_CHECK ([#163264](https://github.com/pytorch/pytorch/pull/163264))
- [CD] Add statically linked windows libraries to exclude list ([#163768](https://github.com/pytorch/pytorch/pull/163768))
- [OpenReg][BE] Replacing explicit prefix/suffix with CMake variables ([#163850](https://github.com/pytorch/pytorch/pull/163850))
- [Tools] Adapting the Hypothesis library (version 5.x) for use with the PyTorch framework ([#163748](https://github.com/pytorch/pytorch/pull/163748))
- [torchfuzz] simplify codegen and runner ([#163743](https://github.com/pytorch/pytorch/pull/163743))
- [torchfuzz] make generated code much more concise and cleaner ([#163812](https://github.com/pytorch/pytorch/pull/163812))
- [BE] Remove HermeticPyObjectTLS and Simplify PythonOpRegistrationTrampoline ([#163464](https://github.com/pytorch/pytorch/pull/163464))
- [export] _detect_attribute_assignment gives warning instead of raising ValueError ([#163809](https://github.com/pytorch/pytorch/pull/163809))
- Add Static Dispatch Kernels (#163676) ([#163870](https://github.com/pytorch/pytorch/pull/163870))
- [Code Clean] Replace `std::runtime_error` with `TORCH_CHECK` ([#163610](https://github.com/pytorch/pytorch/pull/163610))
- Exporting aten.conv with cuda under fake mode on a cuda-less machine ([#163912](https://github.com/pytorch/pytorch/pull/163912))
- Fix specialize_impl from triton.runtime.jit ([#163844](https://github.com/pytorch/pytorch/pull/163844))
- [CPU] Support transpose and packing fusion for bit8 ([#163233](https://github.com/pytorch/pytorch/pull/163233))
- Update torch-xpu-ops commit pin ([#163758](https://github.com/pytorch/pytorch/pull/163758))
- Expose torch.nn.utils.parametrize ([#163835](https://github.com/pytorch/pytorch/pull/163835))
- [testing] Add test owner labels for some cuda? tests ([#163296](https://github.com/pytorch/pytorch/pull/163296))
- Add tests for aot_export_joint_with_descriptors annotation ([#163893](https://github.com/pytorch/pytorch/pull/163893))
- Fix preserve annotation with decomp ([#163896](https://github.com/pytorch/pytorch/pull/163896))
- [cuDNN][conv][64-bit] Disable cuDNN for 64-bit depthwise convs again ([#163171](https://github.com/pytorch/pytorch/pull/163171))
- [torchfuzz] Add support for fuzz templates  ([#163890](https://github.com/pytorch/pytorch/pull/163890))
- [cuDNN][SDPA] Disable dropout for cuDNN SDPA on 9.11 - 9.13 ([#163903](https://github.com/pytorch/pytorch/pull/163903))
- Fix setting of memory fraction in test_garbage_collect_expandable ([#164000](https://github.com/pytorch/pytorch/pull/164000))
- Change python grid calc for MTIA back to python mode ([#163601](https://github.com/pytorch/pytorch/pull/163601))
- [torchfuzz] ones over zero ([#164002](https://github.com/pytorch/pytorch/pull/164002))
- lint: Only include files in pytorch ([#164008](https://github.com/pytorch/pytorch/pull/164008))
- lint: Filter out /usr/include from results ([#164012](https://github.com/pytorch/pytorch/pull/164012))
- Add runtime_overhead PR Time Benchmark ([#163866](https://github.com/pytorch/pytorch/pull/163866))
- [torchfuzz] fix bool propagation ([#164003](https://github.com/pytorch/pytorch/pull/164003))
- [amd] Add cudaHostFn_t to cuda_to_hip_mappings ([#164007](https://github.com/pytorch/pytorch/pull/164007))
- [AMP] Add deprecated decorator for torch.xxx.amp.autocast class ([#163654](https://github.com/pytorch/pytorch/pull/163654))
- [test][scan] refactor inductor test and prepare for adding bw tests ([#161557](https://github.com/pytorch/pytorch/pull/161557))
- [scan][be] remove unnecessary tensor checks ([#161664](https://github.com/pytorch/pytorch/pull/161664))
- [hop] refactor check input alias and mutation to be a graph pass ([#162025](https://github.com/pytorch/pytorch/pull/162025))
- Add type annotations to MPS profiler utilities ([#163486](https://github.com/pytorch/pytorch/pull/163486))
- Handle DDE in infer_size_impl ([#163822](https://github.com/pytorch/pytorch/pull/163822))
- Remove unused argument from DEFINE_BINARY macro.  ([#163868](https://github.com/pytorch/pytorch/pull/163868))
- Better error handling in torch/nativert/* ([#163308](https://github.com/pytorch/pytorch/pull/163308))
- Move control flow export tests to new tracer ([#163259](https://github.com/pytorch/pytorch/pull/163259))
- Remove C++ workarounds for Python < 3.10 ([#164055](https://github.com/pytorch/pytorch/pull/164055))
- registraion replaced with registration in jit_type.h file comment ([#164072](https://github.com/pytorch/pytorch/pull/164072))
- fixes import error 'functionalize' from functorch ([#163746](https://github.com/pytorch/pytorch/pull/163746))
- [dynamo] Special path for cloning of torch dispatch tensors ([#164081](https://github.com/pytorch/pytorch/pull/164081))
- Eliminate setup.py install/develop in the codebose ([#162329](https://github.com/pytorch/pytorch/pull/162329))
- [Fix] Adding missing `f` prefixes to formatted strings [3/N] ([#164067](https://github.com/pytorch/pytorch/pull/164067))
- Update slow tests ([#163493](https://github.com/pytorch/pytorch/pull/163493))
- Add less warps config to inner reductions ([#162447](https://github.com/pytorch/pytorch/pull/162447))
- Enable outer reductions in fbcode ([#163884](https://github.com/pytorch/pytorch/pull/163884))
- [MPSInductor] Unskip test_repeat_interleave_Tensor_decomp ([#164136](https://github.com/pytorch/pytorch/pull/164136))
- Fix comment on broadcasting example to clarify dimension mismatch ([#162177](https://github.com/pytorch/pytorch/pull/162177))
- [inductor] do comm compute overlap at aten fx level ([#163215](https://github.com/pytorch/pytorch/pull/163215))
- refactor bucketing ([#163754](https://github.com/pytorch/pytorch/pull/163754))
- Helper to augment graph with additional deps ([#163959](https://github.com/pytorch/pytorch/pull/163959))
- Consistently use c10_ovrsource in arvr mode everywhere ([#164128](https://github.com/pytorch/pytorch/pull/164128))
- [testing] upload test stats: Add info to the invoking file summary and some other changes ([#164016](https://github.com/pytorch/pytorch/pull/164016))
- Install `fmtlib` headers. ([#164139](https://github.com/pytorch/pytorch/pull/164139))
- [doc] Add AOTInductor intermediate debug printer OSS user manual ([#163794](https://github.com/pytorch/pytorch/pull/163794))
- [CI] Push `viable/strict/${time}` tags ([#164183](https://github.com/pytorch/pytorch/pull/164183))
- [torchfuzz] Make scalar and tensor distribution configurable ([#164034](https://github.com/pytorch/pytorch/pull/164034))
- [inductor] do comm compute overlap at aten fx level ([#163215](https://github.com/pytorch/pytorch/pull/163215))
- refactor bucketing ([#163754](https://github.com/pytorch/pytorch/pull/163754))
- Helper to augment graph with additional deps ([#163959](https://github.com/pytorch/pytorch/pull/163959))
- [dynamo] Special path for cloning of torch dispatch tensors ([#164081](https://github.com/pytorch/pytorch/pull/164081))
- [OpenReg] Add AMP Integration guide for accelerators ([#162050](https://github.com/pytorch/pytorch/pull/162050))
- Remove unused PyIntXXX, THPUtils_newReal_BOOL, THPQXXX macros ([#164056](https://github.com/pytorch/pytorch/pull/164056))
- [BE] Remove not existing mnist mirror ([#164238](https://github.com/pytorch/pytorch/pull/164238))
- [CUDA][Expandable Segments] Follow-up cleanups for even more expandable segments tests ([#163297](https://github.com/pytorch/pytorch/pull/163297))
- [testing] Add upload for test status during test stat uploads ([#164189](https://github.com/pytorch/pytorch/pull/164189))
- [precompile] Add option to disable guard check on aot-compiled function. ([#163432](https://github.com/pytorch/pytorch/pull/163432))
- Fix TestExportOpInfo ([#164184](https://github.com/pytorch/pytorch/pull/164184))
- Exporting aten.sdpa with cuda under fake mode on a cuda-less machine ([#164162](https://github.com/pytorch/pytorch/pull/164162))
- [dynamo, 3.14] compile actual code in C dynamo ([#161555](https://github.com/pytorch/pytorch/pull/161555))
- [dynamo, 3.14] fix stack ref copy error ([#163796](https://github.com/pytorch/pytorch/pull/163796))
- [dynamo, 3.14] fix _detect_and_normalize_assert_statement for 3.14 ([#164005](https://github.com/pytorch/pytorch/pull/164005))
- fix: inductor non_blocking test - warmup events to make test pass whether it is the first run or not ([#164188](https://github.com/pytorch/pytorch/pull/164188))
- Update persons of interest for XLA. The previous one is out of date. ([#158652](https://github.com/pytorch/pytorch/pull/158652))
- Fix silent incorrectness for bmm/baddmm out_dtype overload ([#164095](https://github.com/pytorch/pytorch/pull/164095))
- [CUDA][CUDAGraph] Reduce capture overhead in CUDA Graph memory reuse ([#162186](https://github.com/pytorch/pytorch/pull/162186))
- [PyTorch CCA] Add an API to get expandable segment sizes ([#163771](https://github.com/pytorch/pytorch/pull/163771))
- Revert new-test part of #163829 ([#164259](https://github.com/pytorch/pytorch/pull/164259))
- [torchfuzz] remove erroneous can_produce check ([#164209](https://github.com/pytorch/pytorch/pull/164209))
- [torchfuzz] raise if Operator abstract method is not implemented ([#164211](https://github.com/pytorch/pytorch/pull/164211))
- [torchfuzz] add layout operators ([#164210](https://github.com/pytorch/pytorch/pull/164210))
- [PyTorch Pinned Allocator] Pinned memory stats and perf fixes around allocating blocks ([#163777](https://github.com/pytorch/pytorch/pull/163777))
- [CI] Push `viable/strict/${time}` tags ([#164183](https://github.com/pytorch/pytorch/pull/164183))
- Improve repeat op to a single copy ([#163842](https://github.com/pytorch/pytorch/pull/163842))
- [BE][Easy] update CUDA and ROCm sources in nightly tool ([#162324](https://github.com/pytorch/pytorch/pull/162324))
- [Easy] Add notes for setting up dev venv with specific Python version ([#164214](https://github.com/pytorch/pytorch/pull/164214))
- Register MTIA kernel for all_all_out ([#164293](https://github.com/pytorch/pytorch/pull/164293))
- Adds Issue#153109 as a test for CUDAPluggableAllocator ([#163575](https://github.com/pytorch/pytorch/pull/163575))
- Add a RECORD_FUNCTION for Python fallback so it shows in profile ([#160573](https://github.com/pytorch/pytorch/pull/160573))
- [PyTorch] Pull ARM's box-cox ([#164152](https://github.com/pytorch/pytorch/pull/164152))
- Use TMA loads always for Triton grouped MM kernel ([#164256](https://github.com/pytorch/pytorch/pull/164256))
- [testing] Better short job name during upload additional stats ([#164287](https://github.com/pytorch/pytorch/pull/164287))
- Skip windows unittest in fbcode ([#164363](https://github.com/pytorch/pytorch/pull/164363))
- Enable torch.nn.functional.batch_norm in test_export_opinfo ([#164261](https://github.com/pytorch/pytorch/pull/164261))
- Speed up FP precision lookup ([#164044](https://github.com/pytorch/pytorch/pull/164044))
- [torchfuzz] make fuzzer deterministic ([#164397](https://github.com/pytorch/pytorch/pull/164397))
- [torchfuzz] add matmuls ([#164284](https://github.com/pytorch/pytorch/pull/164284))
- [torchfuzz] keep track of operator stats ([#164334](https://github.com/pytorch/pytorch/pull/164334))
- [MTIA] Enable deserialization for FP8 checkpoint loading ([#163559](https://github.com/pytorch/pytorch/pull/163559))
- unbacked reshape_copy ([#164336](https://github.com/pytorch/pytorch/pull/164336))
- [vision hash update] update the pinned vision hash ([#154694](https://github.com/pytorch/pytorch/pull/154694))
- Better path handling for nightly setup tool ([#164215](https://github.com/pytorch/pytorch/pull/164215))
- [PyTorch / Sigrid GPU] Fixes in pinned stats collection and add new ODS pinned memory stats ([#164412](https://github.com/pytorch/pytorch/pull/164412))
- [torchfuzz] add test suite of fuzzer repros that we xfail ([#164430](https://github.com/pytorch/pytorch/pull/164430))
- Add failing bitwise equivalence UT for aot_eager on rms_norm ([#164280](https://github.com/pytorch/pytorch/pull/164280))
- Add methods to access data and unpack_hook on SavedVariable ([#164358](https://github.com/pytorch/pytorch/pull/164358))
- Stop parsing command line arguments every time common_utils is imported. ([#156703](https://github.com/pytorch/pytorch/pull/156703))
- Add CUDA release architecture matrix ([#164471](https://github.com/pytorch/pytorch/pull/164471))
- Add provenance to inductor IR nodes created after graph.run ([#164255](https://github.com/pytorch/pytorch/pull/164255))
- Update torch.rst ([#164408](https://github.com/pytorch/pytorch/pull/164408))
- Add Aidyn-A to CUDA codeowners ([#164436](https://github.com/pytorch/pytorch/pull/164436))
- Use posix_fallocate() to reserve disk space for shared memory ([#161910](https://github.com/pytorch/pytorch/pull/161910))
- typo corrected in ivalue.cpp's comment ([#164485](https://github.com/pytorch/pytorch/pull/164485))
- Stop parsing command line arguments every time common_utils is imported. ([#156703](https://github.com/pytorch/pytorch/pull/156703))
- Fix FloorDiv should not generate non integer rationals (due to sympy bug) ([#164398](https://github.com/pytorch/pytorch/pull/164398))
- MX: Remove redundant PLATFORM_SUPPORTS_MX_GEMM constant ([#164320](https://github.com/pytorch/pytorch/pull/164320))
- [torchfuzz] Support EagerVsFullGraphDynamicCompileWithNumericsCheck ([#164432](https://github.com/pytorch/pytorch/pull/164432))
- [torchfuzz] add nn functional ops ([#164434](https://github.com/pytorch/pytorch/pull/164434))
- [torchfuzz] add norm operators ([#164514](https://github.com/pytorch/pytorch/pull/164514))
- Pin conda version for Docker builds ([#164575](https://github.com/pytorch/pytorch/pull/164575))
- forward fix #164481 ([#164578](https://github.com/pytorch/pytorch/pull/164578))
- Change default device to current acclerator ([#164399](https://github.com/pytorch/pytorch/pull/164399))
- [CUDA] Add experimental green context support for SM carveout ([#159104](https://github.com/pytorch/pytorch/pull/159104))
- Stop building nativert in OSS ([#164463](https://github.com/pytorch/pytorch/pull/164463))
- Fix -Wno-duplicate-decl-specifier is valid for C/ObjC but not for C++ ([#164552](https://github.com/pytorch/pytorch/pull/164552))
- [TorchGen] Remove unused variables and function imports ([#164538](https://github.com/pytorch/pytorch/pull/164538))
- Speed up FP precision lookup ([#164044](https://github.com/pytorch/pytorch/pull/164044))
- Add pure view support in autograd Function ([#164467](https://github.com/pytorch/pytorch/pull/164467))
- Make custom op alias check consistent ([#164576](https://github.com/pytorch/pytorch/pull/164576))
- [dynamo, 3.14] fix _detect_and_normalize_assert_statement for 3.14 ([#164005](https://github.com/pytorch/pytorch/pull/164005))
- [fr] Re-order mismatch check in fr analysis script ([#164606](https://github.com/pytorch/pytorch/pull/164606))
- Change intra-graph offset dtype to `uint64_t` ([#164515](https://github.com/pytorch/pytorch/pull/164515))
- Fix mesh.get_local_rank when it is > 1d ([#164473](https://github.com/pytorch/pytorch/pull/164473))
- Numpy zerotensor handling ([#164487](https://github.com/pytorch/pytorch/pull/164487))
- [dynamo, 3.14] prevent StackRef compilation in 3.14 Windows ([#164400](https://github.com/pytorch/pytorch/pull/164400))
- remove no longer needed torch._check_is_size calls from test_dynamic_shapes ([#164627](https://github.com/pytorch/pytorch/pull/164627))
- Enable ruff FURB161 rule ([#164654](https://github.com/pytorch/pytorch/pull/164654))
- [Compile] Fix Compile Warning for Capture Id ([#163898](https://github.com/pytorch/pytorch/pull/163898))
- AC should work with pre-dispatch IR ([#164505](https://github.com/pytorch/pytorch/pull/164505))
- Update slow tests ([#164726](https://github.com/pytorch/pytorch/pull/164726))
- Add num_store to inductor_meta and use it to scale persistent reduction x block ([#162446](https://github.com/pytorch/pytorch/pull/162446))
- [torchfuzz] check in some more xfail repros ([#164619](https://github.com/pytorch/pytorch/pull/164619))
- [Max Autotune][B200] Fix decompose_k test failure ([#164021](https://github.com/pytorch/pytorch/pull/164021))
- [Max Autotune][B200] Relax absolute tolerance for MM+MM test ([#164022](https://github.com/pytorch/pytorch/pull/164022))
- Add pure view support in autograd Function ([#164736](https://github.com/pytorch/pytorch/pull/164736))
- Use PyObject_GetOptionalAttrString in PyObject_FastGetAttrString when available ([#164624](https://github.com/pytorch/pytorch/pull/164624))
- Limit path search within range ([#164581](https://github.com/pytorch/pytorch/pull/164581))
- Fix docker build issue after 164575 ([#164774](https://github.com/pytorch/pytorch/pull/164774))
- [CUDA] Cleanup persistent cuBLASLt workspaces before compile-regions test ([#163299](https://github.com/pytorch/pytorch/pull/163299))
- [CUDA] Add experimental green context support for SM carveout ([#159104](https://github.com/pytorch/pytorch/pull/159104))
- Numpy zerotensor handling ([#164487](https://github.com/pytorch/pytorch/pull/164487))
- Make custom op alias check consistent ([#164576](https://github.com/pytorch/pytorch/pull/164576))
- [annotate] Copy fwd to bwd metadata for subgraphs as well ([#164795](https://github.com/pytorch/pytorch/pull/164795))
- Remove unused THPXXX macros ([#164660](https://github.com/pytorch/pytorch/pull/164660))
- [torchfuzz] update README.md ([#164646](https://github.com/pytorch/pytorch/pull/164646))
- [torchfuzz] don't use the first gpu in multi process fuzzer ([#164647](https://github.com/pytorch/pytorch/pull/164647))
- [torchfuzz] add support for operator weights ([#164649](https://github.com/pytorch/pytorch/pull/164649))
- [torchfuzz] support more unbacked functions ([#164687](https://github.com/pytorch/pytorch/pull/164687))
- [torchfuzz] move into experimental dir ([#164688](https://github.com/pytorch/pytorch/pull/164688))
- [torchfuzz] consolidate on a base implementation of args_codegen ([#164693](https://github.com/pytorch/pytorch/pull/164693))
- [torchfuzz] make ops_fuzzer deterministic ([#164694](https://github.com/pytorch/pytorch/pull/164694))
- [torchfuzz] various edge case fixes ([#164715](https://github.com/pytorch/pytorch/pull/164715))
- [torchfuzz] synthesize inputs for data dependent ops ([#164716](https://github.com/pytorch/pytorch/pull/164716))
- remove check_is_size from test_misc.py ([#164667](https://github.com/pytorch/pytorch/pull/164667))
- Add memory estimator ([#164738](https://github.com/pytorch/pytorch/pull/164738))
- Enable batch samples in sparse tests ([#164677](https://github.com/pytorch/pytorch/pull/164677))
- Remove device_id param from DeviceCachingAllocator::malloc ([#164798](https://github.com/pytorch/pytorch/pull/164798))
- CUDA 13.0 builds fix on Amazon Linux 2023 ([#164870](https://github.com/pytorch/pytorch/pull/164870))
- Enable all flake8-logging-format rules ([#164655](https://github.com/pytorch/pytorch/pull/164655))
- [opaque obj] Error for torch.library.custom_op infer_schema ([#163277](https://github.com/pytorch/pytorch/pull/163277))
- [opaque_obj] Add make_fx tracing support ([#163278](https://github.com/pytorch/pytorch/pull/163278))
- use check_size instead of check_is_size in ops.py ([#164668](https://github.com/pytorch/pytorch/pull/164668))
- [Benchmark] remove old timm models from benchmark ([#164805](https://github.com/pytorch/pytorch/pull/164805))
- [ez] fix small doc error ([#164915](https://github.com/pytorch/pytorch/pull/164915))
- [Max Autotune][B200] Skip carveout tests ([#164435](https://github.com/pytorch/pytorch/pull/164435))
- Use TMA loads always for Triton grouped MM kernel ([#164256](https://github.com/pytorch/pytorch/pull/164256))
- Add memory estimator ([#164738](https://github.com/pytorch/pytorch/pull/164738))
- [ez] remove unnecessary wrapper ([#164720](https://github.com/pytorch/pytorch/pull/164720))
- [BE] Use torch check the way its intended ([#164987](https://github.com/pytorch/pytorch/pull/164987))
- Use runner with more memory for ASAN builds ([#165000](https://github.com/pytorch/pytorch/pull/165000))
- Change test_emulate_precision_casts_mean_ratio_chain from gelu to relu ([#164997](https://github.com/pytorch/pytorch/pull/164997))
- [CUDA][cuBLAS] addmm -- some refactoring for easier navigation between the Lt and non-Lt paths ([#163955](https://github.com/pytorch/pytorch/pull/163955))
- Fix truediv numerics between eager and compile ([#164144](https://github.com/pytorch/pytorch/pull/164144))
- Add less warps config to inner reductions ([#162447](https://github.com/pytorch/pytorch/pull/162447))
- [vllm hash update] update the pinned vllm hash ([#164628](https://github.com/pytorch/pytorch/pull/164628))
- Introduce joint_custom_pass callback ([#164981](https://github.com/pytorch/pytorch/pull/164981))
- Fix Avoid DDE in item numel check ([#164934](https://github.com/pytorch/pytorch/pull/164934))
- Update torch-xpu-ops commit pin ([#164237](https://github.com/pytorch/pytorch/pull/164237))
- [Code Clean] Remove support of python3.9 ([#163846](https://github.com/pytorch/pytorch/pull/163846))
- Fix Avoid DDE in item numel check ([#164934](https://github.com/pytorch/pytorch/pull/164934))
- Fix truediv numerics between eager and compile ([#164144](https://github.com/pytorch/pytorch/pull/164144))
- avoid bit cast for bfloat16_t ([#159946](https://github.com/pytorch/pytorch/pull/159946))
- Enable mimalloc on non-Windows platforms and make default for AArch64 builds ([#164741](https://github.com/pytorch/pytorch/pull/164741))
- Remove shared_ptr from MHAGraphCache ([#164895](https://github.com/pytorch/pytorch/pull/164895))
- reorder wrappers in aot_stage2_inference to match forward compile in aot_stage2_autograd ([#165016](https://github.com/pytorch/pytorch/pull/165016))
- [CodeClean] Replace std::runtime_error with TORCH_CHECK ([#164129](https://github.com/pytorch/pytorch/pull/164129))
- Enable mimalloc on non-Windows platforms and make default for AArch64 builds ([#164741](https://github.com/pytorch/pytorch/pull/164741))
- Mark functions const in CUDACachingAllocator ([#165007](https://github.com/pytorch/pytorch/pull/165007))
- Call internal log_compilation_event if it exists ([#164855](https://github.com/pytorch/pytorch/pull/164855))
- [flex attention] change "==" to "is" in inspect parameter comparison ([#165003](https://github.com/pytorch/pytorch/pull/165003))
- [cuDNN][SDPA] Handle noncontig nested tensors in cuDNN SDPA ([#164958](https://github.com/pytorch/pytorch/pull/164958))
- [CD] Do not propagate download.pytorch.org IP into container ([#165075](https://github.com/pytorch/pytorch/pull/165075))
- Hotfix test scaled matmul cuda ([#165104](https://github.com/pytorch/pytorch/pull/165104))
- [inductor] verify determinism with inductor benchmark script ([#164904](https://github.com/pytorch/pytorch/pull/164904))
- [inductor][eazy] change how torch.use_deterministic_algorithms affect inductor ([#164905](https://github.com/pytorch/pytorch/pull/164905))
- [hop] local_map fix fw_gm/bw_gm naming ([#164419](https://github.com/pytorch/pytorch/pull/164419))
- [hop] local_map validate partitioned fw/bw wrt placements ([#164420](https://github.com/pytorch/pytorch/pull/164420))
- [hop] trace local_map with local shapes in fake key ([#164340](https://github.com/pytorch/pytorch/pull/164340))
- [hop] support local_map None gradients ([#164431](https://github.com/pytorch/pytorch/pull/164431))
- [hop] support local_map filtered gradients ([#164437](https://github.com/pytorch/pytorch/pull/164437))
- [CD] Do not propagate download.pytorch.org IP into container ([#165075](https://github.com/pytorch/pytorch/pull/165075))
- [Code Clean] Remove support of python3.9 ([#163846](https://github.com/pytorch/pytorch/pull/163846))
- [AMP][Refactor] Simplify dtype support logic in autocast context manager ([#163446](https://github.com/pytorch/pytorch/pull/163446))
- Fix truediv numerics between eager and compile ([#164144](https://github.com/pytorch/pytorch/pull/164144))
- [torchfuzz] remove fixed xfail ([#165116](https://github.com/pytorch/pytorch/pull/165116))
- outline various stages from aot stage2 compile ([#164808](https://github.com/pytorch/pytorch/pull/164808))
- [dynamo][executorch] Do not trace into exeuctorch LoweredBackendModule ([#165126](https://github.com/pytorch/pytorch/pull/165126))
- [Code Clean] Replace std::runtime_error with TORCH_CHECK ([#163927](https://github.com/pytorch/pytorch/pull/163927))
- [torchfuzz] add more context to xfail test file ([#165149](https://github.com/pytorch/pytorch/pull/165149))
- [testing] Print something for log classifier to better differentiate reruns vs real failures ([#165163](https://github.com/pytorch/pytorch/pull/165163))
- Disable failing test_int8_woq_mm_cuda on slow grad check ([#165147](https://github.com/pytorch/pytorch/pull/165147))
- Fix truediv numerics between eager and compile ([#164144](https://github.com/pytorch/pytorch/pull/164144))
- error message for instantiating CUDA Stream if CUDA not available ([#159868](https://github.com/pytorch/pytorch/pull/159868))
- [inductor] verify determinism with inductor benchmark script ([#164904](https://github.com/pytorch/pytorch/pull/164904))
- [inductor][eazy] change how torch.use_deterministic_algorithms affect inductor ([#164905](https://github.com/pytorch/pytorch/pull/164905))
- [Code Clean] Replace std::runtime_error with TORCH_CHECK ([#163437](https://github.com/pytorch/pytorch/pull/163437))
- Fix pre-dispatch AC HOP calling convention ([#165145](https://github.com/pytorch/pytorch/pull/165145))
- [vllm hash update] update the pinned vllm hash ([#164628](https://github.com/pytorch/pytorch/pull/164628))
- Update slow tests ([#165301](https://github.com/pytorch/pytorch/pull/165301))
- Disable failing test_int8_woq_mm_concat_cuda on slow grad check ([#165331](https://github.com/pytorch/pytorch/pull/165331))
- [torchfuzz] fix some errors when walkthroughing README.md ([#165225](https://github.com/pytorch/pytorch/pull/165225))
- Force inlining into torch_function_mode_enabled ([#164617](https://github.com/pytorch/pytorch/pull/164617))
- Remove FIXME comment about reset_max_memory_reserved ([#165249](https://github.com/pytorch/pytorch/pull/165249))
- Use sym_eq in _check_rms_norm_inputs_symint ([#165112](https://github.com/pytorch/pytorch/pull/165112))
- Update windows cuda build to use 12.8 ([#165345](https://github.com/pytorch/pytorch/pull/165345))
- use sym_numel, to allow fake tensors to work ([#163831](https://github.com/pytorch/pytorch/pull/163831))
- MTIA _cdist_forward registration ([#165333](https://github.com/pytorch/pytorch/pull/165333))
- Add CLAUDE_CONTEXT directory to gitignore ([#165358](https://github.com/pytorch/pytorch/pull/165358))
- [torch/utils][Code Clean] Clean asserts in `torch/utils/_sympy` ([#165279](https://github.com/pytorch/pytorch/pull/165279))
- Replace insert with std::rotate_copy for RingBuffer ([#165348](https://github.com/pytorch/pytorch/pull/165348))
- Update torch-xpu-ops commit pin ([#165321](https://github.com/pytorch/pytorch/pull/165321))
- [distributed] Replace assert statements with AssertionError exceptions ([#165216](https://github.com/pytorch/pytorch/pull/165216))
- Update windows cuda build to use 12.8 ([#165345](https://github.com/pytorch/pytorch/pull/165345))
- [Bugfix][vLLM] Explicitly do not support instead of crashing for named tuples in infer schema ([#165191](https://github.com/pytorch/pytorch/pull/165191))
- Fix IValue from SymBool on big-endian system ([#163647](https://github.com/pytorch/pytorch/pull/163647))
- [torch/utils][Code Clean] Clean asserts in `hipify/`, `jit/`, `model_dump` and `tensorboard` of `torch/utils` ([#165311](https://github.com/pytorch/pytorch/pull/165311))
- [Fix] Completely remove stride normalization on DLPack Tensor ([#164161](https://github.com/pytorch/pytorch/pull/164161))
- Add option to run AOT Precompile in benchmark ([#164906](https://github.com/pytorch/pytorch/pull/164906))
- [export] Handle kwargs better in aot_export_joint_with_descriptors ([#165334](https://github.com/pytorch/pytorch/pull/165334))
- make aotdispatcher opinfo tests keep input mutations in graph ([#165327](https://github.com/pytorch/pytorch/pull/165327))
- [AOTI] skip Windows XPU crashed UTs. ([#165393](https://github.com/pytorch/pytorch/pull/165393))
- [benchmark] Add more timm models ([#165381](https://github.com/pytorch/pytorch/pull/165381))
- feat(dynamo): IS#160752 make F.one_hot work with jacfwd + torch.compile(dynamic=True) ([#160837](https://github.com/pytorch/pytorch/pull/160837))
- [inductor] Expand use of generic benchmark function ([#164938](https://github.com/pytorch/pytorch/pull/164938))
- [ATen][CMake] Fix duplicated CUTLASS path ([#165424](https://github.com/pytorch/pytorch/pull/165424))
- [Inductor][CuTeDSL] Move load_template up two directories ([#165347](https://github.com/pytorch/pytorch/pull/165347))
- Refactor and unify v1/v2 _scaled_mm codes ([#165436](https://github.com/pytorch/pytorch/pull/165436))
- See if we can handle uploading all test data ([#165484](https://github.com/pytorch/pytorch/pull/165484))
- Fix `_StridedShard` incorrect split ([#165533](https://github.com/pytorch/pytorch/pull/165533))
- consolidate fw and inference compile paths ([#165457](https://github.com/pytorch/pytorch/pull/165457))
- [Inductor][CuTeDSL] Move load_template up two directories (#165347) ([#165576](https://github.com/pytorch/pytorch/pull/165576))
- Add mingw to docker ([#165560](https://github.com/pytorch/pytorch/pull/165560))
- [hop] run local_map with interpreter to preserve fx_traceback annotations ([#165336](https://github.com/pytorch/pytorch/pull/165336))
- Reuse kLargeBuffer in XPUCachingAllocator ([#165508](https://github.com/pytorch/pytorch/pull/165508))
- 12/n : Remove fbandroid_compiler_flags ([#165558](https://github.com/pytorch/pytorch/pull/165558))
- Refine XPU allocator message when OOM ([#165509](https://github.com/pytorch/pytorch/pull/165509))
- [PowerPC] Disable MKLDNN TF32 on PowerPC to fix build failure ([#163454](https://github.com/pytorch/pytorch/pull/163454))
- refactor: replace runtime_error with TORCH_CHECK for better error handling ([#163628](https://github.com/pytorch/pytorch/pull/163628))
- [BE] Fold cond into `TORCH_CHECK(false,...)` ([#165593](https://github.com/pytorch/pytorch/pull/165593))
- Restore AcceleratorAllocatorConfig to avoid potential regression ([#165129](https://github.com/pytorch/pytorch/pull/165129))
- Register CUDAAllocatorConfig to AcceleratorAllocatorConfig ([#165131](https://github.com/pytorch/pytorch/pull/165131))
- Reuse AcceleratorAllocatorConfig in CUDAAllocatorConfig ([#165135](https://github.com/pytorch/pytorch/pull/165135))
- Remove unused code in CUDAAllocatorConfig ([#165136](https://github.com/pytorch/pytorch/pull/165136))
- Refactor CUDAAllocatorConfig using ConfigTokenizer ([#165281](https://github.com/pytorch/pytorch/pull/165281))
- Repro for property related Dynamo graph break ([#165609](https://github.com/pytorch/pytorch/pull/165609))
- [DeviceMesh] Prefer using _layout over _mesh for all sorts of things ([#165554](https://github.com/pytorch/pytorch/pull/165554))
- [FP8] Add other Blackwell compute-capabiilities to expected fail `test_honor_sm_carveout` ([#165159](https://github.com/pytorch/pytorch/pull/165159))
- [DeviceMesh] Introduce private constructor instead of _create_mesh_from_ranks ([#165555](https://github.com/pytorch/pytorch/pull/165555))
- [DeviceMesh] Simplify unflatten method ([#165556](https://github.com/pytorch/pytorch/pull/165556))
- 158232  Fix autocast cache incorrectly retaining no_grad state ([#165068](https://github.com/pytorch/pytorch/pull/165068))
- [Fix] Use sys.executable instead of hardcoded python ([#165633](https://github.com/pytorch/pytorch/pull/165633))
- Register var for MTIA ([#165382](https://github.com/pytorch/pytorch/pull/165382))
- 12/n : Remove fbandroid_compiler_flags ([#165558](https://github.com/pytorch/pytorch/pull/165558))
- [easy] Fix graph_capture in aot_joint_with_descriptors test ([#165660](https://github.com/pytorch/pytorch/pull/165660))
- Add NEON acceleration for `Vectorized<int[8|16|32|64>` ([#165273](https://github.com/pytorch/pytorch/pull/165273))
- [DebugMode][1/N] refactor logs into _DebugCalls ([#165376](https://github.com/pytorch/pytorch/pull/165376))
- Add mingw to docker ([#165560](https://github.com/pytorch/pytorch/pull/165560))
- [BE][Ez]: Use sys.executable instead of hardcoded Python ([#165679](https://github.com/pytorch/pytorch/pull/165679))
- [torchfuzz] check in some more ignore regexes ([#164749](https://github.com/pytorch/pytorch/pull/164749))
- [torchfuzz] add support for --stop-at-first-failure flag ([#165529](https://github.com/pytorch/pytorch/pull/165529))
- [CUDA][cuBLAS] Only `xFail` `addmm` with reduced precision reductions on non-RTX skus ([#165379](https://github.com/pytorch/pytorch/pull/165379))
- [codemod][lowrisk] Remove unused exception parameter from some files ([#165700](https://github.com/pytorch/pytorch/pull/165700))
- Remove torch.serialization entries from the doc ignore list ([#160224](https://github.com/pytorch/pytorch/pull/160224))
- User-passed alpha to scaled_gemm ([#165563](https://github.com/pytorch/pytorch/pull/165563))
- fix wrong accuracy_status when exception. ([#165731](https://github.com/pytorch/pytorch/pull/165731))
- [dynamo][user_defined] Replace UserFunctionVariable with VariableTracker build ([#165706](https://github.com/pytorch/pytorch/pull/165706))
- [dynamo][misc] Replace UserFunctionVariable with VariableTracker build ([#165707](https://github.com/pytorch/pytorch/pull/165707))
- Escaped html tags name and target to appear as strings ([#165543](https://github.com/pytorch/pytorch/pull/165543))
- [DebugMode][2/N] add nn.Module tracking ([#165498](https://github.com/pytorch/pytorch/pull/165498))
- Fix B200 test fails in scaled_mm ([#165747](https://github.com/pytorch/pytorch/pull/165747))
- [DeviceMesh] Prefer using _layout over _mesh for all sorts of things ([#165554](https://github.com/pytorch/pytorch/pull/165554))
- [DeviceMesh] Introduce private constructor instead of _create_mesh_from_ranks ([#165555](https://github.com/pytorch/pytorch/pull/165555))
- [DeviceMesh] Simplify unflatten method ([#165556](https://github.com/pytorch/pytorch/pull/165556))
- Improve error message for non-positive groups in convolution ([#165669](https://github.com/pytorch/pytorch/pull/165669))
- Enable C407 of flake8 ([#165046](https://github.com/pytorch/pytorch/pull/165046))
- Fix `_StridedShard` incorrect split ([#165533](https://github.com/pytorch/pytorch/pull/165533))
- Enable more DTensor tests in local tensor mode and fix more integration issues ([#165716](https://github.com/pytorch/pytorch/pull/165716))
- .venv/ in .gitignore  ([#165418](https://github.com/pytorch/pytorch/pull/165418))
- [BE]: Update cudnn frontend submodule to 1.15.0 ([#165776](https://github.com/pytorch/pytorch/pull/165776))
- [Submodule] Bump FBGEMM to latest ([#165544](https://github.com/pytorch/pytorch/pull/165544))
- Various C++ code fixes in LSAN integration ([#165818](https://github.com/pytorch/pytorch/pull/165818))
- Remove CUDA 11 workarounds for CUB_SUPPORTS_SCAN_BY_KEY and CUB_SUPPORTS_UNIQUE_BY_KEY ([#164637](https://github.com/pytorch/pytorch/pull/164637))
- Fix missing closing quote  in __init__.py documentation ([#165827](https://github.com/pytorch/pytorch/pull/165827))
- Enable more DTensor tests in local tensor mode and fix more integration issues ([#165716](https://github.com/pytorch/pytorch/pull/165716))
- Enable all flake8-logging-format rules ([#164655](https://github.com/pytorch/pytorch/pull/164655))
- [BE][Ez]: Update Eigen to 5.0.0. C++14 support and more! ([#165840](https://github.com/pytorch/pytorch/pull/165840))
- [BE]: Update kleidai submodule to v1.15.0 ([#165842](https://github.com/pytorch/pytorch/pull/165842))
- [BE][Ez]: Update torch.is_tensor documentation ([#165841](https://github.com/pytorch/pytorch/pull/165841))
- Refine Allocator Config error message friendly ([#165288](https://github.com/pytorch/pytorch/pull/165288))
- Deprecate overlapped functions in CUDAAllocatorConfig ([#165289](https://github.com/pytorch/pytorch/pull/165289))
- Refine CUDA BackendStaticInitializer for allocator select ([#165298](https://github.com/pytorch/pytorch/pull/165298))
- Fix AllocatorConfig parse roundup division bug ([#165304](https://github.com/pytorch/pytorch/pull/165304))
- Migrating some more callsites ([#163580](https://github.com/pytorch/pytorch/pull/163580))
- Remove E721 suppression in flake8 ([#165855](https://github.com/pytorch/pytorch/pull/165855))
- test_scaled_matmul_cuda: fix infer_scale_swizzle ([#165788](https://github.com/pytorch/pytorch/pull/165788))
- [Fix XPU CI] [Inductor UT] Fix test cases broken by community.  ([#165714](https://github.com/pytorch/pytorch/pull/165714))
- [Code Clean] Clean asserts in torch/autograd. ([#165627](https://github.com/pytorch/pytorch/pull/165627))
- [torch/utils][Code Clean] Clean asserts in `torch/utils/*.py` ([#165410](https://github.com/pytorch/pytorch/pull/165410))
- [stage 2c] make autograd and inference functions ([#165668](https://github.com/pytorch/pytorch/pull/165668))
- Add XPU part for persons_of_interest ([#165920](https://github.com/pytorch/pytorch/pull/165920))
- Add clang-tidy misc-definitions-in-headers check ([#164959](https://github.com/pytorch/pytorch/pull/164959))
- [dynamo][misc] Replace UserFunctionVariable with VariableTracker build ([#165707](https://github.com/pytorch/pytorch/pull/165707))
- Move hardware_destructive_interference_size to c10/core/alignment.h ([#160067](https://github.com/pytorch/pytorch/pull/160067))
- [export] Handle kwargs better in aot_export_joint_with_descriptors ([#165334](https://github.com/pytorch/pytorch/pull/165334))
- Improve PATH hints in FindvecLib.cmake ([#165881](https://github.com/pytorch/pytorch/pull/165881))
- Remove AOTI cross compilation time from internal CI ([#165935](https://github.com/pytorch/pytorch/pull/165935))
- Update workaround to old CUDA bug (#164354) ([#165984](https://github.com/pytorch/pytorch/pull/165984))
- update fr trace analysis ([#165994](https://github.com/pytorch/pytorch/pull/165994))
- [CI] Extend test_transfomers to MPS ([#165960](https://github.com/pytorch/pytorch/pull/165960))
- [CUDA][cuBLAS] addmm -- some refactoring for easier navigation between the Lt and non-Lt paths ([#163955](https://github.com/pytorch/pytorch/pull/163955))
- [AMP][Refactor] Autocast dtype handling to simplify device-specific c… ([#165221](https://github.com/pytorch/pytorch/pull/165221))
- [Pytorch] Add NEON Vectorized<uint> family of translation layers ([#165690](https://github.com/pytorch/pytorch/pull/165690))
- Add operator name to output json  ([#164583](https://github.com/pytorch/pytorch/pull/164583))
- [Code Clean] Replace `std::runtime_error` with `TORCH_CHECK` ([#165209](https://github.com/pytorch/pytorch/pull/165209))
- Simplify c10::guts::apply ([#164566](https://github.com/pytorch/pytorch/pull/164566))
- [AMD] Run int4_mm tests only for compatible arch ([#165630](https://github.com/pytorch/pytorch/pull/165630))
- Move allocation size config to AllocatorConfig for cross-allocator sharing ([#159553](https://github.com/pytorch/pytorch/pull/159553))
- [12/n][take2] : Remove fbandroid_compiler_flags platform args ([#165916](https://github.com/pytorch/pytorch/pull/165916))
- Update doc ([#166024](https://github.com/pytorch/pytorch/pull/166024))
- [scan x vmap] support scan in vmap ([#165580](https://github.com/pytorch/pytorch/pull/165580))
- [dirsync] Switch to top-level `xplat/third-party/pthreadpool` ([#165995](https://github.com/pytorch/pytorch/pull/165995))
- [lint] workflow consistency linter to look at all files instead of just changed files ([#165171](https://github.com/pytorch/pytorch/pull/165171))
- [DebugMode] refactor logs into _DebugCalls ([#165376](https://github.com/pytorch/pytorch/pull/165376))
- [PyTorch] Improve aarch64 performance of bfloat16 ops ([#166028](https://github.com/pytorch/pytorch/pull/166028))
- [dynamo][user_defined] Replace UserFunctionVariable with VariableTracker build ([#165706](https://github.com/pytorch/pytorch/pull/165706))
- [Submodule] Bump FBGEMM to latest ([#165544](https://github.com/pytorch/pytorch/pull/165544))
- [CUDA] Add experimental green context support for SM carveout ([#159104](https://github.com/pytorch/pytorch/pull/159104))
- test for #165446 ([#165853](https://github.com/pytorch/pytorch/pull/165853))
- Use `is` rather than `==` to work around slow enum comparion in _ops.py ([#165936](https://github.com/pytorch/pytorch/pull/165936))
- Enable `torch.Generator` to support pytorch/xla generator implementation  ([#161369](https://github.com/pytorch/pytorch/pull/161369))
- [pytorch/kineto] Update Kineto Submodule ([#166150](https://github.com/pytorch/pytorch/pull/166150))
- [CUDA][cuBLAS] Fix a compilation issue in #163955 when CUDA_VERSION < 12010 ([#166137](https://github.com/pytorch/pytorch/pull/166137))
- [DebugMode] add nn.Module tracking ([#165498](https://github.com/pytorch/pytorch/pull/165498))
- Simplify the CUPTI  CMake check for kineto ([#161370](https://github.com/pytorch/pytorch/pull/161370))
- inductor: avoid unrolling argmin/argmax reductions to preserve index … ([#164040](https://github.com/pytorch/pytorch/pull/164040))
- [CUDA][Grouped Gemm] remove `xFail` on Group GEMM tests after fallback was added ([#165378](https://github.com/pytorch/pytorch/pull/165378))
- Fix accuracy for layernorm/rmsnorm benchmarking ([#166005](https://github.com/pytorch/pytorch/pull/166005))
- Allow BlockDescriptorOptions classes to be overridden In TritonKernel ([#165899](https://github.com/pytorch/pytorch/pull/165899))
- Add a Claude skill for writing docstrings. ([#166175](https://github.com/pytorch/pytorch/pull/166175))
- [lint] workflow consistency linter to look at all files instead of just changed files ([#165171](https://github.com/pytorch/pytorch/pull/165171))
- [OpenReg] Remove the Unnecessary Fallback Implementation for AutogradPrivate1 ([#165316](https://github.com/pytorch/pytorch/pull/165316))
- Export flex attention with kwargs and DTensor ([#166045](https://github.com/pytorch/pytorch/pull/166045))
- Remove likely unnecessary _EXPAND trick for non-windows in HIDDEN_NAMESPACE_BEGIN ([#166203](https://github.com/pytorch/pytorch/pull/166203))
- Factor out shared scaled mm routines ([#166139](https://github.com/pytorch/pytorch/pull/166139))
- Split grouped_mm methods into their own file ([#166140](https://github.com/pytorch/pytorch/pull/166140))
- Add file size limits to linters and refactor grep_linter ([#166202](https://github.com/pytorch/pytorch/pull/166202))
- [Intel GPU] Xpu matmul implementation for complex dtype ([#160867](https://github.com/pytorch/pytorch/pull/160867))
- [feat]: add optimized exp_u20 implementation from Arm Optimized Routi… ([#161049](https://github.com/pytorch/pytorch/pull/161049))
- [cuDNN][SDPA] Handle `c10:Error` when checking device capability for prefer-cuDNN SDPA check ([#166201](https://github.com/pytorch/pytorch/pull/166201))
- Simplify SingletonOrSharedTypePtr ([#166183](https://github.com/pytorch/pytorch/pull/166183))
- Propose Out-of-tree Backend Integration (PrivateUse1) as a module and FFFrog as the maintainer ([#165958](https://github.com/pytorch/pytorch/pull/165958))
- [BE][Opinfo] Mark `[c]double` as unsupported for MPS ([#166213](https://github.com/pytorch/pytorch/pull/166213))
- Enable Intel GPU on 4 unit test cases ([#165405](https://github.com/pytorch/pytorch/pull/165405))
- Update torch-xpu-ops commit pin ([#166129](https://github.com/pytorch/pytorch/pull/166129))
- Update slow tests ([#165894](https://github.com/pytorch/pytorch/pull/165894))
- [dynamo][misc] Replace UserFunctionVariable with VariableTracker build ([#165707](https://github.com/pytorch/pytorch/pull/165707))
- Add XLAHooksInterface to bazel file ([#166179](https://github.com/pytorch/pytorch/pull/166179))
- [torch/utils][Code Clean] Clean asserts in `torch/utils/*.py` ([#165410](https://github.com/pytorch/pytorch/pull/165410))
- [dynamo][guards] 1/N Guard selectively for DTensor ([#165824](https://github.com/pytorch/pytorch/pull/165824))
- Export flex attention with kwargs and DTensor ([#166045](https://github.com/pytorch/pytorch/pull/166045))
- Update cuDNN 9.10.2 in Manylinux 2.28 Docker files ([#165913](https://github.com/pytorch/pytorch/pull/165913))
- [cuDNN] Smoke-test runtime cuDNN version matches compile time version in CI ([#165922](https://github.com/pytorch/pytorch/pull/165922))
- [torchfuzz] check in more ignore regexes ([#166187](https://github.com/pytorch/pytorch/pull/166187))
- [torchfuzz] fix group norm operator ([#166188](https://github.com/pytorch/pytorch/pull/166188))
- [CD] Upgrade to CUDA 13.0.2 for nightly binaries ([#165470](https://github.com/pytorch/pytorch/pull/165470))
- [torchfuzz] make pointwise subclasses defined torch_op_name ([#166220](https://github.com/pytorch/pytorch/pull/166220))
- [torchfuzz] add sdpa operator ([#166189](https://github.com/pytorch/pytorch/pull/166189))
- [torchfuzz] add mhaf operator ([#166190](https://github.com/pytorch/pytorch/pull/166190))
- [torchfuzz] split, chunk, stack, cat, expand, gather, cumsum, clamp, index_select, split ([#166221](https://github.com/pytorch/pytorch/pull/166221))
- [hop] local_map MoE: fix unbacked symints during tracing and symint activations order in the wrapper ([#165551](https://github.com/pytorch/pytorch/pull/165551))
- [PyTorch] Use events from pool in copy_device_to_device ([#165647](https://github.com/pytorch/pytorch/pull/165647))
- [AMP][Refactor] Autocast dtype handling to simplify device-specific c… ([#165221](https://github.com/pytorch/pytorch/pull/165221))
- [Pytorch] Update Kineto Submodule ([#166317](https://github.com/pytorch/pytorch/pull/166317))
- [CD] Upgrade to CUDA 13.0.2 for nightly binaries ([#165470](https://github.com/pytorch/pytorch/pull/165470))
- [1/2] Split `cublasCommonArgs` into its own file ([#166313](https://github.com/pytorch/pytorch/pull/166313))
- [2/2] Move scaled_mm routines to their own file ([#166314](https://github.com/pytorch/pytorch/pull/166314))
- Use correct layout convention for skills ([#166265](https://github.com/pytorch/pytorch/pull/166265))
- Add a skill for writing skills ([#166266](https://github.com/pytorch/pytorch/pull/166266))
- Remove not needed code path.  ([#166278](https://github.com/pytorch/pytorch/pull/166278))
- Fixes torch.compile(nn.ModuleList()) changes bool() behavior  ([#159208](https://github.com/pytorch/pytorch/pull/159208))
- [ez] Fix print for failing test when entire file fails ([#166420](https://github.com/pytorch/pytorch/pull/166420))
- NVFP4 grouped gemm support via. FBGEMM kernels ([#166308](https://github.com/pytorch/pytorch/pull/166308))
- Change t.is_cuda to t.device.type == 'cuda' in torch/utils/viz ([#156418](https://github.com/pytorch/pytorch/pull/156418))
- [xpu][test] Enable skipped `SparseAdam` UTs ([#166375](https://github.com/pytorch/pytorch/pull/166375))
- [BE] Move GreenContext implementation details to cpp ([#166462](https://github.com/pytorch/pytorch/pull/166462))
- [OpenReg] Update Installation in README.md ([#166235](https://github.com/pytorch/pytorch/pull/166235))
- Suppress std::hardware_destructive_interference_size warning on GCC 13+ ([#166297](https://github.com/pytorch/pytorch/pull/166297))
- Introduce a new API torch.xpu.set_per_process_memory_fraction ([#165510](https://github.com/pytorch/pytorch/pull/165510))
- [user-cuda-streams] Add cuda streams test suite ([#162901](https://github.com/pytorch/pytorch/pull/162901))
- Move static from_ivalue/to_ivalue to new shim_common.cpp ([#166373](https://github.com/pytorch/pytorch/pull/166373))
- Fix incomplete torch.cdist tests ([#166507](https://github.com/pytorch/pytorch/pull/166507))
- Fix incomplete test_memory_plots_metadata ([#166508](https://github.com/pytorch/pytorch/pull/166508))
- FC/BC policy for libtorch stable ABI ([#163991](https://github.com/pytorch/pytorch/pull/163991))
- Remove METADATA.bzl files ([#166574](https://github.com/pytorch/pytorch/pull/166574))
- Fix comparing inductor actual strides vs bw graph for activations should not throw DDE.  ([#166277](https://github.com/pytorch/pytorch/pull/166277))
- Fix a  syntactic error in test_indexing.py ([#166390](https://github.com/pytorch/pytorch/pull/166390))
- address DDE in matmul decomp ([#166541](https://github.com/pytorch/pytorch/pull/166541))
- Introduce a new API torch.xpu.get_per_process_memory_fraction ([#165511](https://github.com/pytorch/pytorch/pull/165511))
- Enable verify_dynamo on Python 3.13 ([#166497](https://github.com/pytorch/pytorch/pull/166497))
- [CUDA] xFail `max-autotune` grouped gemm tests on devices with insufficient SM count ([#165921](https://github.com/pytorch/pytorch/pull/165921))
- Better 1x128, 128x128 error handling on non-Hopper ([#166639](https://github.com/pytorch/pytorch/pull/166639))
- Add warning when users have incomplete setup for type checking ([#166603](https://github.com/pytorch/pytorch/pull/166603))
- [BE] Move GreenContext implementation details to cpp ([#166462](https://github.com/pytorch/pytorch/pull/166462))
- Remove AT_USE_HIPSPARSE_GENERIC_API ([#166393](https://github.com/pytorch/pytorch/pull/166393))
- Fix torch.full with dynamic tensor fill_value in torch.compile ([#166554](https://github.com/pytorch/pytorch/pull/166554))
- [GraphPartition] cache get_free_symbol_uses ([#166338](https://github.com/pytorch/pytorch/pull/166338))
- [MTIAGraph][Pytorch][2/n] Add binding for Python to C++, and hook for Pytorch to Fbcode ([#165963](https://github.com/pytorch/pytorch/pull/165963))
- [BE][Typing][Dynamo] Type misc files in `torch/_dynamo/variables/` ([#166569](https://github.com/pytorch/pytorch/pull/166569))
- [nativert] Downcast triton double arguments to floats ([#166620](https://github.com/pytorch/pytorch/pull/166620))
- [BE][Typing][Dynamo] Type misc files in `torch/_dynamo/variables/` ([#166569](https://github.com/pytorch/pytorch/pull/166569))
- Cache even more work for return_and_correct_aliasing ([#166365](https://github.com/pytorch/pytorch/pull/166365))
- partitioner option to ignore partitioner_tag for abstract usage ([#166725](https://github.com/pytorch/pytorch/pull/166725))
- fix broken nn_convolution test ([#166666](https://github.com/pytorch/pytorch/pull/166666))
- Add missing device to namedtensor tests ([#166717](https://github.com/pytorch/pytorch/pull/166717))
- [BE] Move GreenContext implementation details to cpp ([#166462](https://github.com/pytorch/pytorch/pull/166462))
- [BE][Typing][Dynamo] Type misc files in `torch/_dynamo/variables/` ([#166569](https://github.com/pytorch/pytorch/pull/166569))
- [GraphPartition] cache get_free_symbol_uses ([#166338](https://github.com/pytorch/pytorch/pull/166338))
- Remove setup-env instructions; it's confusing ([#166749](https://github.com/pytorch/pytorch/pull/166749))
- Avoid DDE in narrow with unbacked start ([#166361](https://github.com/pytorch/pytorch/pull/166361))
- Fix comparing inductor actual strides vs bw graph for activations should not throw DDE.  ([#166277](https://github.com/pytorch/pytorch/pull/166277))
- [FP8][H100][TF32] Disable tf32 for emulated reference computation in `test_scaled_mm_vs_emulated_block_wise` ([#162997](https://github.com/pytorch/pytorch/pull/162997))
- report geomean for norm bwd benchmarking ([#166675](https://github.com/pytorch/pytorch/pull/166675))
- add a curve for customized compilation in the kernel benchmarking scripts ([#166697](https://github.com/pytorch/pytorch/pull/166697))
- Fix: list index out of range with softmax when using 0 dim ([#166547](https://github.com/pytorch/pytorch/pull/166547))
- [user-streams] Add basic stream tests ([#164523](https://github.com/pytorch/pytorch/pull/164523))
- Remove setup-env instructions; it's confusing ([#166749](https://github.com/pytorch/pytorch/pull/166749))
- Add claude skills for uint support and AT_DISPATCH_V2 ([#166814](https://github.com/pytorch/pytorch/pull/166814))
- [BE] Using std::move to reduce copy constructor calls by one. ([#163599](https://github.com/pytorch/pytorch/pull/163599))
- Fix: type promotion in FakeTensor ([#166522](https://github.com/pytorch/pytorch/pull/166522))
- Give full Dynamo stack traces in CI ([#160417](https://github.com/pytorch/pytorch/pull/160417))
- Remove nightly pth check from pyrefly ([#166857](https://github.com/pytorch/pytorch/pull/166857))
- Avoid std::copy_n in CopyKernel and IndexKernel ([#143544](https://github.com/pytorch/pytorch/pull/143544))
- Remove tools from BC linter ([#166858](https://github.com/pytorch/pytorch/pull/166858))
- [Inductor] addmm with bias -> unfuse bias if there is a pointwise/reduction consumer ([#166165](https://github.com/pytorch/pytorch/pull/166165))
- Fix build error by checking cuda version in CUDAGreenContext ([#166800](https://github.com/pytorch/pytorch/pull/166800))
- [FP8] Enable FP16 output support for torch scaled_mm when using CUTLASS on SM90 ([#166744](https://github.com/pytorch/pytorch/pull/166744))
- Avoid DDE in narrow with unbacked start ([#166361](https://github.com/pytorch/pytorch/pull/166361))
- [cuDNN][SDPA] Check-in test for #166211 ([#166570](https://github.com/pytorch/pytorch/pull/166570))
- [easy][MTIAGraph][Pytorch] clang-format files ([#166805](https://github.com/pytorch/pytorch/pull/166805))
- [distributed] Replace assert statements with AssertionError exceptions ([#165216](https://github.com/pytorch/pytorch/pull/165216))
- [CUDA] Skip pynvml test on platforms that don't have complete support ([#159689](https://github.com/pytorch/pytorch/pull/159689))
- Fix  unused assignments  ([#166791](https://github.com/pytorch/pytorch/pull/166791))
- [inductor] Expand use of generic benchmark function ([#164938](https://github.com/pytorch/pytorch/pull/164938))
- Fix cuda blas build error due to extra && ([#166811](https://github.com/pytorch/pytorch/pull/166811))
- [MTIAGraph][Pytorch][2.1/n] Add API to destroy graph C++ instance ([#166806](https://github.com/pytorch/pytorch/pull/166806))
- Fix torch.full with dynamic tensor fill_value in torch.compile ([#166554](https://github.com/pytorch/pytorch/pull/166554))
- fix test_type_hints ([#163150](https://github.com/pytorch/pytorch/pull/163150))
- Enable clang-tidy on some excluded headers ([#166835](https://github.com/pytorch/pytorch/pull/166835))
- Fix  unused assignments  ([#166791](https://github.com/pytorch/pytorch/pull/166791))
- Fixes torch.compile(nn.ModuleList()) changes bool() behavior  ([#159208](https://github.com/pytorch/pytorch/pull/159208))
- Fix MSCV C++ compilation error of `pycore_stackref.h` header ([#165686](https://github.com/pytorch/pytorch/pull/165686))
- [dynamo, 3.14] fix dynamo error message test for 3.14 ([#166894](https://github.com/pytorch/pytorch/pull/166894))
- [dynamo, 3.14] add explicit SymFloat int conversion ([#166902](https://github.com/pytorch/pytorch/pull/166902))
- [Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel ([#165036](https://github.com/pytorch/pytorch/pull/165036))
- [xpu][test] Add UT for expandable segments ([#166495](https://github.com/pytorch/pytorch/pull/166495))
- Fix DeepSeek scaling tensor handling ([#166752](https://github.com/pytorch/pytorch/pull/166752))
- [ez] Print some more test timing info in the logs ([#166447](https://github.com/pytorch/pytorch/pull/166447))
- [BE] Use `[[maybe_unused]]` ([#166865](https://github.com/pytorch/pytorch/pull/166865))
- [Inductor] addmm with bias -> unfuse bias if there is a pointwise/reduction consumer ([#166165](https://github.com/pytorch/pytorch/pull/166165))
- Delete deprecated fp32 precision warnings ([#166956](https://github.com/pytorch/pytorch/pull/166956))
- Fixed some syntax errors in SECURITY.md file. ([#166718](https://github.com/pytorch/pytorch/pull/166718))
- [cuDNN] Smoke-test runtime cuDNN version matches compile time version in CI ([#165922](https://github.com/pytorch/pytorch/pull/165922))
- More pyrefly local errors ([#166976](https://github.com/pytorch/pytorch/pull/166976))
- [aotd] Compiled saved tensor hooks context ([#166887](https://github.com/pytorch/pytorch/pull/166887))
- Upload test stats for trunk/sha tag ([#166916](https://github.com/pytorch/pytorch/pull/166916))
- Update triton to 3.5.1 release ([#166968](https://github.com/pytorch/pytorch/pull/166968))
- Avoid DDE in narrow with unbacked start ([#166361](https://github.com/pytorch/pytorch/pull/166361))
- make narrow_tensor_symint DDE-free ([#166379](https://github.com/pytorch/pytorch/pull/166379))
- [export] Codemod unittests to use new graph capture API ([#166957](https://github.com/pytorch/pytorch/pull/166957))
- Fix the vmap op fallback bug ([#166032](https://github.com/pytorch/pytorch/pull/166032))
- Annotation should be deepcopied ([#167017](https://github.com/pytorch/pytorch/pull/167017))
- Fix typos in complex numbers docs ([#166671](https://github.com/pytorch/pytorch/pull/166671))
- Fix typo in gloo_hip library name ([#166502](https://github.com/pytorch/pytorch/pull/166502))
- [Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel ([#167003](https://github.com/pytorch/pytorch/pull/167003))
- [CI] Parse xml and upload json while running ([#166988](https://github.com/pytorch/pytorch/pull/166988))
- [Code Clean] Replace `assert` with if statement and raise `AssertionError` ([#166935](https://github.com/pytorch/pytorch/pull/166935))
- Add some code for exploring the space of accessible size/stride configs via plain views ([#167076](https://github.com/pytorch/pytorch/pull/167076))
- Update triton to 3.5.1 release ([#166968](https://github.com/pytorch/pytorch/pull/166968))
- Update typing docs to reference pyrefly  ([#166883](https://github.com/pytorch/pytorch/pull/166883))
- Don't hardcode double argument for reduction base ([#166951](https://github.com/pytorch/pytorch/pull/166951))
- Remove python workaround for ContextDecorator ([#167049](https://github.com/pytorch/pytorch/pull/167049))
- [user-streams] Add backward test ([#167021](https://github.com/pytorch/pytorch/pull/167021))
- Add missing skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION) to test/test_transformers.py ([#166969](https://github.com/pytorch/pytorch/pull/166969))
- [cuBLAS] Force tensor-core-no-reduction algo in `cuBLASLt` for `n=1` cases ([#166735](https://github.com/pytorch/pytorch/pull/166735))
- Avoid DDE in narrow with unbacked start ([#166361](https://github.com/pytorch/pytorch/pull/166361))
- Fix duplicate benchmarking entries for addmm ([#166652](https://github.com/pytorch/pytorch/pull/166652))
- Refactor: extract OperatorArgsKwargsView from parseIValuesToPyArgsKwargs ([#166368](https://github.com/pytorch/pytorch/pull/166368))
- [dynamo, 3.14] disable dynamo cpython tests in 3.14 (again) ([#167000](https://github.com/pytorch/pytorch/pull/167000))
- Update `tensorpipe` submodule ([#167108](https://github.com/pytorch/pytorch/pull/167108))
- make narrow_tensor_symint DDE-free ([#166379](https://github.com/pytorch/pytorch/pull/166379))
- [MTIA][PyTorch] Add mtia as native device for PyTorch tests ([#167089](https://github.com/pytorch/pytorch/pull/167089))
- [Sigmoid][Delta Update][2/N] update delta update api to load original value first before casting to target dtype ([#167039](https://github.com/pytorch/pytorch/pull/167039))
- [Inductor][Grouped Gemm] Add Blackwell CuTeDSL Kernel ([#167182](https://github.com/pytorch/pytorch/pull/167182))
- [BE][Ez]: Update fmtlib submodule to 12.1.0 ([#166983](https://github.com/pytorch/pytorch/pull/166983))
- Update pythoncapi_compat.h ([#167138](https://github.com/pytorch/pytorch/pull/167138))
- Fix flaky memory profiler test ([#167168](https://github.com/pytorch/pytorch/pull/167168))
- Fix boxcox to return same result for same input in one batch ([#166986](https://github.com/pytorch/pytorch/pull/166986))
- [user-streams] add requires cuda decorator ([#167180](https://github.com/pytorch/pytorch/pull/167180))
- [user-streams] Add requires cuda to all test cases ([#167195](https://github.com/pytorch/pytorch/pull/167195))
- [BE][Typing][Dynamo] Type torch/_dynamo/variables/functions.py ([#167103](https://github.com/pytorch/pytorch/pull/167103))
- Add THO_DISPATCH_V2 macro ([#166629](https://github.com/pytorch/pytorch/pull/166629))
- Make pyrefly installable by lintrunner on Python-3.14 ([#167270](https://github.com/pytorch/pytorch/pull/167270))
- Update torch-xpu-ops commit pin ([#166945](https://github.com/pytorch/pytorch/pull/166945))
- Added a couple of utils for Pallas TPU backend. ([#167264](https://github.com/pytorch/pytorch/pull/167264))
- Unit test for torch.compile bmm dtype ([#167140](https://github.com/pytorch/pytorch/pull/167140))
- [ez] Remove some unused vars in common_utils.py ([#166453](https://github.com/pytorch/pytorch/pull/166453))
- [ez] Remove some dead code from test artifact related files ([#166966](https://github.com/pytorch/pytorch/pull/166966))
- Register floor_divide.out for MTIA ([#167280](https://github.com/pytorch/pytorch/pull/167280))
- Fix: list index out of range with softmax when using 0 dim ([#166547](https://github.com/pytorch/pytorch/pull/166547))
- [ez] Remove experiment for uploading all test runs ([#167133](https://github.com/pytorch/pytorch/pull/167133))
- [Test CI] Bump ruff to 0.14.4 ([#167286](https://github.com/pytorch/pytorch/pull/167286))
- [BE][Typing][Dynamo] Type torch/_dynamo/variables/functions.py ([#167103](https://github.com/pytorch/pytorch/pull/167103))
- [BE] use undeprecated from/to in libtorch_agnostic tests ([#167126](https://github.com/pytorch/pytorch/pull/167126))
- Update Kineto Submodule ([#167343](https://github.com/pytorch/pytorch/pull/167343))
- [CI][serialization] Fix exception regexes with Python-3.14 ([#167333](https://github.com/pytorch/pytorch/pull/167333))
- [dynamo][guards] 1/N Guard selectively for DTensor ([#165824](https://github.com/pytorch/pytorch/pull/165824))
- [BugFix] Fix compute_error in coo_mean_time and csr_mean_time ([#166795](https://github.com/pytorch/pytorch/pull/166795))
- another version of fixing CachingHostAllocatorImpl destructor ([#167408](https://github.com/pytorch/pytorch/pull/167408))
- Update pythoncapi_compat.h to 11cb80f2652cb2fe5231bf60b9dd98c83a4e25f4 ([#167413](https://github.com/pytorch/pytorch/pull/167413))
- Move types from typing_extensions to typing ([#167185](https://github.com/pytorch/pytorch/pull/167185))
- Separately handle null data_ptr storages when creating unique  ID ([#167405](https://github.com/pytorch/pytorch/pull/167405))
- Register functorch XPU/HPU dispatch keys ([#167095](https://github.com/pytorch/pytorch/pull/167095))
- Update slow tests ([#166844](https://github.com/pytorch/pytorch/pull/166844))
- Add the ruff rule and skip everything for now ([#167360](https://github.com/pytorch/pytorch/pull/167360))
- [BE] Fix out-of-bounds index_put in test_mps.py ([#167444](https://github.com/pytorch/pytorch/pull/167444))
- Fix flaky memory profiler test [2] ([#167268](https://github.com/pytorch/pytorch/pull/167268))
- Fix -ffunction-sections, -fdata-sections not being added on aarch64. ([#166407](https://github.com/pytorch/pytorch/pull/166407))
- [inductor, 3.14] fix itertools.product pickle error in test_cpu_repro ([#167382](https://github.com/pytorch/pytorch/pull/167382))
- [inductor, 3.14] catch pickle.PicklingError exceptions ([#167383](https://github.com/pytorch/pytorch/pull/167383))
- Use c7i.2xlarge for B200 build ([#167078](https://github.com/pytorch/pytorch/pull/167078))
- Support AC in default partitioner when functionalization is enabled ([#166610](https://github.com/pytorch/pytorch/pull/166610))
- [Pytorch] Extend OSS conversion benchmarks ([#167099](https://github.com/pytorch/pytorch/pull/167099))
- [MTIAGraph][Pytorch][3/n] Implement mtia_graph python wrapper in pytorch ([#166964](https://github.com/pytorch/pytorch/pull/166964))
- [torchbench][optimus] Add backend optimus ([#167357](https://github.com/pytorch/pytorch/pull/167357))
- [xpu][fix] Format XPU c10 and aten code ([#167298](https://github.com/pytorch/pytorch/pull/167298))
- [PT2] Supply index to fake tensors on mtia device ([#167457](https://github.com/pytorch/pytorch/pull/167457))
- Expose `THPVariable_Wrap()` with a type argument ([#167488](https://github.com/pytorch/pytorch/pull/167488))
- Fix check_compiler_is_gcc to detect versioned GCC compilers ([#167501](https://github.com/pytorch/pytorch/pull/167501))
- Fix command injection vulnerability in PCH compilation ([#167502](https://github.com/pytorch/pytorch/pull/167502))
- [XPU][Test] Enable XPU tests in inductor/test_analysis.py ([#166840](https://github.com/pytorch/pytorch/pull/166840))
- Update Arm copyright dates in `LICENSE` file ([#167529](https://github.com/pytorch/pytorch/pull/167529))
- Clarify that crashes/OOB accesses and not security threats ([#167519](https://github.com/pytorch/pytorch/pull/167519))
- Remove superflous/misplaced `TestFailure` specs ([#165989](https://github.com/pytorch/pytorch/pull/165989))
- [OpenReg] Initialize device stream states for all devices in `initOpenRegStreamsOnce` ([#167528](https://github.com/pytorch/pytorch/pull/167528))
- [DebugMode] record triton kernels, run-to-run determinism checks ([#167028](https://github.com/pytorch/pytorch/pull/167028))
- [DTensor] Add CPU instruction count benchmark for dispatch ([#167394](https://github.com/pytorch/pytorch/pull/167394))
- [xpu][test]port embedding indexing and native_mha test files for Intel GPU ([#165886](https://github.com/pytorch/pytorch/pull/165886))
- [OpenReg][Feat][Docs] Enrich hook implementation and add focused documentation ([#165980](https://github.com/pytorch/pytorch/pull/165980))
- [Fix XPU typo] Fix a comment typo of FindSYCLToolkit.cmake ([#165884](https://github.com/pytorch/pytorch/pull/165884))
- address DDE in matmul decomp ([#166541](https://github.com/pytorch/pytorch/pull/166541))
- [PyTorch] fix profiler issue with empty exported trace file ([#167601](https://github.com/pytorch/pytorch/pull/167601))
- [inductor, 3.14] fix itertools.product pickle error in test_cpu_repro ([#167382](https://github.com/pytorch/pytorch/pull/167382))
- [inductor, 3.14] catch pickle.PicklingError exceptions ([#167383](https://github.com/pytorch/pytorch/pull/167383))
- [EZ][BE] Remove unnecessary semicolon in Module.cpp ([#167756](https://github.com/pytorch/pytorch/pull/167756))
- address DDE in matmul decomp ([#166541](https://github.com/pytorch/pytorch/pull/166541))
- [3.14] Skip failing spherical_bessel_j0 tests ([#167691](https://github.com/pytorch/pytorch/pull/167691))
- [OpenReg][Feat][Docs] Enrich OpenReg device management implementation and add focused documentation ([#165897](https://github.com/pytorch/pytorch/pull/165897))
- Fix different seq length ([#167481](https://github.com/pytorch/pytorch/pull/167481))
- [ATen][CUDA] Add sm_121a flag for RowwiseScaledMM ([#167734](https://github.com/pytorch/pytorch/pull/167734))
- Re-land "Fix thread safety in getCurrentCUDABlasHandle and getCUDABlasLtWorkspace" ([#167722](https://github.com/pytorch/pytorch/pull/167722))
- Fix mvlgamma_ FPE crash on x86 with integer input ([#164230](https://github.com/pytorch/pytorch/pull/164230))
- [CUDA][Test] Add `serialTest()` to some `largeTensorTest` tests ([#167471](https://github.com/pytorch/pytorch/pull/167471))
- Tiling bug fix ([#167771](https://github.com/pytorch/pytorch/pull/167771))
- [BE]: Update NVTX submodule to 3.3.0 ([#167751](https://github.com/pytorch/pytorch/pull/167751))
- [doc] Add example for torch.is_storage ([#161898](https://github.com/pytorch/pytorch/pull/161898))
- [torch.export] Fix for flaky test_annotate_on_assert ([#167805](https://github.com/pytorch/pytorch/pull/167805))
- [BE] No need to pass const enum values by reference ([#167868](https://github.com/pytorch/pytorch/pull/167868))
- [targets2buck] Clean up get_pt_ops_deps ([#167690](https://github.com/pytorch/pytorch/pull/167690))
- Remove python workaround for ContextDecorator ([#167049](https://github.com/pytorch/pytorch/pull/167049))
- [CUDA][CUDA Graphs] Respect node-priority in `cudaGraphInstantiate` ([#167346](https://github.com/pytorch/pytorch/pull/167346))
- Skip stable diffusion models in torchbench, get tests and benchmarks green ([#167896](https://github.com/pytorch/pytorch/pull/167896))
- [HOP][print] Add functionalization (make sure ordering) for print ([#167016](https://github.com/pytorch/pytorch/pull/167016))
- Fix typo in FP16 accumulation section ([#167703](https://github.com/pytorch/pytorch/pull/167703))
- Fix grammar issues in C++ frontend documentation ([#167702](https://github.com/pytorch/pytorch/pull/167702))
- [xpu][fix]Fall back deterministic `index_copy` to `index_put` on XPU ([#167830](https://github.com/pytorch/pytorch/pull/167830))
- Refactor TensorAccessor for headeronly. ([#166855](https://github.com/pytorch/pytorch/pull/166855))
- Allow same triton kernels in export ([#167862](https://github.com/pytorch/pytorch/pull/167862))
- Re-land#2 "Fix thread safety in getCurrentCUDABlasHandle and getCUDABlasLtWorkspace" ([#167928](https://github.com/pytorch/pytorch/pull/167928))
- Update torch-xpu-ops commit pin ([#167698](https://github.com/pytorch/pytorch/pull/167698))
- [CD] Add libopenblas to dep list for AArch64+CPU whl ([#167841](https://github.com/pytorch/pytorch/pull/167841))
- Disable CUDA MXFP4 on non-B200 GPUs ([#167857](https://github.com/pytorch/pytorch/pull/167857))
- Improve benchmarks/dynamo:check_perf_csv output and failure summary ([#161728](https://github.com/pytorch/pytorch/pull/161728))
- [inductor] fix the decision of inner reduction ([#167697](https://github.com/pytorch/pytorch/pull/167697))
- Tiling bug fix ([#167771](https://github.com/pytorch/pytorch/pull/167771))
- [torchfuzz] set default device cuda ([#167938](https://github.com/pytorch/pytorch/pull/167938))
- [torchfuzz] update IGNORE_PATTERNS ([#167939](https://github.com/pytorch/pytorch/pull/167939))
- [torchfuzz] check in test_fuzzer_issue_167937 ([#168005](https://github.com/pytorch/pytorch/pull/168005))
- [torchfuzz] clean up ignore patterns ([#168006](https://github.com/pytorch/pytorch/pull/168006))
- [XPU] [Feature] [2/3] add fp8 scaled_mm_v2 implementation for XPU ([#167518](https://github.com/pytorch/pytorch/pull/167518))
- [pytree][compile] Slightly faster TreeSpec init ([#168024](https://github.com/pytorch/pytorch/pull/168024))
- Fix TORCH_FEATURE_VERSION guards ([#167802](https://github.com/pytorch/pytorch/pull/167802))
- Split libtorch agnostic tests by feature version ([#167803](https://github.com/pytorch/pytorch/pull/167803))
- Test that TORCH_FEATURE_VERSION guards are used where needed ([#167962](https://github.com/pytorch/pytorch/pull/167962))
- [ATen][CUDA] Add sm_121a flag for RowwiseScaledMM ([#167734](https://github.com/pytorch/pytorch/pull/167734))
- [refcycle-logger] Output tensor size in the refcycle visualization ([#167079](https://github.com/pytorch/pytorch/pull/167079))
- [xpu][test] port some distributed tensor test files for Intel GPU ([#161703](https://github.com/pytorch/pytorch/pull/161703))
- typo corrected in type.cpp ([#167907](https://github.com/pytorch/pytorch/pull/167907))
- Fix stable ABI to/from deprecation warnings. Add my_shape test. ([#167923](https://github.com/pytorch/pytorch/pull/167923))
- [2/3][XPU][feature] The implementation of MemPool for XPU ([#166833](https://github.com/pytorch/pytorch/pull/166833))
- Update AGENTS.md ([#168111](https://github.com/pytorch/pytorch/pull/168111))
- Shrink binary size ([#168080](https://github.com/pytorch/pytorch/pull/168080))
- Improve build logic in activities for kineto ([#167204](https://github.com/pytorch/pytorch/pull/167204))
- [invoke_subgraph] Don't run the graph twice when autograd enabled ([#167245](https://github.com/pytorch/pytorch/pull/167245))
- [fix] Assign CUDAEvent external member properly ([#167711](https://github.com/pytorch/pytorch/pull/167711))
- [DeviceMemory] Add Basic Statistics to Device Memory in OpenReg ([#166395](https://github.com/pytorch/pytorch/pull/166395))
- Error when non stable/headeronly/shim headers are included by stable extension ([#167855](https://github.com/pytorch/pytorch/pull/167855))
- Fix link for core maintainers request form ([#168089](https://github.com/pytorch/pytorch/pull/168089))
- [dynamo][compile time] Special case for torch.utils._pytree._get_node_type ([#168054](https://github.com/pytorch/pytorch/pull/168054))
- [dynamo][pytree][compile time] Specialize tree_is_leaf ([#168070](https://github.com/pytorch/pytorch/pull/168070))
- Support AC in default partitioner when functionalization is enabled ([#166610](https://github.com/pytorch/pytorch/pull/166610))
- Fix tensor -> scalar variant swap ([#168007](https://github.com/pytorch/pytorch/pull/168007))
- [DebugMode] wait before hashing collectives by default ([#168119](https://github.com/pytorch/pytorch/pull/168119))
- [pytree][compile] Slightly faster TreeSpec init ([#168024](https://github.com/pytorch/pytorch/pull/168024))
- conv: refactor for lookup table support ([#167179](https://github.com/pytorch/pytorch/pull/167179))
- Change NamedTupleVariable implementation to subclass UserDefinedTupleVariable ([#167468](https://github.com/pytorch/pytorch/pull/167468))
- [torch/utils/data] Update CODEOWNERS ([#168172](https://github.com/pytorch/pytorch/pull/168172))
- [3.14] Add python version adjustment for frame count changes ([#168190](https://github.com/pytorch/pytorch/pull/168190))
- Improve OpenReg test coverage ([#167819](https://github.com/pytorch/pytorch/pull/167819))
- Add basic spin linting documentation ([#167227](https://github.com/pytorch/pytorch/pull/167227))
- [BE] Update xpu driver repo for CD used almalinux 8.10 ([#157356](https://github.com/pytorch/pytorch/pull/157356))
- Fixes floor divide int min overflow issue ([#166127](https://github.com/pytorch/pytorch/pull/166127))
- Fixes remainder and fmod operation and makes it same as cuda ([#165833](https://github.com/pytorch/pytorch/pull/165833))
- Re-enable ConvTranspose operator benchmarks for AArch64 ([#166731](https://github.com/pytorch/pytorch/pull/166731))
- Fix debug assertion in autograd_not_implemented_fallback.cpp ([#168280](https://github.com/pytorch/pytorch/pull/168280))
- [varlen attn] batch invariance testing ([#167865](https://github.com/pytorch/pytorch/pull/167865))
- Skip _assert_scalar in default partitioner ([#168289](https://github.com/pytorch/pytorch/pull/168289))
- [doc] README add cmake prefix for non-conda env ([#167714](https://github.com/pytorch/pytorch/pull/167714))
- [3.14] Use refcount difference for TestNumPyInterop.test_from_numpy_no_leak_on_invalid_dtype ([#168191](https://github.com/pytorch/pytorch/pull/168191))
- [dynamo][compile time] Special case for torch.utils._pytree._get_node_type ([#168054](https://github.com/pytorch/pytorch/pull/168054))
- Improve build logic in activities for kineto ([#167204](https://github.com/pytorch/pytorch/pull/167204))
- bucketing compile time improve ([#168122](https://github.com/pytorch/pytorch/pull/168122))
- [Full Inductor][Pytorch] Prevent decomposition and enable fallback of aten.native_layer_norm for MTIA ([#168290](https://github.com/pytorch/pytorch/pull/168290))
- [3.14] Update profiler test ([#168205](https://github.com/pytorch/pytorch/pull/168205))
- Remove c10::is_pod ([#166383](https://github.com/pytorch/pytorch/pull/166383))
- Update numpy tests for python 3.11/3.12 ([#168299](https://github.com/pytorch/pytorch/pull/168299))
- Revert #154859 ([#168297](https://github.com/pytorch/pytorch/pull/168297))
- [inductor] Use custom triton kernel subclass when available ([#167456](https://github.com/pytorch/pytorch/pull/167456))
- Fix edge-data handling in cudaGraphNodeGetDependencies for CUDA 13 in graph_capture_record_stream_reuse ([#168305](https://github.com/pytorch/pytorch/pull/168305))
- Fix cublasLtMatmul failure ([#167873](https://github.com/pytorch/pytorch/pull/167873))
- Use r7i.4xlarge for B200 build ([#167078](https://github.com/pytorch/pytorch/pull/167078))
- [Inductor XPU GEMM] Step 2/N: Move out cutlass files from torch/_inductor/codegen/cuda ([#160685](https://github.com/pytorch/pytorch/pull/160685))
- [CUDA] Update minimum NVIDIA driver version requirement in Green Context test ([#168188](https://github.com/pytorch/pytorch/pull/168188))
- Add template for add_overflows ([#168035](https://github.com/pytorch/pytorch/pull/168035))
- Fix memory leak test for SDPA op call ([#168040](https://github.com/pytorch/pytorch/pull/168040))
- Fix `hash(Size([SymInt, ...]))` on Python 3.14+ ([#168256](https://github.com/pytorch/pytorch/pull/168256))
- Revert #154859 ([#168297](https://github.com/pytorch/pytorch/pull/168297))
- [dynamo][pytree][compile time] Specialize tree_is_leaf ([#168070](https://github.com/pytorch/pytorch/pull/168070))
- [DTensor] update redistribute_cost, add disable_graph_based_transform ([#166747](https://github.com/pytorch/pytorch/pull/166747))
- [NativeRT] Fix out_t index handling in TritonKernel ([#168384](https://github.com/pytorch/pytorch/pull/168384))
- Fix lints with newer triton ([#168340](https://github.com/pytorch/pytorch/pull/168340))
- [BugFix] Fix incorrect type hint.  ([#168892](https://github.com/pytorch/pytorch/pull/168892))
- [BE] Delete `missing_vXXX_neon headers ([#168909](https://github.com/pytorch/pytorch/pull/168909))
- [CUDA] Fix truncated error messages in cudaMallocAsync Allocator ([#168369](https://github.com/pytorch/pytorch/pull/168369))
- [tutorial] typo fix, update torch.compiler_cudagraph_trees.md ([#167713](https://github.com/pytorch/pytorch/pull/167713))
- Removed deprecated `split_cat_fx_passes` ([#167738](https://github.com/pytorch/pytorch/pull/167738))
- Checking if the input is finite before calculation in lowering of pow func ([#167723](https://github.com/pytorch/pytorch/pull/167723))
- Fix local_map default partitioner issue ([#168396](https://github.com/pytorch/pytorch/pull/168396))
- [AOTI] Skip emit_multi_arch_kernel when CUDA version is lower than 12.8 ([#168985](https://github.com/pytorch/pytorch/pull/168985))
- [invoke_subgraph] Don't run the graph twice when autograd enabled ([#167245](https://github.com/pytorch/pytorch/pull/167245))
- inductor: fix failure in test_flex_decoding on <sm80 ([#165404](https://github.com/pytorch/pytorch/pull/165404))
- Adding check for step size=0 in unfold backward to avoid divide by 0 … ([#162720](https://github.com/pytorch/pytorch/pull/162720))
- [6/N] Use key in dict for existence checks ([#168350](https://github.com/pytorch/pytorch/pull/168350))
- Remove cudaProfilerInitialize ([#168918](https://github.com/pytorch/pytorch/pull/168918))
- Remove unused inplace loop in test_conv2d_clamp of test_jit_llga_fuser.py‎ ([#166691](https://github.com/pytorch/pytorch/pull/166691))
- [CUDA][BugFix] fix truncated error messages ([#168942](https://github.com/pytorch/pytorch/pull/168942))
- [BE][5/5] fix typos in aten/ (aten/src/ATen/) ([#157554](https://github.com/pytorch/pytorch/pull/157554))
- [Durin] Bump Kineto Submodule to latest ([#169098](https://github.com/pytorch/pytorch/pull/169098))
- [openreg] Expand autocast test coverage for custom device ([#169029](https://github.com/pytorch/pytorch/pull/169029))
- Add `torch.Tensor.__annotate__` to the `testing_ignore` list ([#169013](https://github.com/pytorch/pytorch/pull/169013))
- [dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker ([#169204](https://github.com/pytorch/pytorch/pull/169204))
- [3.14] Skip broken numpy test ([#169030](https://github.com/pytorch/pytorch/pull/169030))
- [3.14] Fix dynamo error on np.broadcast_shapes ([#168888](https://github.com/pytorch/pytorch/pull/168888))
- Remove the CUPTI  CMake check for kineto ([#161370](https://github.com/pytorch/pytorch/pull/161370))
- Replace vscode recommendation for type checker ([#169021](https://github.com/pytorch/pytorch/pull/169021))
- bucketing compile time improve ([#168122](https://github.com/pytorch/pytorch/pull/168122))
- [xpu][feature][1/N] Enable SDPA XPU FlashAttention backend with SYCL-TLA implementation  ([#169101](https://github.com/pytorch/pytorch/pull/169101))
- [10/N] Use Python 3.10 typing  ([#169229](https://github.com/pytorch/pytorch/pull/169229))
- [precompile] generate nonce key if enable_aot_compile is enabled ([#169244](https://github.com/pytorch/pytorch/pull/169244))
- [dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker ([#169204](https://github.com/pytorch/pytorch/pull/169204))
- Refactor: Remove unnecessary ConstantVariable wrapping in raise_observed_exception ([#168337](https://github.com/pytorch/pytorch/pull/168337))
- [MTIAGraph][Pytorch] Add the graph_pool_handle api ([#169283](https://github.com/pytorch/pytorch/pull/169283))
- [Dynamo][Guards]Fix TLParse CPP guard message with sorting get_leaf_guards and verbose_code_parts ([#169102](https://github.com/pytorch/pytorch/pull/169102))
- Support AC in default partitioner when functionalization is enabled ([#166610](https://github.com/pytorch/pytorch/pull/166610))
- [Submodule] Update to cutlass 4.3 ([#168308](https://github.com/pytorch/pytorch/pull/168308))
- Triton 3.6 pin update ([#168096](https://github.com/pytorch/pytorch/pull/168096))
- [ez] Remove maybe unused var in common_utils.py ([#166455](https://github.com/pytorch/pytorch/pull/166455))
- [GB300] [Triton] Disable experimental Triton API inside PyTorch when specified ([#169014](https://github.com/pytorch/pytorch/pull/169014))
- [dynamo, guards] add guard builder microbenchmark ([#169087](https://github.com/pytorch/pytorch/pull/169087))
- Silent XNNPACK GCC14 warnings ([#166873](https://github.com/pytorch/pytorch/pull/166873))
- [xpu][feature] Upgrade XPU OneDNN to v3.10.2 ([#169443](https://github.com/pytorch/pytorch/pull/169443))
- [generator] Close all open generators in compile_subgraph ([#157149](https://github.com/pytorch/pytorch/pull/157149))
- Support AC rematerializing in forward+loss+bwd graph ([#168082](https://github.com/pytorch/pytorch/pull/168082))
- [Submodule] cutlass bump to minor bug fix ([#169433](https://github.com/pytorch/pytorch/pull/169433))
- [dynamo, 3.14] Don't use _PyObject_GC_TRACK which is internal and calls an unexported API symbol ([#169490](https://github.com/pytorch/pytorch/pull/169490))
- update the MHA to enable cudnn-frontend 1.12.1 ([#169086](https://github.com/pytorch/pytorch/pull/169086))
- [effect] Remove special handling for profiler op ([#168389](https://github.com/pytorch/pytorch/pull/168389))
- Support module.to in strict export ([#167555](https://github.com/pytorch/pytorch/pull/167555))
- Add public documentation for stable_topological_sort ([#169498](https://github.com/pytorch/pytorch/pull/169498))
- [opaque obj] Improve error msg for intermediate opaques ([#167742](https://github.com/pytorch/pytorch/pull/167742))
- [Dynamo][Guard]Add the user-friendly TYPE_MATCH for type ([#169025](https://github.com/pytorch/pytorch/pull/169025))
- Update slow tests ([#167967](https://github.com/pytorch/pytorch/pull/167967))
- [effect] Remove special handling for profiler op ([#168389](https://github.com/pytorch/pytorch/pull/168389))
- Replace `msg` by `args` in `raise_observed_exception` ([#169343](https://github.com/pytorch/pytorch/pull/169343))
- Fix torch.fx for the newer "|" union syntax ([#169453](https://github.com/pytorch/pytorch/pull/169453))
- Fix torch.fx for the newer "|" union syntax ([#169453](https://github.com/pytorch/pytorch/pull/169453))
- [10/N] Use Python 3.10 typing  ([#169229](https://github.com/pytorch/pytorch/pull/169229))
- Restore ability to use global stableivalue from without template ([#169475](https://github.com/pytorch/pytorch/pull/169475))
- [Inductor] ReLU/GELU(Addmm) fusions ([#168157](https://github.com/pytorch/pytorch/pull/168157))
- Strengthen the implementation of OpenReg Stream ([#166115](https://github.com/pytorch/pytorch/pull/166115))
- Triton 3.6 pin update ([#168096](https://github.com/pytorch/pytorch/pull/168096))
- Clarify checkpointing docs ([#169007](https://github.com/pytorch/pytorch/pull/169007))
- Add a single GPU variant of modded-nanogpt to torchbench (#169502) ([#169505](https://github.com/pytorch/pytorch/pull/169505))
- expandable_segments + memory pool ([#169491](https://github.com/pytorch/pytorch/pull/169491))
- [opaque obj] Improve error msg for intermediate opaques ([#167742](https://github.com/pytorch/pytorch/pull/167742))
- [dynamo] Refactor isinstance(x, ConstantVariable) to x.is_python_constant() ([#169006](https://github.com/pytorch/pytorch/pull/169006))
- [DLPack] C Functions for DLPack Speed Exchange and Stream Handling ([#165483](https://github.com/pytorch/pytorch/pull/165483))
- Remove unnecessary code in exception registration ([#169365](https://github.com/pytorch/pytorch/pull/169365))
- Remove outdated Makefile targets ([#164679](https://github.com/pytorch/pytorch/pull/164679))
- improve accuracy of division by scalar ([#169507](https://github.com/pytorch/pytorch/pull/169507))
- Remove outdated skip conditons of CUDA and ROCm ([#166391](https://github.com/pytorch/pytorch/pull/166391))
- Fix log_sigmoid_backward_batch_rule on XPU ([#169215](https://github.com/pytorch/pytorch/pull/169215))
- [Dynamo][Guard]Add the user-friendly TYPE_MATCH for type ([#169025](https://github.com/pytorch/pytorch/pull/169025))
- [BE] Delete `install_vision` from Docker builds ([#169609](https://github.com/pytorch/pytorch/pull/169609))
- [Dynamo][Guards]Fix TLParse CPP guard message with sorting get_leaf_guards and verbose_code_parts ([#169102](https://github.com/pytorch/pytorch/pull/169102))
- Triton 3.6 update. Add CooperativeReductionTests to slow ([#169630](https://github.com/pytorch/pytorch/pull/169630))
- activation offloading implementation ([#167880](https://github.com/pytorch/pytorch/pull/167880))
- activation offloading reordering for comp<>comm overlaps ([#168316](https://github.com/pytorch/pytorch/pull/168316))
- [precompile] disable dispatch when deepcloning in PrecompileContext.record_artifact ([#169242](https://github.com/pytorch/pytorch/pull/169242))
- Correct some grammatical and expression errors in the CONTRIBUTING.md ([#167926](https://github.com/pytorch/pytorch/pull/167926))
- Replace `msg` by `args` in `raise_observed_exception` ([#169343](https://github.com/pytorch/pytorch/pull/169343))
- activation offloading implementation ([#167880](https://github.com/pytorch/pytorch/pull/167880))
- activation offloading reordering for comp<>comm overlaps ([#168316](https://github.com/pytorch/pytorch/pull/168316))
- [CUDA][CUDA Graphs] Use `count` when allocating storage for `edgeData` ([#169576](https://github.com/pytorch/pytorch/pull/169576))
- [pallas backend] skip aoti tests ([#169652](https://github.com/pytorch/pytorch/pull/169652))
- TestTritonDotReduction.test_matmul_fp16 correct tol ([#169682](https://github.com/pytorch/pytorch/pull/169682))
- [BE] documenting more functions ([#167439](https://github.com/pytorch/pytorch/pull/167439))
- Warn earlier when there is no annotated bwd ([#169686](https://github.com/pytorch/pytorch/pull/169686))
- Make hop semantics match other ops in PyTorch ([#169704](https://github.com/pytorch/pytorch/pull/169704))
- Support for vector of Tensors in autograd::Function ([#169155](https://github.com/pytorch/pytorch/pull/169155))
- [Dynamo][Guard]Add the user-friendly TYPE_MATCH for type ([#169025](https://github.com/pytorch/pytorch/pull/169025))
- Mempool use_on_oom order ([#169699](https://github.com/pytorch/pytorch/pull/169699))
- [dynamo][dicts] Decentralize and Improve key hash implementation for Dict variable tracker ([#169204](https://github.com/pytorch/pytorch/pull/169204))
- [precompile] disable dispatch when deepcloning in PrecompileContext.record_artifact ([#169242](https://github.com/pytorch/pytorch/pull/169242))
- [pallas backend] add remaining CPU tests as xfail so that passing ones ([#169655](https://github.com/pytorch/pytorch/pull/169655))
- Document: torch.compile support limitations for Python ([#169401](https://github.com/pytorch/pytorch/pull/169401))
- [dynamo] Rehaul the autograd.Function support ([#166788](https://github.com/pytorch/pytorch/pull/166788))
- Fix test_dlpack_tensor_on_different_device to use torch.cuda.device ([#169568](https://github.com/pytorch/pytorch/pull/169568))
- [dynamo][hops] Allow side effects in autograd.Function forward graph tracing ([#169399](https://github.com/pytorch/pytorch/pull/169399))
- [xpu][fix] Fix OneDNN deconv when using output_padding ([#169176](https://github.com/pytorch/pytorch/pull/169176))
- Add EikanWang and gujinghui as codeowners for get_start_xpu.rst ([#161890](https://github.com/pytorch/pytorch/pull/161890))
- [Inductor XPU GEMM] Step 2/N: Move out cutlass files from torch/_inductor/codegen/cuda ([#160685](https://github.com/pytorch/pytorch/pull/160685))
- [AArch64][Build] allow missing cutlass file if CUDA disabled ([#167720](https://github.com/pytorch/pytorch/pull/167720))
- Adjust the `gradcheck` tests for `ComplexTensor` ([#168248](https://github.com/pytorch/pytorch/pull/168248))
- [Inductor] ReLU/GELU(Addmm) fusions ([#168157](https://github.com/pytorch/pytorch/pull/168157))
- [OpenReg][Guard] Enrich OpenReg guard implementation and add focused documentation ([#166288](https://github.com/pytorch/pytorch/pull/166288))
- [DLPack] Change C Exchange API to __dlpack_c_exchange_api__ PyCapsule ([#169724](https://github.com/pytorch/pytorch/pull/169724))
- [CI] Add eager tests for CUDA 13.0 ([#167207](https://github.com/pytorch/pytorch/pull/167207))
- [dynamo] Refactor isinstance(x, ConstantVariable) to x.is_python_constant() ([#169006](https://github.com/pytorch/pytorch/pull/169006))
- Fix vhappi_common_backend GCC build by moving Clang-specific flags ([#169746](https://github.com/pytorch/pytorch/pull/169746))
- Update FBGEMM 3rd-party dependency ([#169744](https://github.com/pytorch/pytorch/pull/169744))
- [TEST][SDPA] Use `assertLessOrEqual` in varlen test ([#169843](https://github.com/pytorch/pytorch/pull/169843))
- Fix grouped_gemm docs ([#169862](https://github.com/pytorch/pytorch/pull/169862))
- Skip tests requiring flash attn on <sm80 ([#165300](https://github.com/pytorch/pytorch/pull/165300))
- [BE] Remove ideep.hpp include in Version.cpp ([#169871](https://github.com/pytorch/pytorch/pull/169871))
- [BE] [1/N] IDeep unraveling ([#169884](https://github.com/pytorch/pytorch/pull/169884))
- [BE][2/N] Replace IDEEP_PREREQ with DNNL_PREPREQ ([#169885](https://github.com/pytorch/pytorch/pull/169885))
- dynamo torch factory functions apply default device mirroring __torch… ([#169081](https://github.com/pytorch/pytorch/pull/169081))
- Update expected results for basic_modules_ListOfLinears_eager ([#169895](https://github.com/pytorch/pytorch/pull/169895))
- Escaped html tags name and target to appear as strings ([#165543](https://github.com/pytorch/pytorch/pull/165543))
- [BE][Typing][Dynamo] Type torch/_dynamo/variables/user_defined.py ([#169319](https://github.com/pytorch/pytorch/pull/169319))
- Modify log1p and exp1 to use ulp guided tolerances ([#168323](https://github.com/pytorch/pytorch/pull/168323))
- [OpenReg] Fixed the issue where streams and events could still be bou… ([#169052](https://github.com/pytorch/pytorch/pull/169052))
- [openreg] Add docstrings and expand test coverage ([#169344](https://github.com/pytorch/pytorch/pull/169344))
- Document PYTORCH_ALLOC_CONFIG and mark PYTORCH_CUDA_ALLOC_CONFIG as its alias ([#167659](https://github.com/pytorch/pytorch/pull/167659))
- [OpenReg][Feat] Add OpenReg profiler integration tests and implementation ([#168153](https://github.com/pytorch/pytorch/pull/168153))
- [OpenReg][Docs] Optimize docs for better expression ([#169913](https://github.com/pytorch/pytorch/pull/169913))
- Ceanup FBGEMM MXFP4 integration and fix undefined reference error ([#169853](https://github.com/pytorch/pytorch/pull/169853))
- [Official Docker build] Use uv instead of conda  ([#169873](https://github.com/pytorch/pytorch/pull/169873))
- Fix evaluating sympy constant causes error ([#169726](https://github.com/pytorch/pytorch/pull/169726))
- [BE] Improve `TestMPS.test_inverse` ([#169922](https://github.com/pytorch/pytorch/pull/169922))
- [profiler] Change dynamo_timed to use torch._C._profiler._RecordFunctionFast ([#169858](https://github.com/pytorch/pytorch/pull/169858))
- [MTIA][RNG] Add torch bindings for get and set default generator ([#169271](https://github.com/pytorch/pytorch/pull/169271))
- Add conditions to test_main_loop_scaling ([#169317](https://github.com/pytorch/pytorch/pull/169317))
- Bugfix: Make BUILD_TEST default match match CMake ([#166892](https://github.com/pytorch/pytorch/pull/166892))
- Silence expected double registration warning ([#169900](https://github.com/pytorch/pytorch/pull/169900))
- Remove use of deprecated NumPy symbols. ([#169695](https://github.com/pytorch/pytorch/pull/169695))
- [xpu][fix] Fix Inductor UT failure introduced by community. ([#169813](https://github.com/pytorch/pytorch/pull/169813))
- [partitioner] New configuration for considering custom operations in knapsack optimization (#167083) ([#169636](https://github.com/pytorch/pytorch/pull/169636))
- [inductor][fx] clarify padding logic ([#169577](https://github.com/pytorch/pytorch/pull/169577))
- [tests] Align SDPA tests with cuDNN versions ([#169321](https://github.com/pytorch/pytorch/pull/169321))
- [CI] Add eager tests for CUDA 13.0 ([#167207](https://github.com/pytorch/pytorch/pull/167207))
### security
