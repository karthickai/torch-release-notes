
# Release Notes worksheet distributed

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## distributed
### bc breaking
### deprecation
### new features
### improvements
- c10d
  - Added improvements to eager init of `ProcessGroupNCCL` ([#156748](https://github.com/pytorch/pytorch/pull/156748))
  - Simplified unique hash management of `ProcessGroupNCCL` ([#156790](https://github.com/pytorch/pytorch/pull/156790))
  - Supported per operation timeouts in `ProcessGroupGloo` ([#158128](https://github.com/pytorch/pytorch/pull/158128))
  - Allowed ping to be retried in `TCPStore` ([#159165](https://github.com/pytorch/pytorch/pull/159165))
  - supported scalar tensor for functional `all_gather` ([#149913](https://github.com/pytorch/pytorch/pull/149913))
  - Exposed `unsafe_get_ptr` for dist.ProcessGroupNCCL.NCCLConfig ([#161136](https://github.com/pytorch/pytorch/pull/161136))
  - Added batch option for `send/recv_object_list` ([#160342](https://github.com/pytorch/pytorch/pull/160342))
  - Made it a not require FakeStore to be passed into fake backend ([#162164](https://github.com/pytorch/pytorch/pull/162164))
  - Enabled complex datatype support in `ProcessGroupGloo` ([#156633](https://github.com/pytorch/pytorch/pull/156633))
  - Moved thread-local capture mode guard to include `work.isStarted` ([#160398](https://github.com/pytorch/pytorch/pull/160398))
- DistributedDataParallel (DDP)
  - Support ddp zero hook XCCL path ([#159240](https://github.com/pytorch/pytorch/pull/159240))
- DTensor
  - Relaxed `device_mesh` argument constraint in `local_map` ([#157049](https://github.com/pytorch/pytorch/pull/157049))
  - Supported complex numbers in DTensor redistribute ([#157329](https://github.com/pytorch/pytorch/pull/157329))
  - Reworked partial propagation in point-wise op and support mul ([#157340](https://github.com/pytorch/pytorch/pull/157340))
  - Allowed dynamic shapes for `DTensor` slice ([#157953](https://github.com/pytorch/pytorch/pull/157953))
  - Implemented `histc` op ([#158298](https://github.com/pytorch/pytorch/pull/158298))
  - Made dispatch to sharding prop over decomps ([#159324](https://github.com/pytorch/pytorch/pull/159324))
  - Supported user-supplied Generator for random ops ([#159933](https://github.com/pytorch/pytorch/pull/159933))
  - Added `propagate_tensor_meta` function that skips cache if _are_we_tracing ([#161334](https://github.com/pytorch/pytorch/pull/161334))
  - Supported `local_map` as a decorator ([#161353](https://github.com/pytorch/pytorch/pull/161353))
- Device mesh
  - Enabled the use of user set backend and pg option even for the global mesh ([#157501](https://github.com/pytorch/pytorch/pull/157501))
  - Enabled slicing a submesh with warnings ([#158899](https://github.com/pytorch/pytorch/pull/158899))
  - Allowed controlling PG backend and options via `init_device_mesh` ([#159371](https://github.com/pytorch/pytorch/pull/159371))
- FullyShardedDataParallel2 (FSDP2)
  - Supported custom `all_gather` and `reduce_scatter` comms ([#155189](https://github.com/pytorch/pytorch/pull/155189))
  - Made it fail `set_allocate_memory_from_process_group` if used together with custom comm hooks ([#157487](https://github.com/pytorch/pytorch/pull/157487))
  - Used `reduceOpSum` for world size 1 ([#157529](https://github.com/pytorch/pytorch/pull/157529))
  - Skipped `allgather` when world size is 1 ([#160135](https://github.com/pytorch/pytorch/pull/160135))
  - Used `post_reduce_stream.record_event()` on hsdp+cpuoffload ([#160481](https://github.com/pytorch/pytorch/pull/160481))
- Tensor Parallel (TP)
  - Improved parallelize_module API to support more cases ([#157182](https://github.com/pytorch/pytorch/pull/157182))
- TensorPipe
  - Updated TensorPipe pinned dependency version ([#159834](https://github.com/pytorch/pytorch/pull/159834))
- TorchElastic
  - Enabled NUMA binding integration with elastic agent and torchrun ([#149334](https://github.com/pytorch/pytorch/pull/149334))
  - Supported NUMA Binding for Callable Entrypoints ([#160163](https://github.com/pytorch/pytorch/pull/160163), [#161183](https://github.com/pytorch/pytorch/pull/161183))
- Pipeline Parallelism (PP)
  - Added `eval()` API to schedule ([#157795](https://github.com/pytorch/pytorch/pull/157795))
  - Allowed intermediate nodes in zero bubble to have multiple grads ([#159084](https://github.com/pytorch/pytorch/pull/159084))
  - Supported `OVERLAP_F_B` computation type ([#158978](https://github.com/pytorch/pytorch/pull/158978))
  - Initialized P2P communicators on first step ([#160210](https://github.com/pytorch/pytorch/pull/160210))
  - Added `DualPipeV` schedule ([#159591](https://github.com/pytorch/pytorch/pull/159591))
### bug fixes
- c10d
  - Fixed slow init due to repeated dns resolution failure in socket ([#159596](https://github.com/pytorch/pytorch/pull/159596))
  - Fixed `setGroupName` and `setGroupDesc` in `group_split` and `merge_remote_group` ([#159429](https://github.com/pytorch/pytorch/pull/159429))
  - Fixed a bug of distributed 'gather' with noncontiguous tensors on the Gloo backend ([#158903](https://github.com/pytorch/pytorch/pull/158903))
  - Fix a bug of distributed 'gather' with noncontiguous tensors on the NCCL backend. ([#159549](https://github.com/pytorch/pytorch/pull/159549))
- DeviceMesh
  - Fixed the not incorrectly chained each of the strings as iterables ([#160709](https://github.com/pytorch/pytorch/pull/160709))
- DistributedDataParallel (DDP)
  - Fixed incorrect interaction between `DDPOptimizer` and donated buffers ([#160745](https://github.com/pytorch/pytorch/pull/160745))
- DTensor
  - Fixed DTensor handling of conjugate bit. ([#158030](https://github.com/pytorch/pytorch/pull/158030))
  - Fixed `OpSchema` equality check ([#161231](https://github.com/pytorch/pytorch/pull/161231))
  - Fixed `grouped_mm` strategy for invalid stride cases ([#158245](https://github.com/pytorch/pytorch/pull/158245))
  - Fixed `F.one_hot` in DTensor ([#162307](https://github.com/pytorch/pytorch/pull/162307))
  - Always disabled `ShardingPropagation` cache if compiling ([#156868](https://github.com/pytorch/pytorch/pull/156868))
- FullyShardedDataParallel (FSDP)
  - Fixed the bug in FSDP offload pin_memory ([#157147](https://github.com/pytorch/pytorch/pull/157147))
  - Fixed to ensure writeback handles `NO_SHARD` correctly by flattening tensors before copying ([#154369](https://github.com/pytorch/pytorch/pull/154369))
- FullyShardedDataParallel2 (FSDP2)
  - Fixed error message for `fsdp_pre_all_gather` ([#160817](https://github.com/pytorch/pytorch/pull/160817))
  - Fixed the issue with `set_reduce_scatter_divide_factor` errors and `MixedPrecisionPolicy`  ([#155964](https://github.com/pytorch/pytorch/pull/155964))
- Pipeline Parallelism (PP)
  - Fixed eval step under no_grad() ([#159293](https://github.com/pytorch/pytorch/pull/159293))
  - Fixed zero bubble schedules for eval() ([#159475](https://github.com/pytorch/pytorch/pull/159475))
- TensorPipe
  - Fixed `import torch` if compiled without `TensorPipe` ([#159461](https://github.com/pytorch/pytorch/pull/159461))
- TorchElastic
  - Fixed wrong log file name in the docs of `torch.distributed.elastic.multiprocessing.start_processes()` ([#160396](https://github.com/pytorch/pytorch/pull/160396))
### performance
### docs
- c10d
  - Documented barrier collective's interaction with device_id ([#159389](https://github.com/pytorch/pytorch/pull/159389))
  - Fixed comment to match logic in `distributed_c10d.py` ([#162158](https://github.com/pytorch/pytorch/pull/162158))
- DTensor
  - Rewrote doc of `TupleStrategy` ([#158132](https://github.com/pytorch/pytorch/pull/158132))
  - Documented `redistribute_costs` ([#158495](https://github.com/pytorch/pytorch/pull/158495))
- FullyShardedDataParallel (FSDP)
  - Removed FSDP1 developer note ([#158991](https://github.com/pytorch/pytorch/pull/158991))
### devs
- c10d
  - Added `waitcounter` for watchdog and heartbeat monitoring thread ([#157480](https://github.com/pytorch/pytorch/pull/157480))
  - Made `torch.distributed.breakpoint` set a long timeout ([#158481](https://github.com/pytorch/pytorch/pull/158481))
  - Added `check_rng_sync` util ([#160283](https://github.com/pytorch/pytorch/pull/160283))
  - Added `FlightRecorder` support for `ProcessGroupXCCL` ([#158568](https://github.com/pytorch/pytorch/pull/158568))
  - Added `early_stop` kwarg to `torch.utils.checkpoint` ([#160781](https://github.com/pytorch/pytorch/pull/160781))
- DTensor
  - Wrapped sharding prop error with contextual exception ([#161574](https://github.com/pytorch/pytorch/pull/161574))
  - Added check if tracing for sharding propagation to handle un-hashable keys in DTensor ([#160798](https://github.com/pytorch/pytorch/pull/160798))
- DeviceMesh
  - Added error when users try to slice non contiguous flattened dim submesh ([#157523](https://github.com/pytorch/pytorch/pull/157523))
  - Made the repr shorter when debug ENV not set ([#158822](https://github.com/pytorch/pytorch/pull/158822))
- ShardedTensor
  - Made error message descriptive in ShardedTensor creation (#150627) ([#159423](https://github.com/pytorch/pytorch/pull/159423))
- Pipeline Parallelism (PP)
  - Added profiling to schedule execution ([#160753](https://github.com/pytorch/pytorch/pull/160753))
### Untopiced
### not user facing
- Fix clang builds by adding headers ([#160252](https://github.com/pytorch/pytorch/pull/160252))
- [c10d] Lessen density of barrier warning ([#162015](https://github.com/pytorch/pytorch/pull/162015))
- port 3 distributed test to Intel GPU and unified some common functions ([#158533](https://github.com/pytorch/pytorch/pull/158533))
- Edit a test case to detect potential bugs in all-gathering noncontiguous inputs in the Gloo backend ([#159542](https://github.com/pytorch/pytorch/pull/159542))
- [c10d] Cleanup split_group logic using the newly built splitGroup ([#158488](https://github.com/pytorch/pytorch/pull/158488))
- [cca] [c10d] Refactor CUDAEventCache into separate files ([#158616](https://github.com/pytorch/pytorch/pull/158616))
- [c10d][ez] Fix error message to reflect the correct API name ([#158668](https://github.com/pytorch/pytorch/pull/158668))
- [c10d][PGNCCL] Cleanup unused params for nccl comm split ([#157978](https://github.com/pytorch/pytorch/pull/157978))
- [PP] Rename _load_actions and validate ([#160558](https://github.com/pytorch/pytorch/pull/160558))
- Fix typo: 'Intializes' → 'Initializes' in _distributed_c10d.pyi docst… ([#157455](https://github.com/pytorch/pytorch/pull/157455))
- Revert #156868: Bring back symint check for sharding propagation cache ([#159671](https://github.com/pytorch/pytorch/pull/159671))
- [PGNCCLx] Bring split and merge for PGNCCLx ([#158790](https://github.com/pytorch/pytorch/pull/158790))
- Allow parallel start NUMA binding ([#161576](https://github.com/pytorch/pytorch/pull/161576))
- [reland][DTensor][FSDP2] necessary changes to FSDP and TP to unblock EP ([#158204](https://github.com/pytorch/pytorch/pull/158204))
- [1/N] Port 5 _composable/fsdp distributed test cases to Intel GPU ([#159118](https://github.com/pytorch/pytorch/pull/159118))
- [DTensor][FSDP2] necessary changes to FSDP and TP to unblock EP ([#157216](https://github.com/pytorch/pytorch/pull/157216))
- [dtensor] add support for fused optimizer with parameters across multiple meshes ([#157682](https://github.com/pytorch/pytorch/pull/157682))
- [DTensor][FSDP2] necessary changes to FSDP and TP to unblock EP ([#157216](https://github.com/pytorch/pytorch/pull/157216))
- [DTensor] select strategy with no redistribute when redistribute cost is 0 ([#161882](https://github.com/pytorch/pytorch/pull/161882))
- [DTensor] forbid view ops to redistribute when local split is impossible ([#161950](https://github.com/pytorch/pytorch/pull/161950))
- port distributed tensor parallel test files for Intel GPU ([#161261](https://github.com/pytorch/pytorch/pull/161261))
- port distributed tensor test files for Intel GPU ([#161604](https://github.com/pytorch/pytorch/pull/161604))
- [DeviceMesh][ez] Make the logic within flatten simpler ([#158999](https://github.com/pytorch/pytorch/pull/158999))
- [torch][test] skip `test_transformer_backend_inductor_fullgraph_True` ([#156763](https://github.com/pytorch/pytorch/pull/156763))
- [1/n] refactor the ring attention implementation ([#155441](https://github.com/pytorch/pytorch/pull/155441))
- [2/n] rewrite load balancing and sharding in context parallel ([#155442](https://github.com/pytorch/pytorch/pull/155442))
- Minor error message fix in device_mesh.py ([#157096](https://github.com/pytorch/pytorch/pull/157096))
- Fix `aten::index_put` args Dtensor type mismatch and add a propagation strategy ([#156240](https://github.com/pytorch/pytorch/pull/156240))
- XCCL changes for DDP ([#155497](https://github.com/pytorch/pytorch/pull/155497))
- all_gather_bucketing fx pass ([#157396](https://github.com/pytorch/pytorch/pull/157396))
- Introduce sync_cross_rank_decision ([#156287](https://github.com/pytorch/pytorch/pull/156287))
- Fixing misspelling in documentation ([#157565](https://github.com/pytorch/pytorch/pull/157565))
- Fix index_put propagate strategy arg unpack error ([#157671](https://github.com/pytorch/pytorch/pull/157671))
- [inductor_collectives] Make reorder_collectives_preserve_peak pass grouping nodes ([#157706](https://github.com/pytorch/pytorch/pull/157706))
- Fix slice op redistribute_cost compute ([#157178](https://github.com/pytorch/pytorch/pull/157178))
- [inductor collectives] sink waits iterative ([#157708](https://github.com/pytorch/pytorch/pull/157708))
- Added philox based RNG context for HPU device in Dtensor scenarios ([#156581](https://github.com/pytorch/pytorch/pull/156581))
- Add stack trace of exception to MultiProcContinousTest ([#157589](https://github.com/pytorch/pytorch/pull/157589))
- Fix einsum strategy shard dim > ndim ([#157593](https://github.com/pytorch/pytorch/pull/157593))
- Fixes typo in nccl_window_registration test ([#157293](https://github.com/pytorch/pytorch/pull/157293))
- [BE] fix typo in torch/distributed/tensor/: childs -> children ([#156609](https://github.com/pytorch/pytorch/pull/156609))
- [Easy] Fix the compilation warning ([#157889](https://github.com/pytorch/pytorch/pull/157889))
- [doc] Document an invariant in OpSpec ([#157805](https://github.com/pytorch/pytorch/pull/157805))
- [doc] DeviceMesh invariant on DTensorSpec ([#157806](https://github.com/pytorch/pytorch/pull/157806))
- [BE] Remove stale pyre-fixme ([#157816](https://github.com/pytorch/pytorch/pull/157816))
- [simple_fsdp] Port fx pass to bucket reduce_scatters ([#157780](https://github.com/pytorch/pytorch/pull/157780))
- Tests for #158030 ([#158033](https://github.com/pytorch/pytorch/pull/158033))
- [1/N] cost coverage improvment ([#157504](https://github.com/pytorch/pytorch/pull/157504))
- [2/N] cost coverage improvment  ([#157738](https://github.com/pytorch/pytorch/pull/157738))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [DTensor] support split op on Partial placement ([#157991](https://github.com/pytorch/pytorch/pull/157991))
- [Reducer] Remove custom handling of view tensors for MTIA ([#157882](https://github.com/pytorch/pytorch/pull/157882))
- fix MPCT destroy_pg call ([#157952](https://github.com/pytorch/pytorch/pull/157952))
- [BE][2/16] fix typos in torch/ (torch/_*/) ([#156312](https://github.com/pytorch/pytorch/pull/156312))
- [test][distributed][vllm] stabilize the p2p sharing through ipc ([#158089](https://github.com/pytorch/pytorch/pull/158089))
- [BE][2/16] fix typos in torch/ (torch/_*/) ([#156312](https://github.com/pytorch/pytorch/pull/156312))
- [c10d] Prototype of `group_split` for dist2 work ([#157716](https://github.com/pytorch/pytorch/pull/157716))
- [DTensor] have split_strategy return OpStrategy instead of TupleStrategy ([#158051](https://github.com/pytorch/pytorch/pull/158051))
- [DTensor][BE] improve DTensor ops correctness check utils ([#158112](https://github.com/pytorch/pytorch/pull/158112))
- [SymmMem] Fix NCCL Hang in NVSHMEM Triton Wait Until Test ([#158167](https://github.com/pytorch/pytorch/pull/158167))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [DTensor][BE] add document to ShardingPropagator.register_op_strategy ([#158362](https://github.com/pytorch/pytorch/pull/158362))
- [BE][15/16] fix typos in torch/ (torch/distributed/tensor/) ([#156605](https://github.com/pytorch/pytorch/pull/156605))
- Fix clamp(min/max) strategy ([#158619](https://github.com/pytorch/pytorch/pull/158619))
- Use init_device_mesh API for select tests where possible ([#158675](https://github.com/pytorch/pytorch/pull/158675))
- Using torch.accelerator in comm_mode_features_example.py and visualize_sharding_example.py ([#157317](https://github.com/pytorch/pytorch/pull/157317))
- Making input dynamically adjust. ([#157324](https://github.com/pytorch/pytorch/pull/157324))
- [doc] Updates to distributed.md for XCCL backend ([#155834](https://github.com/pytorch/pytorch/pull/155834))
- [1/N] support of replication fallback strategy ([#158046](https://github.com/pytorch/pytorch/pull/158046))
- Support `sort` and `scatter_add` strategy ([#159022](https://github.com/pytorch/pytorch/pull/159022))
- add a util function _make_all_gather_out_tensor to reduce code duplication ([#149912](https://github.com/pytorch/pytorch/pull/149912))
- [bucketing] Rewrite all_gather, reduce_scatter passes via tracing merge_fn ([#158663](https://github.com/pytorch/pytorch/pull/158663))
- add softmax_backward_strategy missing field ([#159167](https://github.com/pytorch/pytorch/pull/159167))
- Fix SDPA sharding when `return_debug_mask` is False ([#159205](https://github.com/pytorch/pytorch/pull/159205))
- fix torch/distributed contributing doc ([#158934](https://github.com/pytorch/pytorch/pull/158934))
- Fix redistribution costs for slice_scatter ([#159223](https://github.com/pytorch/pytorch/pull/159223))
- Add a title to distributed._dist2.md ([#159385](https://github.com/pytorch/pytorch/pull/159385))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [BE] Fix global config leak in test_c10d_functional_native.py ([#159476](https://github.com/pytorch/pytorch/pull/159476))
- [BE] Fix buf name mismatch in test_c10d_functional_native.py ([#159487](https://github.com/pytorch/pytorch/pull/159487))
- [DTensor] Improve `sort` strategy ([#159189](https://github.com/pytorch/pytorch/pull/159189))
- [PP] Refactor test_schedule_multiproc ([#158780](https://github.com/pytorch/pytorch/pull/158780))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- Fix inductor memory estimation when a single buf has multiple mutations. Add runtime verification of mem tracking ([#159569](https://github.com/pytorch/pytorch/pull/159569))
- [Distributed] Fix `@parametrize` on unordered iterable in distributed test ([#159793](https://github.com/pytorch/pytorch/pull/159793))
- [DTensor] Set up DTensorContinuousTestBase ([#159885](https://github.com/pytorch/pytorch/pull/159885))
- [async-TP] Make scaled-mm + reduce-scatter preserve alignment of scales ([#159957](https://github.com/pytorch/pytorch/pull/159957))
- [DTensor] support _StridedShard in view op ([#159656](https://github.com/pytorch/pytorch/pull/159656))
- [replicate][be] improved readability of test case description ([#160128](https://github.com/pytorch/pytorch/pull/160128))
- [SymmMem] Fix flaky wait_until test ([#159215](https://github.com/pytorch/pytorch/pull/159215))
- improve gather and scatter_add strategy ([#160140](https://github.com/pytorch/pytorch/pull/160140))
- rename-HAS_CUDA-to-HAS_CUDA_AND_TRITON ([#159883](https://github.com/pytorch/pytorch/pull/159883))
- Add type assert for tensor_meta, based on real bug in autoparallel. ([#157927](https://github.com/pytorch/pytorch/pull/157927))
- [FR] Add stack_id and an optional print of stack_id to stack_trace mapping ([#160119](https://github.com/pytorch/pytorch/pull/160119))
- [bucketing] Bucket only adjacent collectives to prevent reordering ([#159983](https://github.com/pytorch/pytorch/pull/159983))
- Ensure outer aliasing on DTensor matches inner aliasing ([#158954](https://github.com/pytorch/pytorch/pull/158954))
- [DTensor] Registers sharding rule for rms_norm ([#159692](https://github.com/pytorch/pytorch/pull/159692))
- [claude-code] Add top-level module doc for torch/distributed/tensor/_op_schema.py ([#157804](https://github.com/pytorch/pytorch/pull/157804))
- [1/N]Port 3  distributed/_tools test cases to Intel GPU ([#159543](https://github.com/pytorch/pytorch/pull/159543))
- [c10d] Fix test test_nccl_user_buffer_registration ([#160497](https://github.com/pytorch/pytorch/pull/160497))
- measure dispatch overhead ([#160504](https://github.com/pytorch/pytorch/pull/160504))
- [PP] Allow larger world_size schedule tests ([#160559](https://github.com/pytorch/pytorch/pull/160559))
- [DTensor] add op support: aten.squeeze_.dim ([#159532](https://github.com/pytorch/pytorch/pull/159532))
- typing distributed.py ([#160365](https://github.com/pytorch/pytorch/pull/160365))
- port 2 distributed pipeline test files for Intel GPU ([#159140](https://github.com/pytorch/pytorch/pull/159140))
- [BE][CUDA][Distributed] Add require_exact_world_size() and a few distributed unit test fixes   ([#160803](https://github.com/pytorch/pytorch/pull/160803))
- [ez] Only use default numa bindings if nproc == cuda device count ([#160848](https://github.com/pytorch/pytorch/pull/160848))
- [C10D] Fix spelling of MultiProcContinuousTest ([#160892](https://github.com/pytorch/pytorch/pull/160892))
- [C10D] Make MultiProcContinuousTest less spammy ([#160821](https://github.com/pytorch/pytorch/pull/160821))
- [ContextParallel] add Document Masking test ([#160700](https://github.com/pytorch/pytorch/pull/160700))
- Fix bucketing introducing cycles ([#160967](https://github.com/pytorch/pytorch/pull/160967))
- [bucketing] allow convert_element_type after fsdp reduce_scatter ([#161159](https://github.com/pytorch/pytorch/pull/161159))
- Use comparison key in OpSchema to avoid duplicate work between `__hash__` and `__eq__` ([#161234](https://github.com/pytorch/pytorch/pull/161234))
- Minor cleanup of DeviceMesh.__eq__ ([#161235](https://github.com/pytorch/pytorch/pull/161235))
- Typo correction in variable name inital_grad of Class TestFullyShardG… ([#161501](https://github.com/pytorch/pytorch/pull/161501))
- [C10D] add _summarize_ranks util ([#160284](https://github.com/pytorch/pytorch/pull/160284))
- [SymmMem] Isolate set_device tests to avoid hang ([#161668](https://github.com/pytorch/pytorch/pull/161668))
- [BE][SymmMEM] Change Optional to the shorthand expression for symmetric memory modules ([#161676](https://github.com/pytorch/pytorch/pull/161676))
- [DTensor] fix DTensorTestCase.destroy_pg() when device_type is "cpu" but CUDA device is available ([#161015](https://github.com/pytorch/pytorch/pull/161015))
- [SymmMEM] Fix test_empty_strided_p2p_persistent ([#161677](https://github.com/pytorch/pytorch/pull/161677))
- [SymmMEM] Move AsyncTP tests to a seperate test class ([#161820](https://github.com/pytorch/pytorch/pull/161820))
- Pass shared_ptr by value ([#161834](https://github.com/pytorch/pytorch/pull/161834))
- [SymmMem][CI] Make sure group names are consistent ([#162035](https://github.com/pytorch/pytorch/pull/162035))
- Add torch.Tensor._make_dtensor to accelerate DTensor.__new__ further ([#161590](https://github.com/pytorch/pytorch/pull/161590))
- Fix `DeviceMesh._flatten` docstring example ([#162277](https://github.com/pytorch/pytorch/pull/162277))
- [3/N] Enable 6 fsdp test on Intel GPU ([#161601](https://github.com/pytorch/pytorch/pull/161601))
- [AsyncTP] Use assertEqual instead of allClose for bf16 tests ([#162041](https://github.com/pytorch/pytorch/pull/162041))
- [bucketing] custom_ops mode to hide inductor copies overhead ([#161499](https://github.com/pytorch/pytorch/pull/161499))
- [c10d] Move the include of header file of TraceUtils.h into NCCLUtil.cpp instead of keeping in hpp ([#156909](https://github.com/pytorch/pytorch/pull/156909))
- [BE] Rename libnvshmem_extension to libtorch_nvshmem ([#158234](https://github.com/pytorch/pytorch/pull/158234))
- [BE] Replace lib with TORCH_INSTALL_LIB_DIR ([#158235](https://github.com/pytorch/pytorch/pull/158235))
- [a2av] Test dispatch-then-combine ([#157026](https://github.com/pytorch/pytorch/pull/157026))
- [a2av] Make test input more random ([#157029](https://github.com/pytorch/pytorch/pull/157029))
- [Refactor] Remove unused variables ([#157654](https://github.com/pytorch/pytorch/pull/157654))
### security

### Removed because not released yet
- [c10d] Error out the case when registering symmetric memory without eager init ([#160145](https://github.com/pytorch/pytorch/pull/160145))
- harden fabric checks for symmetric memory ([#160790](https://github.com/pytorch/pytorch/pull/160790))
- [c10d][nvshmem] modify is_nvshmem_available runtime check to work with static-linked library (#159558) ([#159561](https://github.com/pytorch/pytorch/pull/159561))
- [c10d][nvshmem] fix missing override compilation error for nvshmem symmetric code ([#159557](https://github.com/pytorch/pytorch/pull/159557))
- fix retaining multimem in symmetric memory ([#160343](https://github.com/pytorch/pytorch/pull/160343))
- check driver to be >=12.4 to use fabric handles ([#159697](https://github.com/pytorch/pytorch/pull/159697))
- [SymmMem] Avoid library mismatch in CMake search ([#157836](https://github.com/pytorch/pytorch/pull/157836))
- [SymmMem] Remove redundant dist.barrier in Triton NVSHMEM tests & add device‐side signal_op support ([#156684](https://github.com/pytorch/pytorch/pull/156684))
- [SymmMem] Allow selection of allocation backend ([#156661](https://github.com/pytorch/pytorch/pull/156661))
- [BE] Remove SymmMem allocator destruct log ([#157020](https://github.com/pytorch/pytorch/pull/157020))
- [sym_mem] Further Fix NCCL symm mem unit test ([#157156](https://github.com/pytorch/pytorch/pull/157156))
- [SymmMem] Add NVSHMEM_CHECK macro ([#157174](https://github.com/pytorch/pytorch/pull/157174))
- [symm_mem] Create a one side get api for symm mem ([#157294](https://github.com/pytorch/pytorch/pull/157294))
- [SymmMem] Move code to where it is used ([#157611](https://github.com/pytorch/pytorch/pull/157611))
- [SymmMem] Add NVSHMEM barrier_all, my_pe, n_pes support into Triton ([#158511](https://github.com/pytorch/pytorch/pull/158511))
- [SymmMem] Add NVSHMEM sync_all support into Triton ([#158512](https://github.com/pytorch/pytorch/pull/158512))
- [SymmMem] Add NVSHMEM alltoall support into Triton ([#158513](https://github.com/pytorch/pytorch/pull/158513))
- [SymmMem] Add NVSHMEM broadcast support into Triton ([#158514](https://github.com/pytorch/pytorch/pull/158514))
- [SymmMem] Add NVSHMEM Reduction support (sum, min, max) into Triton ([#158515](https://github.com/pytorch/pytorch/pull/158515))
- [SymmMem]  Standardize NVSHMEM Triton wrappers on byte-based APIs + improve code clarity ([#159136](https://github.com/pytorch/pytorch/pull/159136))
- [SymmMem] Add Triton 3.4 support to NVSHMEM Triton and fix CI tests (make device library discoverable + fix peer calculation bug)  ([#159701](https://github.com/pytorch/pytorch/pull/159701))
- [SymmMem] Initialize NVSHMEM module only for kernels that have nvshmem in their name ([#159734](https://github.com/pytorch/pytorch/pull/159734))
- [SymmMem] Refactor NVSHMEM Reduction API to be more ergonomic with automatic dtype‐based dispatch ([#159755](https://github.com/pytorch/pytorch/pull/159755))
- [SymmMem] Add helpful docstrings for all NVSHMEM APIs  ([#159756](https://github.com/pytorch/pytorch/pull/159756))
- [SymmMem] Send tensors with unerased type information to NVSHMEM Triton kernels ([#159788](https://github.com/pytorch/pytorch/pull/159788))
- [SymmMem] Use _get_default_group() instead of group.WORLD for group_name access ([#158718](https://github.com/pytorch/pytorch/pull/158718))
- [SymmMem] Add team pool to hold duplicated teams for the same rank group ([#162320](https://github.com/pytorch/pytorch/pull/162320))
- [SymmMem] Better tuning of A2AV based on accurate node boundary ([#162003](https://github.com/pytorch/pytorch/pull/162003))
- Add missing fstream include to fix std::ofstream compilation error ([#162421](https://github.com/pytorch/pytorch/pull/162421))
- [SymmMEM] Allow to import _SymmetricMemory when NVSHMEM is not available ([#162142](https://github.com/pytorch/pytorch/pull/162142))
- [AsyncTP] Fixes AsyncMM ([#162040](https://github.com/pytorch/pytorch/pull/162040))
- [SymmMem] Use host/nvshmem_api.h for backward compat ([#159061](https://github.com/pytorch/pytorch/pull/159061))
- [SymmMem] Use global pe for put and get ([#162394](https://github.com/pytorch/pytorch/pull/162394))
- [SymmMem] Check return of nvshmem_malloc ([#160603](https://github.com/pytorch/pytorch/pull/160603))
- [SymmMem] Add device guard before alloc ([#161214](https://github.com/pytorch/pytorch/pull/161214))
- [SymmMem] Make sure CUDA runtime is initialized before NVSHMEM init ([#161232](https://github.com/pytorch/pytorch/pull/161232))
- [SymmMem] Increase minimum nthreads to cover sync needs in NVL72 ([#161983](https://github.com/pytorch/pytorch/pull/161983))
- [SymmMem] Use non-blocking version of getmem ([#162006](https://github.com/pytorch/pytorch/pull/162006))
- [SymmMem] Add a helper API to distinguish intra- and inter- node ([#161984](https://github.com/pytorch/pytorch/pull/161984))
- [SymmMem] Increase signal pad size for NVL72 ([#162026](https://github.com/pytorch/pytorch/pull/162026))
- [SymmMem] Feed tensor.data_ptr instead of handle.buffer_ptr into kernels ([#162193](https://github.com/pytorch/pytorch/pull/162193))
- Use more fine-grained locks in sym mem kernels ([#158523](https://github.com/pytorch/pytorch/pull/158523))
- [1/N][SymmMem] Add offset to handle, cache on base address ([#161470](https://github.com/pytorch/pytorch/pull/161470))
- use host+device_id to make sure devices are unique in rendezvous request ([#161756](https://github.com/pytorch/pytorch/pull/161756))
- [cutlass upgrade] Ignore unused-but-set-variable for AsyncMM.cu ([#159578](https://github.com/pytorch/pytorch/pull/159578))
- fix compilation on cuda < 12.3 ([#159657](https://github.com/pytorch/pytorch/pull/159657))
- guard cuMulticastUnbind call ([#160499](https://github.com/pytorch/pytorch/pull/160499))
- [c10d][nvshmem] add nvshmem build rules and dependency for libtorch_cuda ([#159562](https://github.com/pytorch/pytorch/pull/159562))
- [nccl symm mem] don't use arg for mempool, correctly use symmetric registration in hooks ([#161238](https://github.com/pytorch/pytorch/pull/161238))
- [a2av] Split in_out_splits into in_splits and out_splits_offsets ([#156743](https://github.com/pytorch/pytorch/pull/156743))
- [a2av] Add token combine operator ([#156881](https://github.com/pytorch/pytorch/pull/156881))
- [a2av] not returning out tensor from ops ([#159435](https://github.com/pytorch/pytorch/pull/159435))
- support fabric handles with symmetric memory ([#159319](https://github.com/pytorch/pytorch/pull/159319))
- [replicate][be] improved readability and cleaned up remaining DDP code ([#160133](https://github.com/pytorch/pytorch/pull/160133))
- dist2: cleanup non-option methods on PG (missing, timeouts) ([#158123](https://github.com/pytorch/pytorch/pull/158123))
- dist2: add support for passing custom configs directly to PG ([#158147](https://github.com/pytorch/pytorch/pull/158147))
- [c10d]Prototype of remote_group_merge ([#158287](https://github.com/pytorch/pytorch/pull/158287))
- [FSDP][Replicate] added replicate function that uses FSDP instead of DDP ([#158207](https://github.com/pytorch/pytorch/pull/158207))
- torch.distributed: add initial _dist2 prototype API ([#157841](https://github.com/pytorch/pytorch/pull/157841))
- dist2: add group context manager ([#157988](https://github.com/pytorch/pytorch/pull/157988))
- Work: block_current_stream API ([#156883](https://github.com/pytorch/pytorch/pull/156883))
- [c10d] block_current_stream: correctness fixes ([#158757](https://github.com/pytorch/pytorch/pull/158757))
- Add pg transport and tests ([#154653](https://github.com/pytorch/pytorch/pull/154653))
